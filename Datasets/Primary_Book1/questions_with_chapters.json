[
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. For the equations x+y = 4, 2x−2y = 4, draw the row picture (two intersecting lines) and the column picture (combination of two columns equal to the column vector (4,4) on the right side)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. Solve to find a combination of the columns that equals b: u − v − w = b 1 Triangular system v + w = b 2 w = b . 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. (Recommended) Describe the intersection of the three planes u+v+w+z = 6 and u+w+z=4 and u+w=2 (all in four-dimensional space). Is it a line or a point or an empty set? What is the intersection if the fourth plane u = −1 is included? Find a fourth equation that leaves us with no solution."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "4. Sketch these three lines and decide if the equations are solvable: x + 2y = 2 3 by 2 system x − y = 2 y = 1. What happens if all right-hand sides are zero? Is there any nonzero choice of right- hand sides that allows the three lines to intersect at the same point?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "5. Find two points on the line of intersection of the three planes t = 0 and z = 0 and x+y+z+t =1 in four-dimensional space."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.2 TheGeometryofLinearEquations 11"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "6. Whenb=(2,5,7),findasolution(u,v,w)toequation(4)differentfromthesolution (1,0,1) mentioned in the text."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "7. Give two more right-hand sides in addition to b = (2,5,7) for which equation (4) can be solved. Give two more right-hand sides in addition to b=(2,5,6) for which it cannot be solved."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "8. Explain why the system u + v + w = 2 u + 2v + 3w = 1 v + 2w = 0 is singular by finding a combination of the three equations that adds up to 0 = 1. What value should replace the last zero on the right side to allow the equations to have solutions—and what is one of the solutions?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "9. The column picture for the previous exercise (singular system) is       1 1 1       u1+v2+w3=b. 0 1 2 Show that the three columns on the left lie in the same plane by expressing the third column as a combination of the first two. What are all the solutions (u,v,w) if b is the zero vector (0,0,0)?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "10. (Recommended) Under what condition on y , y , y do the points (0,y ), (1,y ), 1 2 3 1 2 (2,y ) lie on a straight line? 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "11. These equations are certain to have the solution x = y = 0. For which values of a is there a whole line of solutions? ax + 2y = 0 2x + ay = 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "12. Starting with x+4y=7, find the equation for the parallel line through x=0, y=0. Find the equation of another line that meets the first at x=3, y=1. Problems 13–15 are a review of the row and column pictures."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "13. Draw the two pictures in two planes for the equations x−2y=0, x+y=6."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "14. For two linear equations in three unknowns x, y, z, the row picture will show (2 or 3) (lines or planes) in (two or three)-dimensional space. The column picture is in (two or three)-dimensional space. The solutions normally lie on a ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "15. For four linear equations in two unknowns x and y, the row picture shows four . Thecolumnpictureisin -dimensionalspace. Theequationshaveno solution unless the vector on the right-hand side is a combination of ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "16. Find a point with z = 2 on the intersection line of the planes x+y+3z = 6 and x−y+z=4. Find the point with z=0 and a third point halfway between."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "17. The first of these equations plus the second equals the third: x + y + z = 2 x + 2y + z = 3 2x + 3y + 2z = 5. The first two planes meet along a line. The third plane contains that line, because if x, y, z satisfy the first two equations then they also . The equations have infinitely many solutions (the whole line L). Find three solutions."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "18. Move the third plane in Problem 17 to a parallel plane 2x+3y+2z = 9. Now the three equations have no solution—why not? The first two planes meet along the line L, but the third plane doesn’t that line."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "19. In Problem 17 the columns are (1,1,2) and (1,2,3) and (1,1,2). This is a “singular case” because the third column is . Find two combinations of the columns that give b=(2,3,5). This is only possible for b=(4,6,c) if c= ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "20. Normally 4 “planes” in four-dimensional space meet at a . Normally 4 col- umn vectors in four-dimensional space can combine to produce b. What combina- tion of (1,0,0,0), (1,1,0,0), (1,1,1,0), (1,1,1,1) produces b=(3,3,3,2)? What 4 equations for x, y, z,t are you solving?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "21. When equation 1 is added to equation 2, which of these are changed: the planes in the row picture, the column picture, the coefficient matrix, the solution?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "22. If (a,b) is a multiple of (c,d) with abcd (cid:54)=0, show that (a,c) is a multiple of (b,d). This is surprisingly important: call it a challenge question. You could use numbers first to see how a, b, c, and d are related. The question will lead to: (cid:163) (cid:164) If A= a b has dependent rows then it has dependent columns. c d"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "23. In these equations, the third column (multiplying w) is the same as the right side b. The column form of the equations immediately gives what solution for (u,v,w)? 6u + 7v + 8w = 8 4u + 5v + 9w = 9 2u − 2v + 7w = 7."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.3 AnExampleofGaussianElimination 13"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.3 An Example of Gaussian Elimination The way to understand elimination is by example. We begin in three dimensions: 2u + v + w = 5 Original system 4u − 6v = −2 (1) −2u + 7v + 2w = 9. Theproblemistofindtheunknownvaluesofu,v,andw,andweshallapplyGaussian elimination. (Gaussisrecognizedasthegreatestofallmathematicians, butcertainlynot becauseofthisinvention,whichprobablytookhimtenminutes. Ironically,itisthemost frequently used of all the ideas that bear his name.) The method starts by subtracting multiplesofthefirstequationfromtheotherequations. Thegoalistoeliminateufrom the last two equations. This requires that we (a) subtract 2 times the first equation from the second (b) subtract −1 times the first equation from the third. 2u + v + w = 5 Equivalent system − 8v − 2w = −12 (2) 8v + 3w = 14. The coefficient 2 is the first pivot. Elimination is constantly dividing the pivot into the numbers underneath it, to find out the right multipliers. Thepivotforthesecondstageofeliminationis−8. Wenowignorethefirstequation. A multiple of the second equation will be subtracted from the remaining equations (in this case there is only the third one) so as to eliminate v. We add the second equation to the third or, in other words, we (c) subtract −1 times the second equation from the third. The elimination process is now complete, at least in the “forward” direction: 2u + v + w = 5 Triangular system − 8v − 2w = −12 (3) 1w = 2. This system is solved backward, bottom to top. The last equation gives w = 2. Sub- stituting into the second equation, we find v = 1. Then the first equation gives u = 1. This process is called back-substitution. To repeat: Forward elimination produced the pivots 2, −8, 1. It subtracted multiples of each row from the rows beneath, It reached the “triangular” system (3), which is solved in reverse order: Substitute each newly computed value into the equations that are waiting. Remark. One good way to write down the forward elimination steps is to include the right-hand side as an extra column. There is no need to copy u and v and w and = at every step, so we are left with the bare minimum:       2 1 1 5 2 1 1 5 2 1 1 5        4 −6 0 −2−→0 −8 −2 −12−→0 −8 −2 −12. −2 7 2 9 0 8 3 14 0 0 1 2 At the end is the triangular system, ready for back-substitution. You may prefer this arrangement, which guarantees that operations on the left-hand side of the equations are also done on the right-hand side—because both sides are there together. In a larger problem, forward elimination takes most of the effort. We use multiples of the first equation to produce zeros below the first pivot. Then the second column is cleared out below the second pivot. The forward step is finished when the system is triangular; equation n contains only the last unknown multiplied by the last pivot. Back- substitution yields the complete solution in the opposite order—beginning with the last unknown, then solving for the next to last, and eventually for the first. By definition, pivots cannot be zero. We need to divide by them. The Breakdown of Elimination Under what circumstances could the process break down? Something must go wrong in the singular case, and something might go wrong in the nonsingular case. This may seem a little premature—after all, we have barely got the algorithm working. But the possibility of breakdown sheds light on the method itself. The answer is: With a full set of n pivots, there is only one solution. The system is nonsingular,anditissolvedbyforwardeliminationandback-substitution. Butifazero appears in a pivot position, elimination has to stop—either temporarily or permanently. The system might or might not be singular. If the first coefficient is zero, in the upper left corner, the elimination of u from the other equations will be impossible. The same is true at every intermediate stage. Notice that a zero can appear in a pivot position, even if the original coefficient in that place was not zero. Roughly speaking, we do not know whether a zero will appear until we try, by actually going through the elimination process. Inmanycasesthisproblemcanbecured,andeliminationcanproceed. Suchasystem still counts as nonsingular; it is only the algorithm that needs repair. In other cases a breakdown is unavoidable. Those incurable systems are singular, they have no solution or else infinitely many, and a full set of pivots cannot be found."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.3 AnExampleofGaussianElimination 17"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. What multiple (cid:96) of equation 1 should be subtracted from equation 2? 2x + 3y = 1 10x + 9y = 11. Afterthiseliminationstep,writedowntheuppertriangularsystemandcirclethetwo pivots. The numbers 1 and 11 have no influence on those pivots."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. Solve the triangular system of Problem 1 by back-substitution, y before x. Verify that x times (2,10) plus y times (3,9) equals (1,11). If the right-hand side changes to (4,44), what is the new solution?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. What multiple of equation 2 should be subtracted from equation 3? 2x − 4y = 6 −x + 5y = 0. Afterthiseliminationstep,solvethetriangularsystem. Iftheright-handsidechanges to (−6,0), what is the new solution?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "4. What multiple (cid:96) of equation 1 should be subtracted from equation 2? ax + by = f cx + dy = g. The first pivot is a (assumed nonzero). Elimination produces what formula for the second pivot? What is y? The second pivot is missing when ad =bc."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "5. Choose a right-hand side which gives no solution and another right-hand side which gives infinitely many solutions. What are two of those solutions? 3x + 2y = 10 6x + 4y = ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "6. Choose a coefficient b that makes this system singular. Then choose a right-hand side g that makes it solvable. Find two solutions in that singular case. 2x + by = 16 4x + 8y = g."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "7. For which numbers a does elimination break down (a) permanently, and (b) tem- porarily? ax + 3y = −3 4x + 6y = 6. Solve for x and y after fixing the second breakdown by a row exchange."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "8. For which three numbers k does elimination break down? Which is fixed by a row exchange? In each case, is the number of solutions 0 or 1 or ∞? kx + 3y = 6 3x + ky = −6."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "9. What test on b and b decides whether these two equations allow a solution? How 1 2 many solutions will they have? Draw the column picture. 3x − 2y = b 1 6x − 4y = b . 2 Problems 10–19 study elimination on 3 by 3 systems (and possible failure)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "10. Reduce this system to upper triangular form by two row operations: 2x + 3y + z = 8 4x + 7y + 5z = 20 − 2y + 2z = 0. Circle the pivots. Solve by back-substitution for z, y, x."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "11. Apply elimination (circle the pivots) and back-substitution to solve 2x − 3y = 3 4x − 5y + z = 7 2x − y − 3z = 5. List the three row operations: Subtract times row from row ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "12. Which number d forces a row exchange, and what is the triangular system (not sin- gular) for that d? Which d makes this system singular (no third pivot)? 2x + 5y + z = 0 4x + dy + z = 2 y − z = 3."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "13. Which number b leads later to a row exchange? Which b leads to a missing pivot? In that singular case find a nonzero solution x, y, z. x + by = 0 x − 2y − z = 0 y + z = 0."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "14. (a) Construct a 3 by 3 system that needs two row exchanges to reach a triangular form and a solution. (b) Construct a 3 by 3 system that needs a row exchange to keep going, but breaks down later."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.3 AnExampleofGaussianElimination 19"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "15. If rows 1 and 2 are the same, how far can you get with elimination (allowing row exchange)? If columns 1 and 2 are the same, which pivot is missing? 2x−y+z=0 2x+2y+z=0 2x−y+z=0 4x+4y+z=0 4x+y+z=2 6x+6y+z=2."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "16. Construct a 3 by 3 example that has 9 different coefficients on the left-hand side, but rows 2 and 3 become zero in elimination. How many solutions to your system with b=(1,10,100) and how many with b=(0,0,0)?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "17. Which number q makes this system singular and which right-hand side t gives it infinitely many solutions? Find the solution that has z=1. x + 4y − 2z = 1 x + 7y − 6z = 6 3y + qz = t."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "18. (Recommended)Itisimpossibleforasystemoflinearequationstohaveexactlytwo solutions. Explain why. (a) If (x,y,z) and (X,Y,Z) are two solutions, what is another one? (b) If 25 planes meet at two points, where else do they meet?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "19. Three planes can fail to have an intersection point, when no two planes are parallel. The system is singular if row 3 of A is a of the first two rows. Find a third equation that can’t be solved if x+y+z=0 and x−2y−z=1. Problems 20–22 move up to 4 by 4 and n by n."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "20. Find the pivots and the solution for these four equations: 2x + y = 0 x + 2y + z = 0 y + 2z + t = 0 z + 2t = 5."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "21. IfyouextendProblem20followingthe1, 2, 1patternorthe−1, 2, −1pattern, what is the fifth pivot? What is the nth pivot?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "22. Apply elimination and back-substitution to solve 2u + 3v = 0 4u + 5v + w = 3 2u − v − 3w = 5. What are the pivots? List the three operations in which a multiple of one row is subtracted from another."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "23. For the system u + v + w = 2 u + 3v + 3w = 0 u + 3v + 5w = 2, what is the triangular system after forward elimination, and what is the solution?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "24. Solve the system and find the pivots when 2u − v = 0 −u + 2v − w = 0 − v + 2w − z = 0 − w + 2z = 5. You may carry the right-hand side as a fifth column (and omit writing u, v, w, z until the solution at the end)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "25. Apply elimination to the system u + v + w = −2 3u + 3v − w = 6 u − v + w = −1. When a zero arises in the pivot position, exchange that equation for the one below it and proceed. What coefficient of v in the third equation, in place of the present −1, would make it impossible to proceed—and force elimination to break down?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "26. Solve by elimination the system of two equations x − y = 0 3x + 6y = 18. Draw a graph representing each equation as a straight line in the x-y plane; the lines intersect at the solution. Also, add one more line—the graph of the new second equation which arises after elimination."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "27. Find three values of a for which elimination breaks down, temporarily or perma- nently, in au + u = 1 4u + av = 2. Breakdown at the first step can be fixed by exchanging rows—but not breakdown at the last step."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "28. True or false: (a) If the third equation starts with a zero coefficient (it begins with 0u) then no multiple of equation 1 will be subtracted from equation 3."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 MatrixNotationandMatrixMultiplication 21 (b) If the third equation has zero as its second coefficient (it contains 0v) then no multiple of equation 2 will be subtracted from equation 3. (c) If the third equation contains 0u and 0v, then no multiple of equation 1 or equa- tion 2 will be subtracted from equation 3."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "29. (Very optional) Normally the multiplication of two complex numbers (a+ib)(c+id)=(ac−bd)+i(bc+ad) involvesthefourseparatemultiplicationsac,bd,be,ad. Ignoringi,canyoucompute ac−bd andbc+ad withonlythree multiplications? (Youmaydoadditions, suchas forming a+b before multiplying, without any penalty.)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "30. Use elimination to solve u + v + w = 6 u + v + w = 7 u + 2v + 2w = 11 and u + 2v + 2w = 10 2u + 3v − 4w = 3 2u + 3v − 4w = 3."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "31. For which three numbers a will elimination fail to give three pivots? ax+2y+3z=b 1 ax+ay+4z=b 2 ax+ay+az=b . 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "32. Findexperimentallytheaveragesize(absolutevalue)ofthefirstandsecondandthird pivotsforMATLAB’slu(rand(3,3)). Theaverageofthefirstpivotfromabs(A(1,1)) should be 0.5."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 Matrix Notation and Matrix Multiplication With our 3 by 3 example, we are able to write out all the equations in full. We can list the elimination steps, which subtract a multiple of one equation from another and reach a triangular matrix. For a large system, this way of keeping track of elimination would be hopeless; a much more concise record is needed. We now introduce matrix notation to describe the original system, and matrix mul- tiplication to describe the operations that make it simpler. Notice that three different types of quantities appear in our example: Nine coefficients 2u + v + w = 5 Three unknowns 4u − 6v = −2 (1) Three right-hand sides −2u + 7v + 2w = 9 Ontheright-handsideisthecolumnvectorb. Ontheleft-handsidearetheunknownsu, v, w. Also on the left-hand side are nine coefficients (one of which happens to be zero). It is natural to represent the three unknowns by a vector:     u 1     The unknown is x=v The solution is x=1. w 2 The nine coefficients fall into three rows and three columns, producing a 3 by 3 matrix:   2 1 1   Coefficient matrix A= 4 −6 0. −2 7 2 A is a square matrix, because the number of equations equals the number of unknowns. If there are n equations in n unknowns, we have a square n by n matrix. More generally, we might have m equations and n unknowns. Then A is rectangular, with m rows and n columns. It will be an “m by n matrix.” Matrices are added to each other, or multiplied by numerical constants, exactly as vectors are—one entry at a time. In fact we may regard vectors as special cases of matrices; they are matrices with only one column. As with vectors, two matrices can be added only if they have the same shape:           2 1 1 2 3 3 2 1 4 2 Addition A+B           3 0+−3 1=0 1 23 0=6 0. Multiplication 2A 0 4 1 2 1 6 0 4 0 8 Multiplication of a Matrix and a Vector We want to rewrite the three equations with three unknowns u, v, w in the simplified matrix form Ax=b. Written out in full, matrix times vector equals vector:      2 1 1 u 5      Matrix form Ax=b  4 −6 0v=−2. (2) −2 7 2 w 9 The right-hand side b is the column vector of “inhomogeneous terms.” The left-hand side is A times x. This multiplication will be defined exactly so as to reproduce the original system. The first component of Ax comes from “multiplying” the first row of A into the column vector x:   u (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)   Row times column 2 1 1 v= 2u+v+w = 5 . (3) w"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 MatrixNotationandMatrixMultiplication 23 ThesecondcomponentoftheproductAxis4u−6v+0w,fromthesecondrowofA. The matrixequationAx=bisequivalenttothethreesimultaneousequationsinequation(1). Row times column is fundamental to all matrix multiplications. From two vectors it produces a single number. This number is called the inner product of the two vectors. In other words, the product of a 1 by n matrix (a row vector) and an n by 1 matrix (a column vector) is a 1 by 1 matrix:   1 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)   Inner product 2 1 1 1= 2·1+1·1+1·2 = 5 . 2 This confirms that the proposed solution x=(1,1,2) does satisfy the first equation. There are two ways to multiply a matrix A and a vector x. One way is a row at a time, Each row of A combines with x to give a component of Ax. There are three inner products when A has three rows:        1 1 6 2 1·2+1·5+6·0 7        Ax by rows 3 0 15=3·2+0·5+3·0=6. (4) 1 1 4 0 1·2+1·5+4·0 7 ThatishowAxisusuallyexplained,butthesecondwayisequallyimportant. Infactitis more important! It does the multiplication a column at a time. The product Ax is found all at once, as a combination of the three columns of A:         1 1 6 7         Ax by columns 23+50+03=6. (5) 1 1 4 7 The answer is twice column 1 plus 5 times column 2. It corresponds to the “column picture” of linear equations. If the right-hand side b has components 7, 6, 7, then the solution has components 2, 5, 0. Of course the row picture agrees with that (and we eventually have to do the same multiplications). The column rule will be used over and over, and we repeat it for emphasis: 1A Every product Ax can be found using whole columns as in equation (5). Therefore Ax is a combination of the columns of A. The coefficients are the components of x. TomultiplyAtimesxinndimensions,weneedanotationfortheindividualentriesin A. The entry in the ith row and jth column is always denoted by a . The first subscript ij gives the row number, and the second subscript indicates the column. (In equation (4), a is 3 and a is 6.) If A is an m by n matrix, then the index i goes from 1 to m—there 21 13 aremrows—andtheindex j goesfrom1ton. Altogetherthematrix hasmnentries, and a is in the lower right corner. mn One subscript is enough for a vector. The jth component of x is denoted by x . (The j multiplication above had x = 2, x = 5, x = 0.) Normally x is written as a column 1 2 3 vector—like an n by 1 matrix. But sometimes it is printed on a line, as in x = (2,5,0). The parentheses and commas emphasize that it is not a 1 by 3 matrix. It is a column vector, and it is just temporarily lying down. To describe the product Ax, we use the “sigma” symbol Σ for summation: n Sigma notation The ith component of Ax is ∑ a x . ij j j=1 This sum takes us along the ith row of A. The column index j takes each value from 1 to n and we add up the results—the sum is a x +a x +···+a x . i1 1 i2 2 in n We see again that the length of the rows (the number of columns in A) must match the length of x. An m by n matrix multiplies an n-dimensional vector (and produces an m-dimensional vector). Summations are simpler than writing everything out in full, but matrix notation is better. (Einstein used “tensor notation,” in which a repeated index j automatically means summation. He wrote a x or even a x , without the Σ. Not being ij j i j Einstein, we keep the Σ.) The Matrix Form of One Elimination Step So far we have a convenient shorthand Ax = b for the original system of equations. What about the operations that are carried out during elimination? In our example, the first step subtracted 2 times the first equation from the second. On the right-hand side, 2 times the first component of b was subtracted from the second component. The same result is achieved if we multiply b by this elementary matrix (or elimination matrix):   1 0 0   Elementary matrix E =−2 1 0. 0 0 1 This is verified just by obeying the rule for multiplying a matrix and a vector:      1 0 0 5 5      Eb=−2 1 0−2=−12. 0 0 1 9 9 The components 5 and 9 stay the same (because of the 1, 0, 0 and 0, 0, 1 in the rows of E). The new second component −12 appeared after the first elimination step. It is easy to describe the matrices like E, which carry out the separate elimination steps. We also notice the “identity matrix,” which does nothing at all. 1B The identity matrix I, with 1s on the diagonal and 0s everywhere else, leaves every vector unchanged. The elementary matrix E subtracts (cid:96) times ij"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 MatrixNotationandMatrixMultiplication 25 row j from row i. This E includes −(cid:96) in row i, column j. ij       1 0 0 1 0 0 b 1       I =0 1 0 has Ib=b E = 0 1 0 has E b= b . 31 31 2 0 0 1 −(cid:96) 0 1 b −(cid:96)b 3 1 Ib = b is the matrix analogue of multiplying by 1. A typical elimination step multiplies by E . The important question is: What happens to A on the left- 31 hand side? To maintain equality, we must apply the same operation to both sides of Ax = b. In other words, we must also multiply the vector Ax by the matrix E. Our original matrix E subtracts 2 times the first component from the second, After this step the new and simpler system (equivalent to the old) is just E(Ax) = Eb. It is simpler because of the zero that was created below the first pivot. It is equivalent because we can recover the original system (by adding 2 times the first equation back to the second). So the two systems have exactly the same solution x. Matrix Multiplication Nowwecometothemostimportantquestion: Howdowemultiplytwomatrices? There is a partial clue from Gaussian elimination: We know the original coefficient matrix A, weknowtheeliminationmatrixE,andweknowtheresultEAaftertheeliminationstep. We hope and expect that       1 0 0 2 1 1 2 1 1       E =−2 1 0 times A= 4 −6 0 gives EA= 0 −8 −2. 0 0 1 −2 7 2 −2 7 2 Twice the first row of A has been subtracted from the second row. Matrix multiplication is consistent with the row operations of elimination. We can write the result either as E(Ax) = Eb, applying E to both sides of our equation, or as (EA)x = Eb. The matrix EA is constructed exactly so that these equations agree, and we don’t need parentheses: Matrix multiplication (EA times x) equals (E times Ax). We just write EAx. This is the whole point of an “associative law” like 2×(3×4) = (2×3)×4. The law seems so obvious that it is hard to imagine it could be false. But the same could be said of the “commutative law” 2×3=3×2—and for matrices EA is not AE. There is another requirement on matrix multiplication. We know how to multiply Ax, a matrix and a vector. The new definition should be consistent with that one. When a matrix B contains only a single column x, the matrix-matrix product AB should be identical with the matrix-vector product Ax. More than that: When B contains several columns b , b , b , the columns of AB should be Ab , Ab , Ab ! 1 2 3 1 2 3     b Ab 1 1     Multiplication by columns AB=Ab =Ab . 2 2 b Ab 3 3 Ourfirstrequirementhadtodowithrows, andthisoneisconcernedwithcolumns. A third approach is to describe each individual entry in AB and hope for the best. In fact, there is only one possible rule, and I am not sure who discovered it. It makes everything work. It does not allow us to multiply every pair of matrices. If they are square, they must have the same size. If they are rectangular, they must not have the same shape; the number of columns in A has to equal the number of rows in B. Then A can be multiplied into each column of B. If A is m by n, and B is n by p, then multiplication is possible. The product AB will be m by p. We now find the entry in row i and column j of AB. 1C The i, j entry of AB is the inner product of the ith row of A and the jth columnofB. InFigure1.7, the3, 2entryofABcomesfromrow3andcolumn 2: (AB) =a b +a b +a b +a b . (6) 32 31 12 32 22 33 32 34 42 Figure1.7: A3by4matrixAtimesa4by2matrixBisa3by2matrixAB. Note. We write AB when the matrices have nothing special to do with elimination. Our earlier example was EA, because of the elementary matrix E. Later we have PA, or LU, or even LDU. The rule for matrix multiplication stays the same. Example 1. (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 2 3 1 2 0 17 1 0 AB= = . 4 0 5 −1 0 4 8 0 The entry 17 is (2)(1)+(3)(5), the inner product of the first row of A and first column of B. The entry 8 is (4)(2)+(0)(−1), from the second row and second column. The third column is zero in B, so it is zero in AB. B consists of three columns side by side, and A multiplies each column separately. Every column of AB is a combination of the columns of A. Just as in a matrix-vector multiplication, the columns of A are multiplied by the entries in B."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 MatrixNotationandMatrixMultiplication 27 Example 2. (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 0 1 2 3 7 8 Row exchange matrix = . 1 0 7 8 2 3 Example 3. The 1s in the identity matrix I leave every matrix unchanged: Identity matrix IA=A and BI =B. Important: ThemultiplicationABcanalsobedonearowatatime. InExample1,the first row of AB uses the numbers 2 and 3 from the first row of A. Those numbers give 2[row 1]+3[row 2] = [17 1 0]. Exactly as in elimination, where all this started, each row of AB is a combination of the rows of B. We summarize these three different ways to look at matrix multiplication. 1D (i) Each entry of AB is the product of a row and a column: (AB) =(row i of A) times (column j of B) ij (ii) Each column of AB is the product of a matrix and a column: column j of AB=A times (column j of B) (iii) Each row of AB is the product of a row and a matrix: row i of AB= (row i of A) times B. This leads hack to a key property of matrix multiplication. Suppose the shapes of three matrices A, B,C (possibly rectangular) permit them to be multiplied. The rows in A and B multiply the columns in B andC. Then the key property is this: 1E Matrix multiplication is associative: (AB)C =A(BC). Just write ABC. AB timesC equals A times BC. IfC happens to be just a vector (a matrix with only one column) this is the requirement (EA)x = E(Ax) mentioned earlier. It is the whole basis for the laws of matrix multiplication. And if C has several columns, we have only to think of them placed side by side, and apply the same rule several times. Parentheses are not needed when we multiply several matrices. There are two more properties to mention—one property that matrix multiplication has, and another which it does not have. The property that it does possess is: 1F Matrix operations are distributive: A(B+C)=AB+AC and (B+C)D=BD+CD. Of course the shapes of these matrices must match properly—B andC have the same shape, so they can be added, and A and D are the right size for premultiplication and postmultiplication. The proof of this law is too boring for words. The property that fails to hold is a little more interesting: 1G Matrix multiplication is not commutative: Usually FE (cid:54)=EF. Example 4. Suppose E subtracts twice the first equation from the second. Suppose F is the matrix for the next step, to add row 1 to row 3:     1 0 0 1 0 0     E =−2 1 0 and F =0 1 0. 0 0 1 1 0 1 These two matrices do commute and the product does both steps at once:   1 0 0   EF =−2 1 0=FE. 1 0 1 In either order, EF or FE, this changes rows 2 and 3 using row 1. Example 5. Suppose E is the same but G adds row 2 to row 3. Now the order makes a difference. When we apply E and then G, the second row is altered before it affects the third. If E comes after G, then the third equation feels no effect from the first. You will see a zero in the (3,1) entry of EG, where there is a −2 in GE:        1 0 0 1 0 0 1 0 0 1 0 0        GE =0 1 0−2 1 0=−2 1 0 but EG=−2 1 0. 0 1 1 0 0 1 −2 1 1 0 1 1 Thus EG (cid:54)= GE. A random example would show the same thing—most matrices don’t commute. Here the matrices have meaning. There was a reason for EF = FE, and a reason for EG (cid:54)= GE. It is worth taking one more step, to see what happens with all three elimination matrices at once:     1 0 0 1 0 0     GFE =−2 1 0 and EFG=−2 1 0. −1 1 1 −1 1 1 The product GFE is the true order of elimination. It is the matrix that takes the original A to the upper triangularU. We will see it again in the next section. The other matrix EFG is nicer. In that order, the numbers −2 from E and 1 from F and G were not disturbed. They went straight into the product. It is the wrong order for elimination. But fortunately it is the right order for reversing the elimination steps— which also comes in the next section. Notice that the product of lower triangular matrices is again lower triangular."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. Compute the products       (cid:34) (cid:35)(cid:34) (cid:35) 4 0 1 3 1 0 0 5       2 0 1 0 1 04 and 0 1 0−2 and . 1 3 1 4 0 1 5 0 0 1 3 For the third one, draw the column vectors (2,1) and (0,3). Multiplying by (1,1) just adds the vectors (do it graphically)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. Working a column at a time, compute the products        (cid:34) (cid:35) (cid:34) (cid:35) 4 1 1 2 3 0 4 3   1      1 5 1 and 4 5 61 and 6 6 2 . 3 1 6 1 7 8 9 0 8 9 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. Find two inner products and a matrix product:       1 3 1 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)       1 −2 7 −2 and 1 −2 7 5 and −2 3 5 1 . 7 1 7 The first gives the length of the vector (squared)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "4. If an m by n matrix A multiplies an n-dimensional vector x, how many separate multiplications are involved? What if A multiplies an n by p matrix B?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "5. Multiply Ax to find a solution vector x to the system Ax= zero vector. Can you find more solutions to Ax=0?    3 −6 0 2    Ax=0 2 −21. 1 −1 −1 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "6. Writedownthe2by2matricesAandBthathaveentriesa =i+ jandb =(−1)i+j. ij ij Multiply them to find AB and BA."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "7. Give 3 by 3 examples (not just the zero matrix) of (a) a diagonal matrix: a =0 if i(cid:54)= j. ij (b) a symmetric matrix: a =a for all i and j. ij ji (c) an upper triangular matrix: a =0 if i> j. ij (d) a skew-symmetric matrix: a =−a for all i and j. ij ji"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "8. Do these subroutines multiply Ax by rows or columns? Start with B(I)=0: DO 10 I = 1, N DO 10 J = 1, N DO 10 J = 1, N DO 10 I = 1, N 10 B(I) = B(I) + A(I,J) * X(J) 10 B(I) = B(I) + A(I,J) * X(J) The outputs Bx = Ax are the same. The second code is slightly more efficient in FORTRAN and much more efficient on a vector machine (the first changes single entries B(I), the second can update whole vectors)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "9. If the entries of A are a , use subscript notation to write ij (a) the first pivot. (b) the multiplier (cid:96) of row 1 to be subtracted from row i. i1 (c) the new entry that replaces a after that subtraction. ij (d) the second pivot."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "10. True or false? Give a specific counterexample when false. (a) If columns 1 and 3 of B are the same, so are columns 1 and 3 of AB. (b) If rows 1 and 3 of B are the same, so are rows 1 and 3 of AB. (c) If rows 1 and 3 of A are the same, so are rows 1 and 3 of AB. (d) (AB)2 =A2B2."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "11. The first row of AB is a linear combination of all the rows of B. What are the coeffi- cients in this combination, and what is the first row of AB, if   (cid:34) (cid:35) 1 1 2 1 4   A= and B=0 1? 0 −1 1 1 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "12. The product of two lower triangular matrices is again lower triangular (all its entries above the main diagonal are zero). Confirm this with a 3 by 3 example, and then explain how it follows from the laws of matrix multiplication."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "13. By trial and error find examples of 2 by 2 matrices such that (a) A2 =−I, A having only real entries. (b) B2 =0, although B(cid:54)=0. (c) CD=−DC, not allowing the caseCD=0. (d) EF =0, although no entries of E or F are zero."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "14. Describe the rows of EA and the columns of AE if (cid:34) (cid:35) 1 7 E = . 0 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 MatrixNotationandMatrixMultiplication 31"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "15. Suppose A commutes with every 2 by 2 matrix (AB=BA), and in particular (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) a b 1 0 0 1 A= commutes with B = and B = . 1 2 c d 0 0 0 0 Show that a = d and b = c = 0. If AB = BA for all matrices B, then A is a multiple of the identity."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "16. Letxbethecolumnvector(1,0,...,0). Showthattherule(AB)x=A(Bx)forcesthe first column of AB to equal A times the first column of B."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "17. Which of the following matrices are guaranteed to equal (A+B)2? A2+2AB+B2, A(A+B)+B(A+B), (A+B)(B+A), A2+AB+BA+B2."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "18. If A and B are n by n matrices with all entries equal to 1, find (AB) . Summation ij notation turns the product AB, and the law (AB)C =A(BC), into (cid:195) (cid:33) (cid:195) (cid:33) (AB) =∑a b ∑ ∑a b c =∑a ∑b c . ij ik kj ik kj jl ik kj jl k j k k j Compute both sides ifC is also n by n, with every c =2. jl"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "19. A fourth way to multiply matrices is columns of A times rows of B: AB=(column 1)(row 1)+···+(column n)(row n)=sum of simple matrices. Give a 2 by 2 example of this important rule for matrix multiplication."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "20. The matrix that rotates the x-y plane by an angleθis (cid:34) (cid:35) cosθ −sinθ A(θ)= . sinθ cosθ VerifythatA(θ )A(θ )=A(θ +θ )fromtheidentitiesforcos(θ +θ )andsin(θ + 1 2 1 2 1 2 1 θ ). What is A(θ) times A(−θ)? 2"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "21. Find the powers A2, A3 (A2 times A), and B2, B3,C2,C3. What are Ak, Bk, andCk? (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 0 1 −1 A= 2 2 and B= and C =AB= 2 2 1 1 0 −1 1 −1 2 2 2 2 Problems 22–31 are about elimination matrices."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "22. Write down the 3 by 3 matrices that produce these elimination steps: (a) E subtracts 5 times row 1 from row 2. 21 (b) E subtracts −7 times row 2 from row 3. 32 (c) P exchanges rows 1 and 2, then rows 2 and 3."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "23. InProblem22,applyingE andthenE tothecolumnb=(1,0,0)givesE E b= 21 32 32 21 . Applying E before E gives E E b = . When E comes first, row 32 21 21 32 32 feels no effect from row ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "24. Which three matrices E , E , E put A into triangular formU? 21 31 32   1 1 0   A= 4 6 1 and E E E A=U. 32 31 21 −2 2 0 Multiply those E’s to get one matrix M that does elimination: MA=U."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "25. Suppose a = 7 and the third pivot is 5. If you change a to 11, the third pivot is 33 33 . If you change a to , there is zero in the pivot position. 33"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "26. IfeverycolumnofAisamultipleof(1,1,1),thenAxisalwaysamultipleof(1,1,1). Do a 3 by 3 example. How many pivots are produced by elimination?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "27. WhatmatrixE subtracts7timesrow1fromrow3? Toreversethatstep,R should 31 31 7 times row to row . Multiply E by R . 31 31"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "28. (a) E subtracts row 1 from row 2 and then P exchanges rows 2 and 3. What 21 23 matrix M =P E does both steps at once? 23 21 (b) P exchanges rows 2 and 3 and then E subtracts row I from row 3. What 23 31 matrix M = E P does both steps at once? Explain why the M’s are the same 31 23 but the E’s are different."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "29. (a) What 3 by 3 matrix E will add row 3 to row 1? 13 (b) What matrix adds row 1 to row 3 and at the same time adds row 3 to row 1? (c) What matrix adds row 1 to row 3 and then adds row 3 to row 1?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "30. Multiply these matrices:        0 0 1 1 2 3 0 0 1 1 0 0 1 2 3        0 1 04 5 60 1 0 and −1 1 01 3 1. 1 0 0 7 8 9 1 0 0 −1 0 1 1 4 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "31. This 4 by 4 matrix needs which elimination matrices E and E and E ? 21 32 43   2 −1 0 0   −1 2 −1 0  A= .  0 −1 2 −1 0 0 −1 2 Problems 32–44 are about creating and multiplying matrices"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 MatrixNotationandMatrixMultiplication 33"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "32. Write these ancient problems in a 2 by 2 matrix form Ax=b and solve them: (a) X is twice as old asY and their ages add to 39, (b) (x,y)=(2,5) and (3,7) lie on the line y=mx+c. Find m and c."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "33. The parabola y = a+bx+cx2 goes through the points (x,y) = (1,4) and (2,8) and (3,14). Find and solve a matrix equation for the unknowns (a,b,c)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "34. Multiply these matrices in the orders EF and FE and E2:     1 0 0 1 0 0     E =a 1 0 F =0 1 0. b 0 1 0 c 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "35. (a) Suppose all columns of B are the same. Then all columns of EB are the same, because each one is E times . (b) Suppose all rows of B are [1 2 4]. Show by example that all rows of EB are not [1 2 4]. It is true that those rows are ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "36. If E adds row 1 to row 2 and F adds row 2 to row 1, does EF equal FE?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "37. The first component of Ax is ∑a x = a x +···+a x . Write formulas for the 1j j 11 1 1n n third component of Ax and the (1,1) entry of A2."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "38. If AB=I and BC =I, use the associative law to prove A=C."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "39. Ais3by5, Bis5by3,C is5by1, and Dis3by1. Allentriesare1. Whichofthese matrix operations are allowed, and what are the results? BA AB ABD DBA A(B+C)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "40. What rows or columns or matrices do you multiply to find (a) the third column of AB? (b) the first row of AB? (c) the entry in row 3, column 4 of AB? (d) the entry in row 1, column 1 ofCDE?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "41. (3 by 3 matrices) Choose the only B so that for every matrix A, (a) BA=4A. (b) BA=4B. (c) BA has rows 1 and 3 of A reversed and row 2 unchanged. (d) All rows of BA are the same as row 1 of A."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "42. True or false? (a) If A2 is defined then A is necessarily square. (b) If AB and BA are defined then A and B are square. (c) If AB and BA are defined then AB and BA are square. (d) If AB=B then A=I."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "43. If A is m by n, how many separate multiplications are involved when (a) A multiplies a vector x with n components? (b) A multiplies an n by p matrix B? Then AB is m by p. (c) A multiplies itself to produce A2? Here m=n."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "44. To prove that (AB)C =A(BC), use the column vectors b ,...,b of B. First suppose 1 n thatC has only one column c with entries c ,...,c : 1 n AB has columns Ab ,...,Ab , and Bc has one column c b +···+c b . 1 n 1 1 n n Then (AB)c=c Ab +···+c Ab , equals A(c b +···+c b )=A(Bc). 1 1 n n 1 1 n n Linearity gives equality of those two sums, and (AB)c=A(Bc). The same is true for all other ofC. Therefore (AB)C =A(BC). Problems 45–49 use column-row multiplication and block multiplication."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "45. Multiply AB using columns times rows:     (cid:34) (cid:35) 1 0 1 (cid:104) (cid:105)   3 3 0   AB=2 4 =2 3 3 0 + = . 1 2 1 2 1 2"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "46. Block multiplication separates matrices into blocks (submatrices). If their shapes make block multiplication possible, then it is allowed. Replace these x’s by numbers and confirm that block multiplication succeeds.    (cid:34) (cid:35) x x x x x x (cid:104) (cid:105) (cid:104) (cid:105) C    A B = AC+BD and  x x x  x x x . D x x x x x x"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "47. Draw the cuts in A and B and AB to show how each of the four multiplication rules is really a block multiplication to find AB: (a) Matrix A times columns of B. (b) Rows of A times matrix B. (c) Rows of A times columns of B. (d) Columns of A times rows of B."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 MatrixNotationandMatrixMultiplication 35"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "48. Block multiplication says that elimination on column 1 produces (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 1 0 a b a b EA= = . −c/a I c D 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "49. Elimination for a 2 by 2 block matrix: When A−1A =I, multiply the first block row byCA−1 and subtract from the second row, to find the “Schur complement” S: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) I 0 A B A B = . −CA−1 I C D 0 S"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "50. With i2 = −1, the product (A+iB)(x+iy) is Ax+iBx+iAy−By. Use blocks to separate the real part from the imaginary part that multiplies i: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) A −B x Ax−By real part = ? ? y ? imaginary part"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "51. Suppose you solve Ax=b for three special right-hand sides b:       1 0 0       Ax =0 and Ax =1 and Ax =0. 1 2 3 0 0 1 If the solutions x , x , x are the columns of a matrix X, what is AX? 1 2 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "52. If the three solutions in Question 51 are x = (1,1,1) and x = (0,1,1) and x = 1 2 3 (0,0,1), solve Ax=b when b=(3,5,8). Challenge problem: What is A?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "53. Find all matrices (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) a b 1 1 1 1 A= that satisfy A = A. c d 1 1 1 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "54. If you multiply a northwest matrix A and a southeast matrix B, what type of matri- ces are AB and BA? “Northwest” and “southeast” mean zeros below and above the antidiagonal going from (1,n) to (n,1)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "55. Write 2x+3y+z+5t =8 as a matrix A (how many rows?) multiplying the column vector (x,y,z,t) to produce b. The solutions fill a plane in four-dimensional space. The plane is three-dimensional with no 4D volume."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "56. What 2 by 2 matrix P projects the vector (x,y) onto the x axis to produce (x,0)? 1 What matrix P projects onto the y axis to produce (0,y)? If you multiply (5,7) by 2 P and then multiply by P , you get ( ) and ( ). 1 2"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "57. Write the inner product of (1,4,5) and (x,y,z) as a matrix multiplication Ax. A has onerow. ThesolutionstoAx=0lieona perpendiculartothevector . The columns of A are only in -dimensional space."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "58. In MATLAB notation, write the commands that define the matrix A and the column vectors x and b. What command would test whether or not Ax=b? (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 2 5 1 A= x= b= 3 4 −2 7"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "59. The MATLAB commands A = eye(3) and v = [3:5]’ produce the 3 by 3 identity matrix and the column vector (3,4,5). What are the outputs from A ∗ v and v’ ∗ v? (Computer not needed!) If you ask for v ∗ A, what happens?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "60. If you multiply the 4 by 4 all-ones matrix A = ones(4,4) and the column v = ones(4,1), what is A ∗ v? (Computer not needed.) If you multiply B = eye(4) + ones(4,4) times w = zeros(4,1) + 2 ∗ ones(4,1), what is B ∗ w?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "61. Invent a 3 by 3 magic matrix M with entries 1,2,...,9. All rows and columns and diagonals add to 15. The first row could be 8, 3, 4. What is M times (1,1,1)? What (cid:104) (cid:105) is the row vector 1 1 1 times M?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.5 Triangular Factors and Row Exchanges We want to look again at elimination, to see what it means in terms of matrices. The starting point was the model system Ax=b:      2 1 1 u 5      Ax= 4 −6 0v=−2=b. (1) −2 7 2 w 9 Then there were three elimination steps, with multipliers 2, −1, −1: Step 1. Subtract 2 times the first equation from the second; Step 2. Subtract −1 times the first equation from the third; Step 3. Subtract −1 times the second equation from the third. The result was an equivalent systemUx=c, with a new coefficient matrixU:      2 1 1 u 5      Upper triangular Ux=0 −8 −2v=−12=c. (2) 0 0 1 w 2 This matrixU is upper triangular—all entries below the diagonal are zero. The new right side c was derived from the original vector b by the same steps that took A intoU. Forward elimination amounted to three row operations:"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.5 TriangularFactorsandRowExchanges 37 Start with A and b; Apply steps 1, 2, 3 in that order; End withU and c. Ux=c is solved by back-substitution. Here we concentrate on connecting A toU. The matrices E for step 1, F for step 2, and G for step 3 were introduced in the previous section. They are called elementary matrices, and it is easy to see how they work. To subtract a multiple (cid:96) of equation j from equation i, put the number −(cid:96) into the (i, j) position. Otherwise keep the identity matrix, with 1s on the diagonal and 0s elsewhere. Then matrix multiplication executes the row operation. The result of all three steps is GFEA =U. Note that E is the first to multiply A, then F, then G. We could multiply GFE together to find the single matrix that takes A toU (and also takes b to c). It is lower triangular (zeros are omitted):       1 1 1 1       From A toU GFE = 1  1 −2 1 =−2 1 . (3) 1 1 1 1 1 −1 1 1 This is good, but the most important question is exactly the opposite: How would we get fromU back to A? How can we undo the steps of Gaussian elimination? To undo step 1 is not hard. Instead of subtracting, we add twice the first row to the second. (Not twice the second row to the first!) The result of doing both the subtraction and the addition is to bring back the identity matrix:      Inverse of 1 0 0 1 0 0 1 0 0      subtraction 2 1 0−2 1 0=0 1 0. (4) is addition 0 0 1 0 0 1 0 0 1 One operation cancels the other. In matrix terms, one matrix is the inverse of the other. If the elementary matrix E has the number −(cid:96) in the (i, j) position, then its inverse E−1 has +(cid:96) in that position. Thus E−1E =I, which is equation (4). We can invert each step of elimination, by using E−1 and F−1 and G−1. I think it’s not bad to see these inverses now, before the next section. The final problem is to undo the whole process at once, and see what matrix takesU back to A. Since step 3 was last in going from A to U, its matrix G must be the first to be inverted in the reverse direction. Inverses come in the opposite order! The second reverse step is F−1 and the last is E−1: FromU back to A E−1F−1G−1U =A is LU =A. (5) You can substitute GFEA forU, to see how the inverses knock out the original steps. Now we recognize the matrix L that takes U back to A. It is called L, because it is lower triangular. And it has a special property that can be seen only by multiplying the three inverse matrices in the right order:       1 1 1 1       E−1F−1G−1 =2 1  1  1 = 2 1 =L. (6) 1 −1 1 −1 1 −1 −1 1 The special thing is that the entries below the diagonal are the multipliers (cid:96) = 2, −1, and −1. When matrices are multiplied, there is usually no direct way to read off the answer. Here the matrices come in just the right order so that their product can be written down immediately. If the computer stores each multiplier (cid:96) —the number that ij multiplies the pivot row j when it is subtracted from row i, and produces a zero in the i, j position—then these multipliers give a complete record of elimination. The numbers (cid:96) fit right into the matrix L that takesU back to A. ij 1H Triangular factorization A=LU with no exchanges of rows. L is lower triangular,with1sonthediagonal. Themultipliers(cid:96) (takenfromelimination) ij are below the diagonal. U is the upper triangular matrix which appears after forward elimination, The diagonal entries ofU are the pivots. Example 1. (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 2 1 2 1 0 A= goes toU = with L= . Then LU =A. 3 8 0 2 3 1 Example 2. (which needs a row exchange) (cid:34) (cid:35) 0 2 A= cannot be factored into A=LU. 3 4 Example 3. (with all pivots and multipliers equal to 1)      1 1 1 1 0 0 1 1 1      A=1 2 2=1 1 00 1 1=LU. 1 2 3 1 1 1 0 0 1 From A toU there are subtractions of rows. FromU to A there are additions of rows. Example 4. (whenU is the identity and L is the same as A)   1 0 0   Lower triangular case A=(cid:96) 1 0. 21 (cid:96) (cid:96) 1 31 32 The elimination steps on this A are easy: (i) E subtracts (cid:96) times row 1 from row 2, (ii) 21 F subtracts(cid:96) timesrow1fromrow3,and(iii)Gsubtracts(cid:96) timesrow2fromrow3. 31 32 The result is the identity matrixU =I. The inverses of E, F, and G will bring back A:"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.5 TriangularFactorsandRowExchanges 39 E−1 applied to F−1 applied to G−1 applied to I produces A.         1 1 1 1 0 0         (cid:96) 1  times  1  times  1  equals (cid:96) 1 0. 21 21 1 (cid:96) 1 (cid:96) 1 (cid:96) (cid:96) 1 31 32 31 32 The order is right for the (cid:96)’s to fall into position. This always happens! Note that parentheses in E−1F−1G−1 were not necessary because of the associative law. A=LU: The n by n case The factorization A = LU is so important that we must say more. It used to be missing in linear algebra courses when they concentrated on the abstract side. Or maybe it was thought to be too hard—but you have got it. If the last Example 4 allows anyU instead oftheparticularU =I,wecanseehowtheruleworksingeneral. ThematrixL,applied toU, brings back A:    1 0 0 row 1 ofU    A=LU (cid:96) 1 0row 2 ofU=original A. (7) 21 (cid:96) (cid:96) 1 row 3 ofU 31 32 The proof is to apply the steps of elimination. On the right-hand side they take A toU. On the left-hand side they reduce L to I, as in Example 4. (The first step subtracts (cid:96) 21 times (1,0,0) from the second row, which removes (cid:96) .) Both sides of (7) end up equal 21 tothesamematrixU,andthestepstogetthereareallreversible. Therefore(7)iscorrect and A=LU. A = LU is so crucial, and so beautiful, that Problem 8 at the end of this section suggests a second approach. We are writing down 3 by 3 matrices, but you can see how the arguments apply to larger matrices. Here we give one more example, and then put A=LU to use. Example 5. (A=LU, with zeros in the empty spaces)      1 −1 1 1 −1      −1 2 −1  −1 1  1 −1  A= =  .  −1 2 −1  −1 1  1 −1 −1 2 −1 1 1 ThatshowshowamatrixAwiththreediagonalshasfactorsLandU withtwodiagonals. This example comes from an important problem in differential equations (Section 1.7). The second difference in A is a backward difference L times a forward differenceU. One Linear System = Two Triangular Systems There is a serious practical point about A = LU. It is more than just a record of elimi- nation steps; L andU are the right matrices to solve Ax = b. In fact A could be thrown away! We go from b to c by forward elimination (this uses L) and we go from c to x by back-substitution (that usesU). We can and should do it without A: Splitting of Ax=b First Lc=b and then Ux=c. (8) Multiply the second equation by L to give LUx = Lc, which is Ax = b. Each triangular system is quickly solved. That is exactly what a good elimination code will do:"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. Factor (from A find its factors L andU)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. Solve (from L andU and b find the solution x). TheseparationintoFactorandSolvemeansthataseriesofb’scanbeprocessed. The Solve subroutine obeys equation (8): two triangular systems in n2/2 steps each. The solution for any new right-hand side b can be found in only n2 operations. That is far below the n3/3 steps needed to factor A on the left-hand side. Example 6. This is the previous matrix A with a right-hand side b=(1,1,1,1). x − x = 1 1 2 −x + 2x − x = 1 1 2 3 Ax=b splits into Lc=b andUx=c. − x + 2x − x = 1 2 3 4 − x + 2x = 1 3 4   c = 1 1 1   −c + c = 1 2 1 2 Lc=b gives c= . − c + c = 1 3 2 3 − c + c = 1 4 3 4   x − x = 1 10 1 2   x − x = 2  9  2 3 Ux=c gives x= . x − x = 3  7  3 4 x = 4 4 4 For these special “tridiagonal matrices,” the operation count drops from n2 to 2n. You see how Lc=b is solved forward (c comes before c ). This is precisely what happens 1 2 during forward elimination. ThenUx=c is solved backward (x before x ). 4 3 Remark 1. The LU form is “unsymmetric” on the diagonal: L has 1s where U has the"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.5 TriangularFactorsandRowExchanges 41 pivots. This is easy to correct. Divide out ofU a diagonal pivot matrix D:     . . d 1 u /d u /d . 1 12 1 13 1     . Factor out D U =  d 2    1 u 23/d 2 . .  . (9)  ...   ... . . .  d n 1 In the last example all pivots were d = 1. In that case D = I. But that was very excep- i tional, and normally LU is different from LDU (also written LDV). The triangular factorization can be written A=LDU, where L andU have 1s on the diagonal and D is the diagonal matrix of pivots. Whenever you see LDU or LDV, it is understood that U or V has is on the diagonal— each row was divided by the pivot in D. Then L andU are treated evenly. An example of LU splitting into LDU is (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) 1 2 1 1 2 1 1 1 2 A= = = =LDU. 3 4 3 1 −2 3 1 −2 1 That has the 1s on the diagonals of L andU, and the pivots 1 and −2 in D. Remark 2. We may have given the impression in describing each elimination step, that the calculations must be done in that order. This is wrong. There is some freedom, and there is a “Crout algorithm” that arranges the calculations in a slightly different way. There is no freedom in the final L, D, andU. That is our main point: 1I If A = L D U and also A = L D U , where the L’s are lower triangular 1 1 1 2 2 2 with unit diagonal, the U’s are upper triangular with unit diagonal, and the D’s are diagonal matrices with no zeros on the diagonal, then L = L , D = 1 2 1 D , U =U . The LDU factorization and the LU factorization are uniquely 2 1 2 determined by A. The proof is a good exercise with inverse matrices in the next section. Row Exchanges and Permutation Matrices We now have to face a problem that has so far been avoided: The number we expect to use as a pivot might be zero. This could occur in the middle of a calculation. It will happen at the very beginning if a =0. A simple example is 11 (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 0 2 u b 1 Zero in the pivot position = . 3 4 v b 2 The difficulty is clear; no multiple of the first equation will remove the coefficient 3. The remedy is equally clear. Exchange the two equations, moving the entry 3 up into the pivot. In this example the matrix would become upper triangular: 3u+4v = b 2 Exchange rows 2v = b 1 To express this in matrix terms, we need the permutation matrix P that produces the row exchange. It comes from exchanging the rows of I: (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 0 1 0 1 0 2 3 4 Permutation P= and PA= = . 1 0 1 0 3 4 0 2 P has the same effect on b, exchanging b and b . The new system is PAx = Pb. The 1 2 unknowns u and v are not reversed in a row exchange. A permutation matrix P has the same rows as the identity (in some order). There is asingle“1”ineveryrowandcolumn. ThemostcommonpermutationmatrixisP=I (it exchanges nothing). The product of two permutation matrices is another permutation— the rows of I get reordered twice. After P = I, the simplest permutations exchange two rows. Other permutations ex- changemorerows. Therearen!=(n)(n−1)···(1)permutationsofsizen. Row1has n choices, then row 2 has n−1 choices, and finally the last row has only one choice. We can display all 3 by 3 permutations (there are 3!=(3)(2)(1)=6 matrices):       1 1 1       I = 1  P =1  P P = 1 21 32 21 1 1 1       1 1 1       P = 1  P = 1 P P =1 . 31 32 21 32 1 1 1 There will be 24 permutation matrices of order n = 4. There are only two permutation matrices of order 2, namely (cid:34) (cid:35) (cid:34) (cid:35) 1 0 0 1 and . 0 1 1 0 When we know about inverses and transposes (the next section defines A−1 and AT), we discover an important fact: P−1 is always the same as PT. A zero in the pivot location raises two possibilities: The trouble may be easy to fix, or it may be serious. This is decided by looking below the zero. If there is a nonzero entry lower down in the same column, then a row exchange is carried out. The nonzero entry becomes the needed pivot, and elimination can get going again:   0 a b d =0 =⇒ no first pivot   A=0 0 c a=0 =⇒ no second pivot d e f c=0 =⇒ no third pivot."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. When is an upper triangular matrix nonsingular (a full set of pivots)?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. What multiple (cid:96) of row 2 of A will elimination subtract from row 3 of A? Use the 32 factored form    1 0 0 5 7 8    A=2 1 00 2 3. 1 4 1 0 0 6 What will be the pivots? Will a row exchange be required?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. Multiply the matrix L=E−1F−1G−1 in equation (6) by GFE in equation (3):     1 0 0 1 0 0      2 1 0 times −2 1 0. −1 −1 1 −1 1 1 Multiply also in the opposite order. Why are the answers what they are?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.5 TriangularFactorsandRowExchanges 45"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "4. Apply elimination to produce the factors L andU for     (cid:34) (cid:35) 3 1 1 1 1 1 2 1     A= and A=1 3 1 and A=1 4 4. 8 7 1 1 3 1 4 8"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "5. Factor A into LU, and write down the upper triangular systemUx=c which appears after elimination, for      2 3 3 u 2      Ax=0 5 7v=2. 6 9 8 w 5"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "6. Find E2 and E8 and E−1 if (cid:34) (cid:35) 1 0 E = . 6 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "7. Find the products FGH and HGF if (with upper triangular zeros omitted)       1 1 1       2 1  0 1  0 1  F =  G=  H = . 0 0 1  0 2 1  0 0 1  0 0 0 1 0 0 0 1 0 0 2 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "8. (Second proof of A = LU) The third row of U comes from the third row of A by subtracting multiples of rows 1 and 2 (of U!): row 3 ofU =row 3 of A−(cid:96) (row 1 ofU)−(cid:96) (row 2 ofU). 31 32 (a) Why are rows of U subtracted off and not rows of A? Answer: Because by the time a pivot row is used, . (b) The equation above is the same as row 3 of A=(cid:96) (row 1 ofU)+(cid:96) (row 2 ofU)+1(row 3 ofU). 31 32 Which rule for matrix multiplication makes this row 3 of L timesU? The other rows of LU agree similarly with the rows of A."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "9. (a) Under what conditions is the following product nonsingular?     1 0 0 d 1 −1 0 1     A=−1 1 0 d 0 1 −1. 2 0 −1 1 d 0 0 1 3 (b) Solve the system Ax=b starting with Lc=b:      1 0 0 c 0 1      −1 1 0c =0=b. 2 0 −1 1 c 1 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "10. (a) Why does it take approximately n2/2 multiplication-subtraction steps to solve each of Lc=b andUx=c? (b) Howmanystepsdoeseliminationuseinsolving10systemswith thesame60by 60 coefficient matrix A?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "11. Solve as two triangular systems, without multiplying LU to find A:       1 0 0 2 4 4 u 2       LUx=1 1 00 1 2v=0. 1 0 1 0 0 1 w 2"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "12. How could you factor A into a productUL, upper triangular times lower triangular? Would they be the same factors as in A=LU?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "13. Solve by elimination, exchanging rows when necessary: u + 4v + 2w = −2 v + w = 0 −2u − 8v + 3w = 32 and u + v = 0 v + w = 1 u + v + w = 1. Which permutation matrices are required?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "14. Writedownallsixofthe3by3permutationmatrices,includingP=I. Identifytheir inverses,whicharealsopermutationmatrices. TheinversessatisfyPP−1 =I andare on the same list."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "15. Find the PA=LDU factorizations (and check them) for     0 1 1 1 2 1     A=1 0 1 and A=2 4 2. 2 3 4 1 1 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "16. Find a 4 by 4 permutation matrix that requires three row exchanges to reach the end of elimination (which isU =I)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "17. The less familiar form A=LPU exchanges rows only at the end:        1 1 1 1 1 1 1 0 0 1 1 1        A=1 1 3→L−1A=0 0 2=PU =0 0 10 3 6. 2 5 8 0 3 6 0 1 0 0 0 2 WhatisL isthiscase? Comparingwith PA=LU inBox1J,themultipliers nowstay in place ((cid:96) is 1 and (cid:96) is 2 when A=LPU). 21 31"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "18. Decide whether the following systems are singular or nonsingular, and whether they have no solution, one solution, or infinitely many solutions: v − w = 2 v − w = 0 v + w = 1 u − v = 2 and u − v = 0 and u + v = 1 u − w = 2 u − w = 0 u + w = 1."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.5 TriangularFactorsandRowExchanges 47"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "19. Which numbers a, b, c lead to row exchanges? Which make the matrix singular?   (cid:34) (cid:35) 1 2 0   c 2 A=a 8 3 and A= . 6 4 0 b 5 Problems 20–31 compute the factorization A=LU (and also A=LDU). (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "20. Forward elimination changes 1 1 x=b to a triangular 1 1 x=c: 1 2 0 1 (cid:34) (cid:35) (cid:34) (cid:35) x + y = 5 x + y = 5 1 1 5 1 1 5 → → . x + 2y = 7 y = 2 1 2 7 0 1 2 That step subtracted (cid:96) = times row 1 from row 2. The reverse step adds (cid:96) 21 21 times row 1 to row 2. The matrix for that reverse step is L = . Multiply this L (cid:163) (cid:164) (cid:163) (cid:164) times the triangular system 1 1 x= 5 to get = . In letters, L multiplies 0 1 2 Ux=c to give ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "21. (Move to 3 by 3) Forward elimination changes Ax=b to a triangularUx=c: x+y+z=5 x+y+z=5 x+y+z=5 x+2y+3z=7 y+2z=2 y+2z=2 x+3y+6z=11 2y+5z=6 z=2. The equation z=2 inUx=c comes from the original x+3y+6z=11 in Ax=b by subtracting (cid:96) = times equation 1 and (cid:96) = times the final equation 2. 31 32 Reverse that to recover [1 3 6 11] in [A b] from the final [1 1 1 5] and [0 1 2 2] and [0 0 1 2] in [U c]: (cid:104) (cid:105) (cid:104) (cid:105) Row 3 of A b =((cid:96) Row 1 +(cid:96) Row 2+1 Row 3) of U c . 31 32 In matrix notation this is multiplication by L. So A=LU and b=Lc."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "22. What are the 3 by 3 triangular systems Lc=b andUx=c from Problem 21? Check that c=(5,2,2) solves the first one. Which x solves the second one?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "23. WhattwoeliminationmatricesE andE putAintouppertriangularformE E A= 21 32 32 21 U? Multiply by E−1 and E−1 to factor A into LU =E−1E−1U: 31 21 21 32   1 1 1   A=2 4 5. 0 4 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "24. What three elimination matrices E , E , E put A into upper triangular form 21 31 32 E E E A =U? Multiply by E−1, E−1 and E−1 to factor A into LU where L = 32 31 21 32 31 21 E−1E−1E−1. Find L andU: 21 31 32   1 0 1   A=2 2 2. 3 4 5"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "25. When zero appears in a pivot position, A = LU is not possible! (We need nonzero pivots d, f, i inU.) Show directly why these are both impossible:      (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) 1 1 0 1 d e g 0 1 1 0 d e      = 1 1 2=(cid:96) 1  f h. 2 3 (cid:96) 1 0 f 1 2 1 m n 1 i"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "26. Which number c leads to zero in the second pivot position? A row exchange is neededandA=LU isnotpossible. Whichcproduceszerointhethirdpivotposition? Then a row exchange can’t help and elimination fails:   1 c 0   A=2 4 1. 3 5 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "27. What are L and D for this matrix A? What isU in A=LU and what is the newU in A=LDU?   2 4 8   A=0 3 9. 0 0 7"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "28. A and B are symmetric across the diagonal (because 4 =4). Find their triple factor- izations LDU and say howU is related to L for these symmetric matrices:   (cid:34) (cid:35) 1 4 0 2 4   A= and B=4 12 4. 4 11 0 4 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "29. (Recommended) Compute L andU for the symmetric matrix   a a a a   a b b b A= . a b c c a b c d Find four conditions on a, b, c, d to get A=LU with four pivots."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "30. Find L andU for the nonsymmetric matrix   a r r r   a b s s A= . a b c t a b c d Find the four conditions on a, b, c, d, r, s,t to get A=LU with four pivots."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.5 TriangularFactorsandRowExchanges 49"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "31. Tridiagonal matrices have zero entries except on the main diagonal and the two adjacent diagonals. Factor these into A=LU and A=LDV:     1 1 0 a a 0     A=1 2 1 and A=a a+b b . 0 1 2 0 b b+c"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "32. Solve the triangular system Lc=b to find c. Then solveUx=c to find x: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 2 4 2 L= and U = and b= . 4 1 0 1 11 For safety find A=LU and solve Ax=b as usual. Circle c when you see it."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "33. Solve Lc=b to find c. Then solveUx=c to find x. What was A?       1 0 0 1 1 1 4       L=1 1 0 and U =0 1 1 and b=5. 1 1 1 0 0 1 6"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "34. If A and B have nonzeros in the positions marked by x, which zeros are still zero in their factors L andU?     x x x x x x x 0     x x x 0 x x 0 x A=  and B= . 0 x x x x 0 x x 0 0 x x 0 x x x"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "35. (Important) If A has pivots 2, 7, 6 with no row exchanges, what are the pivots for the upper left 2 by 2 submatrix B (without row 3 and column 3)? Explain why."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "36. Starting from a 3 by 3 matrix A with pivots 2, 7, 6, add a fourth row and column to produce M. What are the first three pivots for M, and why? What fourth row and column are sure to produce 9 as the fourth pivot?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "37. Use chol(pascal(5)) to find the triangular factors of MATLAB’s pascal(5). Row exchanges in [L, U] = lu(pascal(5)) spoil Pascal’s pattern!"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "38. (Review) For which numbers c is A=LU impossible—with three pivots?   1 2 0   A=3 c 1. 0 1 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "39. Estimate the time difference for each new right-hand side b when n=800. Create A = rand(800) and b = rand(800,1) and B = rand(800,9). Compare the times from tic; A\\b; toc and tic; A\\B; toc (which solves for 9 right sides). Problems 40–48 are about permutation matrices."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "40. There are 12 “even” permutations of (1,2,3,4), with an even number of exchanges. Two of them are (1,2,3,4) with no exchanges and (4,3,2,1) with two exchanges. List the other ten. Instead of writing each 4 by 4 matrix, use the numbers 4, 3, 2, 1 to give the position of the 1 in each row."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "41. How many exchanges will permute (5,4,3,2,1) back to (1,2,3,4,5)? How many exchanges to change (6,5,4,3,2,1) to (1,2,3,4,5,6)? One is even and the other is odd. For (n,...,1) to (1,...,n), show that n = 100 and 101 are even, n = 102 and 103 are odd."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "42. If P and P are permutation matrices, so is P P . This still has the rows of I in some 1 2 1 2 order. Give examples with P P (cid:54)=P P and P P =P P . 1 2 2 1 3 4 4 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "43. (Try this question.) Which permutation makes PA upper triangular? Which permu- tations make P AP lower triangular? Multiplying A on the right by P exchanges 1 2 2 the of A.   0 0 6   A=1 2 3 0 4 5"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "44. Find a 3 by 3 permutation matrix with P3 =I (but not P=I). Find a 4 by 4 permu- tation P(cid:98)with P(cid:98)4 (cid:54)=I."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "45. If you take powers of a permutation, why is some Pk eventually equal to I? Find a 5 by 5 permutation P so that the smallest power to equal I is P6. (This is a challenge question. Combine a 2 by 2 block with a 3 by 3 block.)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "46. The matrix P that multiplies (x,y,z) to give (z,x,y) is also a rotation matrix. Find P and P3. The rotation axis a=(1,1,1) doesn’t move, it equals Pa. What is the angle of rotation from v=(2,3,−5) to Pv=(−5,2,3)?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "47. If P is any permutation matrix, find a nonzero vector x so that (I−P)x = 0. (This will mean that I−P has no inverse, and has determinant zero.)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "48. If P has 1s on the antidiagonal from (1,n) to (n,1), describe PAP."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 Inverses and Transposes The inverse of an n by n matrix is another n by n matrix. The inverse of A is written A−1 (and pronounced “A inverse”). The fundamental property is simple: If you multiply by A and then multiply by A−1, you are back where you started: Inverse matrix If b=Ax then A−1b=x."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 InversesandTransposes 51 Thus A−1Ax = x. The matrix A−1 times A is the identity matrix. Not all matrices have inverses. An inverse is impossible when Ax is zero and x is nonzero. Then A−1 would have to get back from Ax = 0 to x. No matrix can multiply that zero vector Ax and produce a nonzero vector x. Our goals are to define the inverse matrix and compute it and use it, when A−1 exists—and then to understand which matrices don’t have inverses. 1K The inverse of A is a matrix B such that BA = I and AB = I. There is at most one such B, and it is denoted by A−1: A−1A=I and AA−1 =I. (1) Note 1. The inverse exists if and only if elimination produces n pivots (row exchanges allowed). Elimination solves Ax=b without explicitly finding A−1. Note 2. The matrix A cannot have two different inverses, Suppose BA = I and also AC =I. Then B=C, according to this “proof by parentheses”: B(AC)=(BA)C gives BI =IC which is B=C. (2) This shows that a left-inverse B (multiplying from the left) and a right-inverseC (multi- plying A from the right to give AC =I) must be the same matrix. Note 3. If A is invertible, the one and only solution to Ax=b is x=A−1b: Multiply Ax=b by A−1. Then x=A−1Ax=A−1b. Note 4. (Important) Suppose there is a nonzero vector x such that Ax = 0. Then A cannot have an inverse. To repeat: No matrix can bring 0 back to x. If A is invertible, then Ax=0 can only have the zero solution x=0. Note 5. A 2 by 2 matrix is invertible if and only if ad−bc is not zero: (cid:34) (cid:35) (cid:34) (cid:35) −1 a b 1 d −b 2 by 2 inverse = . (3) c d ad−bc −c a This number ad−bc is the determinant of A. A matrix is invertible if its determinant is not zero (Chapter 4). In MATLAB, the invertibility test is to find n nonzero pivots. Elimination produces those pivots before the determinant appears. Note 6. A diagonal matrix has an inverse provided no diagonal entries are zero:     d 1/d 1 1 If A=  ...   then A−1 =  ...   and AA−1 =I. d 1/d n n When two matrices are involved, not much can be done about the inverse of A+B. The sum might or might not be invertible. Instead, it is the inverse of their product AB which is the key formula in matrix computations. Ordinary numbers are the same: (a+b)−1 is hard to simplify, while 1/ab splits into 1/a times 1/b. But for matrices the order of multiplication must be correct—if ABx = y then Bx = A−1y and x = B−1A−1y. The inverses come in reverse order. 1L A product AB of invertible matrices is inverted by B−1A−1: Inverse of AB (AB)−1 =B−1A−1. (4) Proof. ToshowthatB−1A−1 istheinverseofAB, wemultiplythemandusetheassocia- tive law to remove parentheses. Notice how B sits next to B−1: (AB)(B−1A−1)=ABB−1A−1 =AIA−1 =AA−1 =I (B−1A−1)(AB)=B−1A−1AB=B−1IB=B−1B=I. A similar rule holds with three or more matrices: Inverse of ABC (ABC)−1 =C−1B−1A−1. We saw this change of order when the elimination matrices E, F, G were inverted to come back from U to A. In the forward direction, GFEA was U. In the backward direction, L = E−1F−1G−1 was the product of the inverses. Since G came last, G−1 comes first. Please check that A−1 would beU−1GFE. The Calculation of A−1: The Gauss-Jordan Method Consider the equation AA−1 = I. If it is taken a column at a time, that equation de- termines each column of A−1. The first column of A−1 is multiplied by A, to yield the first column of the identity: Ax = e . Similarly Ax = e and Ax = e the e’s are the 1 1 2 2 3 3 columns of I. In a 3 by 3 example, A times A−1 is I:     2 1 1 1 0 0 (cid:104) (cid:105) (cid:104) (cid:105)     Ax =e  4 −6 0 x x x = e e e =0 1 0. (5) i i 1 2 3 1 2 3 −2 7 2 0 0 1 Thus we have three systems of equations (or n systems). They all have the same coeffi- cient matrix A. The right-hand sides e , e , e are different, but elimination is possible 1 2 3 on all systems simultaneously. This is the Gauss-Jordan method. Instead of stopping at U and switching to back-substitution, it continues by subtracting multiples of a row from the rows above. This produces zeros above the diagonal as well as below. When it reaches the identity matrix we have found A−1. The example keeps all three columns e , e , e , and operates on rows of length six: 1 2 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 InversesandTransposes 53 Example 1. Using the Gauss-Jordan Method to Find A−1   2 1 1 1 0 0 (cid:104) (cid:105)   A e e e = 4 −6 0 0 1 0 1 2 3 −2 7 2 0 0 1   2 1 1 1 0 0   pivot=2→0 −8 −2 −2 1 0 0 8 3 1 0 1   2 1 1 1 0 0 (cid:104) (cid:105)   pivot=−8→0 −8 −2 −2 1 0= U L−1 . 0 0 1 −1 1 1 This completes the first half—forward elimination. The upper triangular U appears in the first three columns. The other three columns are the same as L−1. (This is the effect ofapplyingtheelementaryoperationsGFE totheidentitymatrix.) Nowthesecondhalf will go from U to I (multiplying by U−1). That takes L−1 to U−1L−1 which is A−1. Creating zeros above the pivots, we reach A−1:   2 1 0 2 −1 −1 (cid:104) (cid:105)   Second half U L−1 →0 −8 0 −4 3 2  0 0 1 −1 1 1   2 0 0 12 −5 −6 8 8 8   zeros above pivots→0 −8 0 −4 3 2  0 0 1 −1 1 1   1 0 0 12 − 5 − 6 (cid:104) (cid:105) 16 16 16   divide by pivots→0 1 0 4 −3 −2 = I A−1 . 8 8 8 0 0 1 −1 1 1 At the last step, we divided the rows by their pivots 2 and −8 and 1. The coefficient matrix in the left-hand half became the identity. Since A went to I, the same operations on the right-hand half must have carried I into A−1. Therefore we have computed the inverse. Anoteforthefuture: Youcanseethedeterminant−16appearinginthedenominators of A−1. The determinant is the product of the pivots (2)(−8)(1). It enters at the end when the rows are divided by the pivots. Remark 1. In spite of this brilliant success in computing A−1, I don’t recommend it, I admit that A−1 solves Ax=b in one step. Two triangular steps are better: x=A−1b separates into Lc=b and Ux=c. We could write c = L−1b and then x = U−1c = U−1L−1b. But note that we did not explicitlyform,andinactualcomputationshouldnotform,thesematricesL−1 andU−1. It would be a waste of time, since we only need back-substitution for x (and forward substitution produced c). A similar remark applies to A−1; the multiplication A−1b would still take n2 steps. It is the solution that we want, and not all the entries in the inverse. Remark 2. Purely out of curiosity, we might count the number of operations required to find A−1. The normal count for each new right-hand side is n2, half in the forward direction and half in back-substitution. With n right-hand sides e ,...,e this makes n3. 1 n After including the n3/3 operations on A itself, the total seems to be 4n3/3. This result is a little too high because of the zeros in the e . Forward elimination j changes only the zeros below the 1. This part has only n− j components, so the count for e is effectively changed to (n− j)2/2. Summing over all j, the total for forward j elimination is n3/6. This is to be combined with the usual n3/3 operations that are appliedtoA, andthen(n2/2)back-substitutionstepsthatfinallyproducethecolumnsx j of A−1. The final count of multiplications for computing A−1 is n3: (cid:181) (cid:182) n3 n3 n2 Operation count + +n =n3. 6 3 2 This count is remarkably low. Since matrix multiplication already takes n3 steps, it requires as many operations to compute A2 as it does to compute A−1! That fact seems almost unbelievable (and computing A3 requires twice as many, as far as we can see). Nevertheless, if A−1 is not needed, it should not be computed. Remark 3. In the Gauss-Jordan calculation we went all the way forward to U, before starting backward to produce zeros above the pivots. That is like Gaussian elimination, but other orders are possible. We could have used the second pivot when we were there earlier, to create a zero above it as well as below it. This is not smart. At that time the second row is virtually full, whereas near the end it has zeros from the upward row operations that have already taken place. Invertible = Nonsingular (n pivots) Ultimately we want to know which matrices are invertible and which are not. This question is so important that it has many answers. See the last page of the book! Eachofthefirstfivechapterswillgiveadifferent(butequivalent)testforinvertibility. Sometimes the tests extend to rectangular matrices and one-sided inverses: Chapter 2 looks for independent rows and independent columns, Chapter 3 inverts AAT or ATA. The other chapters look for nonzero determinants or nonzero eigenvalues or nonzero pivots. This last test is the one we meet through Gaussian elimination. We want to show (in a few theoretical paragraphs) that the pivot test succeeds. Suppose A has a full set of n pivots. AA−1 = I gives n separate systems Ax = e i i for the columns of A−1. They can be solved by elimination or by Gauss-Jordan. Row exchanges may be needed, but the columns of A−1 are determined."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 InversesandTransposes 55 Strictly speaking, we have to show that the matrix A−1 with those columns is also a left-inverse. Solving AA−1 = I has at the same time solved A−1A = I, but why? A 1-sided inverse of a square matrix is automatically a 2-sided inverse. To see why, notice that every Gauss-Jordan step is a multiplication on the left by an elementary matrix. We are allowing three types of elementary matrices:"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. E to subtract a multiple (cid:96) of row j from row i ij"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. P to exchange rows i and j ij"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. D (or D−1) to divide all rows by their pivots. The Gauss-Jordan process is really a giant sequence of matrix multiplications: (D−1···E···P···E)A=I. (6) That matrix in parentheses, to the left of A, is evidently a left-inverse! It exists, it equals the right-inverse by Note 2, so every nonsingular matrix is invertible. The converse is also true: If A is invertible, it has n pivots. In an extreme case that is clear: A cannot have a whole column of zeros. The inverse could never multiply a column of zeros to produce a column of I. In a less extreme case, suppose elimination starts on an invertible matrix A but breaks down at column 3:   d x x x 1   Breakdown 0 d x x A(cid:48) = 2 . No pivot in column 3 0 0 0 x 0 0 0 x This matrix cannot have an inverse, no matter what the x’s are. One proof is to use column operations (for the first time?) to make the whole third column zero. By sub- tracting multiples of column 2 and then of column 1, we reach a matrix that is certainly not invertible. Therefore the original A was not invertible. Elimination gives a complete test: An n by n matrix is invertible if and only if it has n pivots. The Transpose Matrix We need one more matrix, and fortunately it is much simpler than the inverse. The transpose of A is denoted by AT. Its columns are taken directly from the rows of A—the ith row of A becomes the ith column of AT:   (cid:34) (cid:35) 2 0 2 1 4   Transpose If A= then AT =1 0. 0 0 3 4 3 AtthesametimethecolumnsofAbecometherowsofAT,IfAisanmbynmatrix,then AT is n by m. The final effect is to flip the matrix across its main diagonal, and the entry in row i, column j of AT comes from row j, column i of A: Entries of AT (AT) =A . (7) ij ji Thetransposeofalowertriangularmatrixisuppertriangular. ThetransposeofAT brings us back to A. If we add two matrices and then transpose, the result is the same as first transposing andthenadding: (A+B)T isthesameasAT+BT. Butwhatisthetransposeofaproduct AB or an inverse A−1? Those are the essential formulas of this section: 1M (i) The transpose of AB is (AB)T =BTAT, (ii) The transpose of A−1 is (A−1)T =(AT)−1. Notice how the formula for (AB)T resembles the one for (AB)−1. In both cases we reverse the order, giving BTAT and B−1A−1. The proof for the inverse was easy, but this one requires an unnatural patience with matrix multiplication. The first row of (AB)T is the first column of AB. So the columns of A are weighted by the first column of B. This amounts to the rows of AT weighted by the first row of BT. That is exactly the first row of BTAT. The other rows of (AB)T and BTAT also agree. (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 1 0 3 3 3 3 3 3 Start from AB= = 1 1 2 2 2 5 5 5     (cid:34) (cid:35) 3 2 3 5   1 1   Transpose to BTAT =3 2 =3 5. 0 1 3 2 3 5 To establish the formula for (A−1)T, start from AA−1 = I and A−1A = I and take trans- poses. On one side, IT = I. On the other side, we know from part (i) the transpose of a product. You see how (A−1)T is the inverse of AT, proving (ii): Inverse of AT =Transpose of A−1 (A−1)TAT =I. (8) Symmetric Matrices With these rules established, we can introduce a special class of matrices, probably the most important class of all. A symmetric matrix is a matrix that equals its own transpose: AT = A. The matrix is necessarily square. Each entry on one side of the diagonalequalsits“mirrorimage”ontheotherside: a =a . Twosimpleexamplesare ij ji A and D (and also A−1): (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 2 1 0 1 8 −2 Symmetric matrices A= and D= and A−1= . 2 8 0 4 4 −2 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. Find the inverses (no special system required) of (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 0 2 2 0 cosθ −sinθ A = , A = , A = . 1 2 3 3 0 4 2 sinθ cosθ"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. (a) Find the inverses of the permutation matrices     0 0 1 0 0 1     P=0 1 0 and P=1 0 0. 1 0 0 0 1 0 (b) Explain for permutations why P−1 is always the same as PT. Show that the 1s are in the right places to give PPT =I."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. From AB=C find a formula for A−1. Also find A−1 from PA=LU."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "4. (a) If A is invertible and AB=AC, prove quickly that B=C. (b) If A=[1 0], find an example with AB=AC but B(cid:54)=C. 0 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "5. If the inverse of A2 is B, show that the inverse of A is AB. (Thus A is invertible whenever A2 is invertible.)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "6. Use the Gauss-Jordan method to invert       1 0 0 2 −1 0 0 0 1       A =1 1 1, A =−1 2 −1, A =0 1 1. 1 2 3 0 0 1 0 −1 2 1 1 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "7. Find three 2 by 2 matrices, other than A=I and A=−I, that are their own inverses: A2 =I."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "8. Show that A=[1 1] has no inverse by solving Ax=0, and by failing to solve 3 3 (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 1 1 a b 1 0 = . 3 3 c d 0 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "9. Suppose elimination fails because there is no pivot in column 3:   2 1 4 6   0 3 8 5 Missing pivot A= . 0 0 0 7 0 0 0 9"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 InversesandTransposes 59 Show that A cannot be invertible. The third row of A−1, multiplying A, should give the third row [0 0 1 0] of A−1A=I. Why is this impossible?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "10. Find the inverses (in any legal way) of       0 0 0 1 1 0 0 0 a b 0 0       0 0 2 0 −1 1 0 0 c d 0 0 A = , A = 2 , A = . 1 0 3 0 0 2  0 −2 1 0 3 0 0 a b 3 4 0 0 0 0 0 −3 1 0 0 c d 4"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "11. Give examples of A and B such that (a) A+B is not invertible although A and B are invertible. (b) A+B is invertible although A and B are not invertible. (c) all of A, B, and A+B are invertible. (d) In the last case use A−1(A+B)B−1 =B−1+A−1 to show thatC =B−1+A−1 is also invertible—and find a formula forC−1."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "12. If A is invertible, which properties of A remain true for A−1? (a) A is triangular. (b) A is symmetric. (c) A is tridiagonal. (d) All entries are whole numbers. (e) All entries are fractions (including numbers like 3). 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "13. If A=[3] and B=[2], compute ATB, BTA, ABT, and BAT. 1 2"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "14. If B is square, show that A=B+BT is always symmetric and K =B−BT is always skew-symmetric—which means that KT = −K. Find these matrices A and K when B = [1 3], and write B as the sum of a symmetric matrix and a skew-symmetric 1 1 matrix."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "15. (a) How many entries can be chosen independently in a symmetric matrix of order n? (b) How many entries can be chosen independently in a skew-symmetric matrix (KT =−K) of order n? The diagonal of K is zero!"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "16. (a) If A = LDU, with 1s on the diagonals of L and U, what is the corresponding factorizationofAT? NotethatAandAT (squarematriceswithnorowexchanges) share the same pivots. (b) What triangular systems will give the solution to ATy=b?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "17. If A=L D U and A=L D U , prove that L =L , D =D , andU =U . If A is 1 1 1 2 2 2 1 2 1 2 1 2 invertible, the factorization is unique. (a) Derive the equation L−1L D = D U U−1, and explain why one side is lower 1 2 2 1 1 2 triangular and the other side is upper triangular. (b) Compare the main diagonals and then compare the off-diagonals."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "18. Under what conditions on their entries are A and B invertible?     a b c a b 0     A=d e 0 B=c d 0. f 0 0 0 0 e"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "19. Compute the symmetric LDLT factorization of   (cid:34) (cid:35) 1 3 5   a b A=3 12 18 and A= . b d 5 18 30"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "20. Find the inverse of   1 0 0 0   1 1 0 0 A=4 . 1 1 1 0 3 3 1 1 1 1 2 2 2"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "21. (Remarkable) If A and B are square matrices, show that I−BA is invertible if I−AB is invertible. Start from B(I−AB)=(1−BA)B."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "22. Find the inverses (directly or from the 2 by 2 formula) of A, B,C: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 0 3 a b 3 4 A= and B= and C = . 4 6 b 0 5 7 (cid:34) (cid:35) x t"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "23. Solve for the columns of A−1 = : y z (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 10 20 x 1 10 20 t 0 = and = . 20 50 y 0 20 50 z 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "24. Show that [1 2] has no inverse by trying to solve for the column (x,y): 3 6 (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 1 2 x t 1 0 1 2 x 1 = must include = . 3 6 y z 0 1 3 6 y 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "25. (Important) If A has row 1 + row 2 = row 3, show that A is not invertible: (a) Explain why Ax=(1,0,0) cannot have a solution. (b) Which right-hand sides (b ,b ,b ) might allow a solution to Ax=b? 1 2 3 (c) What happens to row 3 in elimination?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "26. If A has column 1 + column 2 = column 3, show that A is not invertible:"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 InversesandTransposes 61 (a) Find a nonzero solution x to Ax=0. The matrix is 3 by 3. (b) Elimination keeps column 1 + column 2 = column 3. Explain why there is no third pivot."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "27. Suppose A is invertible and you exchange its first two rows to reach B. Is the new matrix B invertible? How would you find B−1 from A−1?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "28. If the product M = ABC of three square matrices is invertible, then A, B, C are invertible. Find a formula for B−1 that involves M−1 and A andC."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "29. Prove that a matrix with a column of zeros cannot have an inverse."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "30. Multiply [a b] times [ d −b]. What is the inverse of each matrix if ad (cid:54)=bc? c d −c a"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "31. (a) WhatmatrixE hasthesameeffectasthesethreesteps? Subtractrow1fromrow 2, subtract row 1 from row 3, then subtract row 2 from row 3. (b) What single matrix L has the same effect as these three reverse steps? Add row 2 to row 3, add row 1 to row 3, then add row 1 to row 2."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "32. Find the numbers a and b that give the inverse of 5 ∗ eye(4) − ones(4,4):     −1 4 −1 −1 −1 a b b b     −1 4 −1 −1 b a b b   = . −1 −1 4 −1 b b a b −1 −1 −1 4 b b b a What are a and b in the inverse of 6 ∗ eye(5) − ones(5,5)?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "33. Show that A = 4 ∗ eye(4) − ones(4,4) is not invertible: Multiply A ∗ ones(4,1)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "34. There are sixteen 2 by 2 matrices whose entries are 1s and 0s. How many of them are invertible? Problems 35–39 are about the Gauss-Jordan method for calculating A−1."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "35. Change I into A−1 as you reduce A to I (by row operations): (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) 1 3 1 0 1 4 1 0 A I = and A I = . 2 7 0 1 3 9 0 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "36. Follow the 3 by 3 text example but with plus signs in A. Eliminate above and below the pivots to reduce [A I] to [I A−1]:   2 1 0 1 0 0 (cid:104) (cid:105)   A I =1 2 1 0 1 0. 0 1 2 0 0 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "37. Use Gauss-Jordan elimination on [A I] to solve AA−1 =I:     1 a b 1 0 0 (cid:104) (cid:105)     0 1 c x x x =0 1 0. 1 2 3 0 0 1 0 0 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "38. Invert these matrices A by the Gauss-Jordan method starting with [A I]:     1 0 0 1 1 1     A=2 1 3 and A=1 2 2. 0 0 1 1 2 3"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "39. Exchange rows and continue with Gauss-Jordan to find A−1: (cid:34) (cid:35) (cid:104) (cid:105) 0 2 1 0 A I = . 2 2 0 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "40. True or false (with a counterexample if false and a reason if true): (a) A 4 by 4 matrix with a row of zeros is not invertible. (b) A matrix with Is down the main diagonal is invertible. (c) If A is invertible then A−1 is invertible. (d) If AT is invertible then A is invertible."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "41. For which three numbers c is this matrix not invertible, and why not?   2 c c   A=c c c. 8 7 c"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "42. Prove that A is invertible if a(cid:54)=0 and a(cid:54)=b (find the pivots and A−1):   a b b   A=a a b. a a a"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "43. This matrix has a remarkable inverse. Find A−1 by elimination on [A I]. Extend to a 5 by 5 “alternating matrix” and guess its inverse:   1 −1 1 −1   0 1 −1 1  A= . 0 0 1 −1 0 0 0 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 InversesandTransposes 63"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "44. If B has the columns of A in reverse order, solve (A−B)x =0 to show that A−B is not invertible. An example will lead you to x."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "45. Find and check the inverses (assuming they exist) of these block matrices: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) I 0 A 0 0 I . C I C D I D"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "46. Use inv(S) to invert MATLAB’s 4 by 4 symmetric matrix S = pascal(4). Create Pascal’s lower triangular A = abs(pascal(4,1)) and test inv(S) = inv(A’) ∗ inv(A)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "47. If A = ones(4,4) and b = rand(4,1), how does MATLAB tell you that Ax = b has no solution? If b = ones(4,1), which solution to Ax=b is found by A\\b?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "48. M−1 shows the change in A−1 (useful to know) when a matrix is subtracted from A. Check part 3 by carefully multiplying MM−1 to get I:"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. M =I−uvT and M−1 =I+uvT/(1−vTu)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. M =A−uvT and M−1 =A−1+A−1uvTA−1/(1−vTA−1u)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. M =I−UV and M−1 =I +U(I −VU)−1V. n m"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "4. M =A−UW−1V and M−1 =A−1+A−1U(W −VA−1U)−1VA−1. The four identities come from the 1, 1 block when inverting these matrices: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) I u A u I U A U n . vT 1 vT 1 V I V W m Problems 49–55 are about the rules for transpose matrices."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "49. Find AT and A−1 and (A−1)T and (AT)−1 for (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 c A= and also A= . 9 3 c 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "50. Verify that (AB)T equals BTAT but those are different from ATBT: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 3 1 3 A= B= AB= . 2 1 0 1 2 7 In case AB=BA (not generally true!), how do you prove that BTAT =ATBT? (cid:161) (cid:162)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "51. (a) The matrix (AB)−1 T comes from (A−1)T and (B−1)T. In what order? (b) IfU is upper triangular then (U−1)T is triangular."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "52. Show that A2 =0 is possible but ATA=0 is not possible (unless A= zero matrix)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "53. (a) The row vector xT times A times the column y produces what number?   (cid:34) (cid:35) 0 (cid:104) (cid:105) 1 2 3   xTAy= 0 1 1= . 4 5 6 0 (b) This is the row xTA= times the column y=(0,1,0). (c) This is the row xT =[0 1] times the column Ay= ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "54. When you transpose a block matrix M = [A B] the result is MT = . Test it. C D Under what conditions on A, B,C, D is the block matrix symmetric?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "55. Explain why the inner product of x and y equals the inner product of Px and Py. Then (Px)T(Py)=xTy says that PTP=I for any permutation. With x=(1,2,3) and y=(1,4,2), choose P to show that (Px)Ty is not always equal to xT(PTy). Problems 56–60 are about symmetric matrices and their factorizations."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "56. If A=AT and B=BT, which of these matrices are certainly symmetric? (a) A2−B2 (b) (A+B)(A−B) (c) ABA (d) ABAB."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "57. If A=AT needs a row exchange, then it also needs a column exchange to stay sym- metric. In matrix language, PA loses the symmetry of A but recovers the sym- metry."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "58. (a) How many entries of A can be chosen independently, if A=AT is 5 by 5? (b) How do L and D (5 by 5) give the same number of choices in LDLT?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "59. Suppose R is rectangular (m by n) and A is symmetric (m by m). (a) Transpose RTAR to show its symmetry. What shape is this matrix? (b) Show why RTR has no negative numbers on its diagonal."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "60. Factor these symmetric matrices into A=LDLT. The matrix D is diagonal:   (cid:34) (cid:35) (cid:34) (cid:35) 2 −1 0 1 3 1 b   A= and A= and A=−1 2 −1. 3 2 b c 0 −1 2 The next three problems are about applications of (Ax)Ty=xT(ATy)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "61. Wires go between Boston, Chicago, and Seattle. Those cities are at voltages x , x , B C x . With unit resistances between cities, the three currents are in y: S      y 1 −1 0 x BC B      y=Ax is y =0 1 −1x . CS C y 1 0 −1 x BS S"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 InversesandTransposes 65 (a) Find the total currents ATy out of the three cities. (b) Verify that (Ax)Ty agrees with xT(ATy)—six terms in both."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "62. Producing x trucks and x planes requires x +50x tons of steel, 40x +1000x 1 2 1 2 1 2 poundsofrubber,and2x +50x monthsoflabor. Iftheunitcostsy ,y ,y are$700 1 2 1 2 3 per ton, $3 per pound, and $3000 per month, what are the values of one truck and one plane? Those are the components of ATy."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "63. Ax gives the amounts of steel, rubber, and labor to produce x in Problem 62. Find A. Then (Ax)Ty is the of inputs while xT(ATy) is the value of ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "64. Here is a new factorization of A into triangular times symmetric: Start from A=LDU. Then A equals L(UT)−1 timesUTDU. Why is L(UT)−1 triangular? Its diagonal is all 1s. Why isUTDU symmetric?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "65. A group of matrices includes AB and A−1 if it includes A and B. “Products and inverses stay in the group.” Which of these sets are groups? Lower triangularmatri- ces L with is on the diagonal, symmetric matrices S, positive matrices M, diagonal invertible matrices D, permutation matrices P. Invent two more matrix groups."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "66. If every row of a 4 by 4 matrix contains the numbers 0, 1, 2, 3 in some order, can the matrix be symmetric? Can it be invertible?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "67. Prove that no reordering of rows and reordering of columns can transpose a typical matrix."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "68. A square northwest matrix B is zero in the southeast corner, below the antidiagonal that connects (1,n) to (n,1). Will BT and B2 be northwest matrices? Will B−1 be northwest or southeast? What is the shape of BC = northwest times southeast? You are allowed to combine permutations with the usual L and U (southwest and northeast)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "69. Compare tic; inv(A); toc for A = rand(500) and A = rand(1000). The n3 count says that computing time (measured by tic; toc) should multiply by 8 when n is doubled. Do you expect these random A to be invertible?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "70. I =eye(1000); A=rand(1000); B=triu(A);producesarandomtriangular matrix B. Compare the times for inv(B) and B\\I. Backslash is engineered to use the zeros in B, while inv uses the zeros in I when reducing [B I] by Gauss-Jordan. (Compare also with inv(A) and A\\I for the full matrix A.)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "71. Show that L−1 has entries j/i for i≤ j (the −1, 2, −1 matrix has this L):     1 0 0 0 1 0 0 0     −1 1 0 0 1 1 0 0 L= 2  and L−1 =2 .  0 −2 1 0 1 2 1 0 3 3 3 0 0 −3 1 1 2 3 1 4 4 4 4 Test this pattern for L= eye(5) − diag(1:5)\\diag(1:4,−1) and inv(L)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.7 Special Matrices and Applications This section has two goals. The first is to explain one way in which large linear systems Ax=b can arise in practice. The truth is that a large and completely realistic problem in engineeringoreconomicswouldleadusfarafield. Butthereisonenaturalandimportant application that does not require a lot of preparation. The other goal is to illustrate, by this same application, the special properties that co- efficient matrices frequently have. Large matrices almost always have a clear pattern— frequently a pattern of symmetry, and very many zero entries. Since a sparse matrix contains far fewer than n2 pieces of information, the computations ought to be fast. We lookatbandmatrices,toseehowconcentrationnearthediagonalspeedsupelimination. In fact we look at one special tridiagonal matrix. Thematrix itself canbe seen inequation (6). It comes fromchanging a differential equation to a matrix equation. The continuous problem asks for u(x) at every x, and a computer cannot solve it exactly. It has to be approximated by a discrete problem—the more unknowns we keep, the better will be the accuracy and the greater the expense. As a simple but still very typical continuous problem, our choice falls on the differential equation d2u − = f(x), 0≤x≤1. (1) dx2 This is a linear equation for the unknown function u(x). Any combination C+Dx couldbeaddedtoanysolution,sincethesecondderivativeofC+Dxcontributesnothing. TheuncertaintyleftbythesetwoarbitraryconstantsC andDisremovedbya“boundary condition” at each end of the interval: u(0)=0, u(1)=0. (2) Theresultisatwo-pointboundary-valueproblem,describingnotatransientbutasteady- state phenomenon—the temperature distribution in a rod, for example, with ends fixed at 0℃ and with a heat source f(x). Remember that our goal is to produce a discrete problem—in other words, a problem inlinearalgebra. Forthatreasonwecanonlyacceptafiniteamountofinformationabout f(x), say its values at n equally spaced points x = h,x = 2h,...,x = nh. We compute approximate values u ,...,u for the true solution u at these same points. At the ends 1 n x=0 and x=1=(n+1)h, the boundary values are u =0 and u =0. 0 n+1 The first question is: How do we replace the derivative d2u/dx2? The first derivative can be approximated by stopping ∆u/∆x at a finite stepsize, and not permitting h (or ∆x)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.7 SpecialMatricesandApplications 67 to approach zero. The difference ∆u can be forward, backward, or centered: ∆u u(x+h)−u(x) u(x)−u(x−h) u(x+h)−u(x−h) = or or . (3) ∆x h h 2h The last is symmetric about x and it is the most accurate. For the second derivative there is just one combination that uses only the values at x and x±h: d2u ∆2 u u(x+h)−2u(x)+u(x−h) Second difference ≈ = . (4) dx2 ∆x2 h2 This also has the merit of being symmetric about x. To repeat, the right-hand side ap- proaches the true value of d2u/dx2 as h→0, but we have to stop at a positive h. At each meshpoint x = jh, the equation −d2u/dx2 = f(x) is replaced by its discrete analogue (5). We multiplied through by h2 to reach n equations Au=b: Difference equation −u +2u −u =h2f(jh) for j =1,...,n. (5) j+1 j j−1 The first and last equations (j = 1 and j = n) include u = 0 and u = 0, which are 0 n+1 known from the boundary conditions. These values would be shifted to the right-hand side of the equation if they were not zero. The structure of these n equations (5) can be better visualized in matrix form. We choose h= 1, to get a 5 by 5 matrix A: 6      2 −1 u f(h) 1      −1 2 −1 u 2 f(2h)      Matrix equation  −1 2 −1 u =h2 f(3h). (6) 3       −1 2 −1u  f(4h) 4 −1 2 u f(5h) 5 From now on, we will work with equation (6). It has a very regular coefficient matrix, whose order n can be very large. The matrix A possesses many special properties, and three of those properties are fundamental:"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. The matrix A is tridiagonal. All nonzero entries lie on the main diagonal and the two adjacent diagonals. Outside this band all entries are a = 0. These zeros will ij bring a tremendous simplification to Gaussian elimination."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. Thematrixissymmetric. Eachentrya equalsitsmirrorimagea ,sothatAT =A. ij ji The upper triangular U will be the transpose of the lower triangular L, and A = LDLT. This symmetry of A reflects the symmetry of d2u/dx2. An odd derivative like du/dx or d3u/dx3 would destroy the symmetry."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. Thematrixispositivedefinite. Thisextrapropertysaysthatthepivotsarepositive. Row exchanges are unnecessary in theory and in practice. This is in contrast to the matrix B at the end of this section, which is not positive definite. Without a row exchange it is totally vulnerable to roundoff. Positive definiteness brings this whole course together (in Chapter 6)! We return to the fact that A is tridiagonal. What effect does this have on elimination? The first stage of the elimination process produces zeros below the first pivot:     2 −1 2 −1  −1 2 −1    0 3 −1   Elimination    2   −1 2 −1 → −1 2 −1 . on A: Step 1      −1 2 −1  −1 2 −1 −1 2 −1 2 Compared with a general 5 by 5 matrix, that step displays two major simplifications:"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. There was only one nonzero entry below the pivot."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. The pivot row was very short. The multiplier (cid:96) = −1 came from one division. The new pivot 3 came from a single 21 2 2 multiplication-subtraction. Furthermore, the tridiagonal pattern is preserved: Every stage of elimination admits the simplifications (a) and (b). The final result is the LDU =LDLT factorization of A. Notice the pivots!     1 2 1 −1 1 2  −1 1    3    1 −2   2 2 3     A= −2 1  4  1 −3 .  3  3  4   −3 1  5  1 −4 4 4 5 −4 1 6 1 5 5 The L and U factors of a tridiagonal matrix are bidiagonal. The three factors together have the same band structure of three essential diagonals (3n−2 parameters) as A. Note too that L and U are transposes of one another, as expected from the symmetry. The pivots 2/1, 3/2, 4/3, 5/4, 6/5 are all positive. Their product is the determinant of A: detA=6. Thepivotsareobviouslyconvergingto1, asngetslarge. Suchmatricesmake a computer very happy. These sparse factors L andU completely change the usual operation count. Elimina- tion on each column needs only two operations, as above, and there are n columns. In place of n3/3 operations we need only 2n. Tridiagonal systems Ax = b can be solved almost instantly. The cost of solving a tridiagonal system is proportional to n. A band matrix has a = 0 except in the band |i− j| < w (Figure 1.8). The “half ij bandwidth” is w = 1 for a diagonal matrix, w = 2 for a tridiagonal matrix, and w = n for a full matrix. For each column, elimination requires w(w−1) operations: a row of length w acts on w−1 rows below. Elimination on the n columns of a band matrix requires about w2n operations. Aswapproachesn,thematrixbecomesfull,andthecountisroughlyn3. Foranexact count, the lower right-hand corner has no room for bandwidth w. The precise number of divisions and multiplication-subtractions that produce L, D, andU (without assuming a"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.7 SpecialMatricesandApplications 69 w w w A = = w = LU Figure1.8: AbandmatrixAanditsfactorsLandU. symmetric A) is P= 1w(w−1)(3n−2w+1). For a full matrix with w=n, we recover 3 P= 1n(n−1)(n+1). This is a whole number, since n−1, n, and n+1 are consecutive 3 integers, and one of them is divisible by 3. Thatisourlastoperationcount,andweemphasizethemainpoint. Afinite-difference matrix like A has a full inverse. In solving Ax = b, we are actually much worse off knowing A−1 than knowing L and U. Multiplying A−1 by b takes n2 steps, whereas 4n aresufficientfortheforwardeliminationandback-substitutionthatproducex=U−1c= U−1L−1b=A−1b. We hope this example reinforced the reader’s understanding of elimination (which we now assume to be perfectly understood!). It is a genuine example of the large linear systems that are actually met in practice. The next chapter turns to the existence and the uniqueness of x, for m equations in n unknowns. Roundoff Error In theory the nonsingular case is completed. There is a full set of pivots (with row ex- changes). In practice, more row exchanges may be equally necessary—or the computed solution can easily become worthless. We will devote two pages (entirely optional in class) to making elimination more stable—why it is needed and how it is done. For a system of moderate size, say 100 by 100, elimination involves a third of a mil- lion operations (1n3). With each operation we must expect a roundoff error. Normally, 3 wekeepafixednumberofsignificantdigits(saythree,foranextremelyweakcomputer). Then adding two numbers of different sizes gives an error: Roundoff Error .456+.00123→.457 loses the digits 2 and 3. How do all these individual errors contribute to the final error in Ax=b? This is not an easy problem. It was attacked by John von Neumann, who was the leading mathematician at the time when computers suddenly made a million operations possible. In fact the combination of Gauss and von Neumann gives the simple elimina- tion algorithm a remarkably distinguished history, although even von Neumann overes- timatedthefinalroundofferror. ItwasWilkinsonwhofoundtherightwaytoanswerthe question, and his books are now classics. Two simple examples will illustrate three important points about roundoff error. The examples are (cid:34) (cid:35) (cid:34) (cid:35)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. 1. .0001 1. Ill-conditioned A= Well-conditioned B= ."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. 1.0001 1. 1. A is nearly singular whereas B is far from singular. If we slightly change the last entry of A to a =1, it is singular. Consider two very close right-hand sides b: 22 u + v = 2 u + v = 2 and u + 1.0001v = 2 u + 1.0001v = 2.0001 The solution to the first is u = 2, v = 0. The solution to the second is u = v = 1. A changeinthefifthdigitofbwasamplifiedtoachangeinthefirstdigitofthesolution. No numerical method can avoid this sensitivity to small perturbations. The ill-conditioning can be shifted from one place to another, but it cannot be removed. The true solution is very sensitive, and the computed solution cannot be less so. The second point is as follows. 1O Even a well-conditioned matrix like B can be ruined by a poor algorithm. We regret to say that for the matrix B, direct Gaussian elimination is a poor algorithm. Suppose.0001isacceptedasthefirstpivot. Then10,000timesthefirstrowissubtracted from the second. The lower right entry becomes −9999, but roundoff to three places would give −10,000. Every trace of the entry 1 would disappear: Elimination on B .0001u+v = 1 .0001u+v = 1 −→ with small pivot u+v = 2 −9999v = −9998. Roundoff will produce −10,000v=−10,000, or v=1. This is correct to three decimal places. Back-substitution with the right v=.9999 would leave u=1: Correct result .0001u+.9999=1, or u=1. Instead, accepting v=1, which is wrong only in the fourth place, we obtain u=0: Wrong result .0001u+1=1, or u=0. The computed u is completely mistaken. B is well-conditioned but elimination is vio- lently unstable. L, D, andU are completely out of scale with B: (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) 1 0 .0001 0 1 10,000 B= . 10,000 1 0 −9999 0 1 The small pivot .0001 brought instability, and the remedy is clear—exchange rows."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1. Write out the LDU =LDLT factors of A in equation (6) when n=4. Find the deter- minant as the product of the pivots in D."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "2. Modify a in equation (6) from a =2 to a =1, and find the LDU factors of this 11 11 11 new tridiagonal matrix."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "3. Find the 5 by 5 matrix A (h= 1) that approximates 0 6 d2u du du − = f(x), (0)= (1)=0, dx2 dx dx replacing these boundary conditions by u = u and u = u . Check that your A 0 1 6 5 0 times the constant vector (C,C,C,C,C), yields zero; A is singular. Analogously, if 0 u(x) is a solution of the continuous problem, then so is u(x)+C."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "4. Write down the 3 by 3 finite-difference matrix equation (h= 1) for 4 d2u − +u=x, u(0)=u(1)=0. dx2"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "5. With h= 1 and f(x)=4π2sin2πx, the difference equation (5) is 4      2 −1 0 u 1   1  π2   −1 2 −1u =  0 . 2 4 0 −1 2 u −1 3 Solve for u , u , u and find their error in comparison with the true solution u = 1 2 3 sin2πx at x= 1, x= 1, and x= 3. 4 2 4"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "6. What 5 by 5 system replaces (6) if the boundary conditions are changed to u(0)=1, u(1)=0? Problems 7–11 are about roundoff error and row exchanges."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "7. Compute H−1 in two ways for the 3 by 3 Hilbert matrix   1 1 1 2 3   H =1 1 1 , 2 3 4 1 1 1 3 4 5 first by exact computation and second by rounding off each number to three figures. This matrix H is ill-conditioned and row exchanges don’t help."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "8. For the same matrix H, compare the right-hand sides of Hx = b when the solutions are x=(1,1,1) and x=(0,6,−3.6)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "9. Solve Hx=b=(1,0,...,0) for the 10 by 10 Hilbert matrix with h =1/(i+ j−1), ij using any computer code for linear equations. Then change an entry of b by .0001 and compare the solutions."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "10. Compare the pivots in direct elimination to those with partial pivoting for (cid:34) (cid:35) .001 0 A= . 1 1000 (This is actually an example that needs rescaling before elimination.)"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "11. Explain why partial pivoting produces multipliers (cid:96) in L that satisfy |(cid:96) | ≤ 1. Can ij ij you construct a 3 by 3 example with all |a | ≤ 1 whose last pivot is 4? This is the ij worst possible, since each entry is at most doubled when |(cid:96) |≤1. ij Review Exercises"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.1 (a) Write down the 3 by 3 matrices with entries i a =i− j and b = . ij ij j"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.7 SpecialMatricesandApplications 73 (b) Compute the products AB and BA and A2."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.2 For the matrices (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 2 A= and B= , 2 1 0 1 compute AB and BA and A−1 and B−1 and (AB)−1."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.3 Find examp1es of 2 by 2 matrices with a = 1 for which (a) A2 = I. (b) 12 2 A−1 =AT. (c) A2 =A."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.4 Solve by elimination and back-substitution: u + w = 4 v + w = 0 u + v = 3 and u + w = 0 u + v + w = 6 u + v = 6."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.5 Factor the preceding matrices into A=LU or PA=LU."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.6 (a) There are sixteen 2 by 2 matrices whose entries are 1s and 0s. How many are invertible? (b) (Much harder!) If you put 1s and 0s at random into the entries of a 10 by 10 matrix, is it more likely to be invertible or singular?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.7 There are sixteen 2 by 2 matrices whose entries are 1s and −1s. How many are invertible?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.8 How are the rows of EA related to the rows of A in the following cases?     (cid:34) (cid:35) 1 0 0 0 0 1   1 1 1   E =0 2 0 or E = or E =0 1 0. 0 0 0 4 0 1 1 0 0"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.9 Write down a 2 by 2 system with infinitely many solutions."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.10 Find inverses if they exist, by inspection or by Gauss-Jordan:       1 0 1 2 1 0 1 1 −2       A=1 1 0 and A=1 2 1 and A= 1 −2 1  0 1 1 0 1 2 −2 1 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.11 If E is 2 by 2 and it adds the first equation to the second, what are E2 and E8 and 8E?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.12 True or false, with reason if true or counterexample if false: (1) If A is invertible and its rows are in reverse order in B, then B is invertible. (2) If A and B are symmetric then AB is symmetric. (3) If A and B are invertible then BA is invertible. (4) Every nonsingular matrix can be factored into the product A = LU of a lower triangular L and an upper triangularU."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.13 Solve Ax=b by solving the triangular systems Lc=b andUx=c:      1 0 0 2 2 4 0      A=LU =4 1 00 1 3, b=0. 1 0 1 0 0 1 1 What part of A−1 have you found, with this particular b?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.14 If possible, find 3 by 3 matrices B such that (1) BA=2A for every A. (2) BA=2B for every A. (3) BA has the first and last rows of A reversed. (4) BA has the first and last columns of A reversed."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.15 Find the value for c in the following n by n inverse:     n −1 · −1 c 1 · 1     −1 n · −1 1 1 c · 1 if A=  then A−1 =  .  · · · −1 n+1· · · 1 −1 −1 −1 n 1 1 1 c"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.16 For which values of k does kx + y = 1 x + ky = 1 have no solution, one solution, or infinitely many solutions?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.17 Find the symmetric factorization A=LDLT of   (cid:34) (cid:35) 1 2 0   a b A=2 6 4  and A= . b c 0 4 11"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.18 Suppose A is the 4 by 4 identity matrix except for a vector v in column 2:   1 v 0 0 1   0 v 0 0 2 A= . 0 v 1 0 3 0 v 0 1 4"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.7 SpecialMatricesandApplications 75 (a) Factor A into LU, assuming v (cid:54)=0. 2 (b) Find A−1, which has the same form as A."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.19 Solve by elimination, or show that there is no solution: u + v + w = 0 u + v + w = 0 u + 2v + 3w = 0 and u + u + 3w = 0 3u + 5v + 7w = 1 3u + 5v + 7w = 1."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.20 The n by n permutation matrices are an important example of a “group.” If you multiplythemyoustayinsidethegroup;theyhaveinversesinthegroup;theidentity is in the group; and the law P (P P ) = (P P )P is true—because it is true for all 1 2 3 1 2 3 matrices. (a) How many members belong to the groups of 4 by 4 and n by n permutation matrices? (b) Find a power k so that all 3 by 3 permutation matrices satisfy Pk =I."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.21 Describe the rows of DA and the columns of AD if D=[2 0]. 0 5"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.22 (a) If A is invertible what is the inverse of AT? (b) If A is also symmetric what is the transpose of A−1? (c) Illustrate both formulas when A=[2 1]. 1 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.23 By experiment with n=2 and n=3, find (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) n n −1 2 3 2 3 2 3 , , . 0 0 0 1 0 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.24 Starting with a first plane u+2v−w=6, find the equation for (a) the parallel plane through the origin. (b) a second plane that also contains the points (6,0,0) and (2,2,0). (c) a third plane that meets the first and second in the point (4,1,0)."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.25 What multiple of row 2 is subtracted from row 3 in forward elimination of A?    1 0 0 1 2 0    A=2 1 00 1 5. 0 5 1 0 0 1 How do you know (without multiplying those factors) that A is invertible, symmet- ric, and tridiagonal? What are its pivots?"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.26 (a) WhatvectorxwillmakeAx=column1ofA+2(column3),fora3by3matrix A? (b) Construct a matrix that has column 1 + 2(column 3) = 0. Check that A is singular (fewer than 3 pivots) and explain why that must be the case."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.27 True or false, with reason if true and counterexample if false: (1) If L U = L U (upper triangular U’s with nonzero diagonal, lower triangular 1 1 2 2 L’s with unit diagonal), then L = L and U = U . The LU factorization is 1 2 1 2 unique. (2) If A2+A=I then A−1 =A+I. (3) If all diagonal entries of A are zero, then A is singular."
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.28 By experiment or the Gauss-Jordan method compute       n −1 −1 1 0 0 1 0 0 1 0 0       (cid:96) 1 0 , (cid:96) 1 0 , (cid:96) 1 0 . m 0 1 m 0 1 0 m 1"
    },
    {
        "chapter": "MatricesandGaussianElimination",
        "question": "1.29 Write down the 2 by 2 matrices that (a) reverse the direction of every vector. (b) project every vector onto the x axis. 2 (c) turn every vector counterclockwise through 90°. (d) reflect every vector through the 45° line x =x . 1 2 2 Chapter Vector Spaces"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.1 Vector Spaces and Subspaces Elimination can simplify, one entry at a time, the linear system Ax = b. Fortunately it also simplifies the theory. The basic questions of existence and uniqueness—Is there one solution, or no solution, or an infinity of solutions?—are much easier to answer after elimination, We need to devote one more section to those questions, to find every solution for an m by n system. Then that circle of ideas will be complete. But elimination produces only one kind of understanding of Ax=b. Our chief object is to achieve a different and deeper understanding. This chapter may be more difficult than the first one. It goes to the heart of linear algebra. For the concept of a vector space, we start immediately with the most important spaces. They are denoted by R1,R2,R3,...; the space Rn consists of all column vectors with n components. (We write R because the components are real numbers.) R2 is represented by the usual x-y plane; the two components of the vector become the x and y coordinates of the corresponding point. The three components of a vector in R3 give a point in three-dimensional space. The one-dimensional space R1 is a line. The valuable thing for linear algebra is that the extension to n dimensions is so straightforward. For a vector in R7 we just need the seven components, even if the geometry is hard to visualize. Within all vector spaces, two operations are possible: We can add any two vectors, and we can multiply all vectors by scalars. In other words, we can take linear combinations. Addition obeys the commutative law x+y = y+x; there is a “zero vector” satisfying 0+x =x; and there is a vector “−x” satisfying −x+x =0. Eight properties (including those three) are fundamental; the full list is given in Problem 5 at the end of this section. A real vector space is a set of vectors together with rules for vector addition and mul- tiplication by real numbers. Addition and multiplication must produce vectors in the space, and they must satisfy the eight conditions. Normally our vectors belong to one of the spaces Rn; they are ordinary column vec- tors. If x = (1,0,0,3), then 2x (and also x+x) has components 2, 0, 0, 6. The formal definition allows other things to be “vectors”-provided that addition and scalar multipli- cation are all right. We give three examples:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Theinfinite-dimensionalspaceR∞. Itsvectorshaveinfinitelymanycomponents,as in x=(1,2,1,2,...). The laws for x+y and cx stay unchanged."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. The space of 3 by 2 matrices. In this case the “vectors” are matrices! We can add two matrices, and A+B=B+A, and there is a zero matrix, and so on. This space is almost the same as R6. (The six components are arranged in a rectangle instead of a column.) Any choice of m and n would give, as a similar example, the vector space of all m by n matrices."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. The space of functions f(x). Here we admit all functions f that are defined on a fixed interval, say 0 ≤ x ≤ 1. The space includes f(x) = x2, g(x) = sinx, their sum (f +g)(x) = x2+sinx, and all multiples like 3x2 and −sinx. The vectors are functions, and the dimension is somehow a larger infinity than for R∞. Other examples are given in the exercises, but the vector spaces we need most are somewhere else—they are inside the standard spaces Rn. We want to describe them andexplainwhytheyareimportant. Geometrically,thinkoftheusualthree-dimensional R3 and choose any plane through the origin. That plane is a vector space in its own right. If we multiply a vector in the plane by 3, or −3, or any other scalar, we get a vector in the same plane. If we add two vectors in the plane, their sum stays in the plane. Thisplanethrough(0,0,0)illustratesoneofthemostfundamentalideasinlinear algebra; it is a subspace of the original space R3. Definition. Asubspaceofavectorspaceisanonemptysubsetthatsatisfiestherequire- ments for a vector space: Linear combinations stay in the subspace. (i) If we add any vectors x and y in the subspace, x+y is in the subspace. (ii) If we multiply any vector x in the subspace by any scalar c, cx is in the subspace. Noticeouremphasisonthewordspace. Asubspaceisasubsetthatis“closed”under addition and scalar multiplication. Those operations follow the rules of the host space, keeping us inside the subspace. The eight required properties are satisfied in the larger space and will automatically be satisfied in every subspace. Notice in particular that the zero vector will belong to every subspace. That comes from rule (ii): Choose the scalar to be c=0. The smallest subspace Z contains only one vector, the zero vector. It is a “zero- dimensionalspace,”containingonlythepointattheorigin. Rules(i)and(ii)aresatisfied,"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.1 VectorSpacesandSubspaces 79 since the sum 0+0 is in this one-point space, and so are all multiples c0. This is the smallest possible vector space: the empty set is not allowed. At the other extreme. the largest subspace is the whole of the original space. If the original space is R3, then the possible subspaces are easy to describe: R3 itself, any plane through the origin, any line through the origin, or the origin (the zero vector) alone. The distinction between a subset and a subspace is made clear by examples. In each case, can you add vectors and multiply by scalars, without leaving the space? Example 1. Consider all vectors in R2 whose components are positive or zero. This subset is the first quadrant of the x-y plane; the coordinates satisfy x ≥ 0 and y ≥ 0. It is not a subspace, even though it contains zero and addition does leave us within the subset. Rule (ii) is violated, since if the scalar is −1 and the vector is [1 1], the multiple cx=[−1 −1] is in the third quadrant instead of the first. If we include the third quadrant along with the first, scalar multiplication is all right. Everymultiplecxwillstayinthissubset. However,rule(i)isnowviolated,sinceadding [1 2]+[−2 −1] gives [−1 1], which is not in either quadrant. The smallest subspace containing the first quadrant is the whole space R2. Example 2. Start from the vector space of 3 by 3 matrices. One possible subspace is the set of lower triangular matrices. Another is the set of symmetric matrices. A+B and cA are lower triangular if A and B are lower triangular, and they are symmetric if A and B are symmetric. Of course, the zero matrix is in both subspaces. The Column Space of A We now come to the key examples, the column space and the nullspace of a matrix A. The column space contains all linear combinations of the columns of A. It is a subspace of Rm. We illustrate by a system of m=3 equations in n=2 unknowns:     (cid:34) (cid:35) 1 0 b 1   u   Combination of columns equals b 5 4 =b . (1) 2 v 2 4 b 3 With m>n we have more equations than unknowns—and usually there will be no solu- tion. The system will be solvable only for a very “thin” subset of all possible b’s. One way of describing this thin subset is so simple that it is easy to overlook. 2A ThesystemAx=bissolvableifandonlyifthevectorbcanbeexpressed as a combination of the columns of A. Then b is in the column space. This description involves nothing more than a restatement of Ax=b, by columns:       1 0 b 1       Combination of columns u5+v4=b . (2) 2 2 4 b 3 perpendicular to plane 0   4   4 1 0     column 1 = 5 0     b 2 0 column space Figure2.1: ThecolumnspaceC(A),aplaneinthree-dimensionalspace. Thesearethesamethreeequationsintwounknowns. Nowtheproblemis: Findnumbers u and v that multiply the first and second columns to produce b. The system is solvable exactly when such coefficients exist, and the vector (u,v) is the solution x. Wearesayingthattheattainableright-handsidesbareallcombinationsofthecolumns of A. One possible right-hand side is the first column itself; the weights are u = 1 and v = 0. Another possibility is the second column: u = 0 and v = 1. A third is the right- hand side b=0. With u=0 and v=0, the vector b=0 will always be attainable. We can describe all combinations of the two columns geometrically: Ax = b can be solvedifandonlyifbliesintheplanethatisspannedbythetwocolumnvectors(Figure"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.1). Thisisthethinsetofattainableb. Ifbliesofftheplane,thenitisnotacombination of the two columns. In that case Ax=b has no solution. What is important is that this plane is not just a subset of R3 it is a subspace. It is the column space of A, consisting of all combinations of the columns. It is denoted by C(A). Requirements (i) and (ii) for a subspace of Rm are easy to check: (i) Suppose b and b(cid:48) lie in the column space, so that Ax = b for some x and Ax(cid:48) = b(cid:48) for some x(cid:48). Then A(x+x(cid:48)) = b+b(cid:48), so that b+b(cid:48) is also a combination of the columns. The column space of all attainable vectors b is closed under addition. (ii) If b is in the column space C(A), so is any multiple cb. If some combination of columns produces b (say Ax = b), then multiplying that combination by c will produce cb. In other words, A(cx)=cb. For another matrix A, the dimensions in Figure 2.1 may be very different. The small- est possible column space (one vector only) comes from the zero matrix A = 0. The"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Construct a subset of the x-y plane R2 that is (a) closed under vector addition and subtraction, but not scalar multiplication. (b) closed under scalar multiplication but not under vector addition. Hint: Starting with u and v, add and subtract for (a). Try cu and cv for (b)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Which of the following subsets of R3 are actually subspaces? (a) The plane of vectors (b ,b ,b ) with first component b =0. 1 2 3 1 (b) The plane of vectors b with b =1. 1 (c) The vectors b with b b =0 (this is the union of two subspaces, the plane b =0 2 3 2 and the plane b =0). 3 (d) All combinations of two given vectors (1,1,0) and (2,0,1). (e) The plane of vectors (b ,b ,b ) that satisfy b −b +3b =0. 1 2 3 3 2 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Describe the column space and the nullspace of the matrices (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 −1 0 0 3 0 0 0 A= and B= and C = . 0 0 1 2 3 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. Whatisthesmallestsubspaceof3by3matricesthatcontainsallsymmetricmatrices and all lower triangular matrices? What is the largest subspace that is contained in both of those subspaces?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. Addition and scalar multiplication are required to satisfy these eight rules:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.1 VectorSpacesandSubspaces 83"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. x+y=y+x."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. x+(y+z)=(x+y)+z."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. There is a unique “zero vector” such that x+0=x for all x."
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. For each x there is a unique vector −x such that x+(−x)=0."
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. 1x=x."
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. (c c )x=c (c x). 1 2 1 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "7. c(x+y)=cx+cy."
    },
    {
        "chapter": "VectorSpaces",
        "question": "8. (c +c )x=c x+c x. 1 2 1 2 (a) Suppose addition in R2 adds an extra 1 to each component, so that (3,1)+(5,0) equals(9,2)insteadof(8,1). Withscalarmultiplicationunchanged, whichrules are broken? (b) Showthatthesetofallpositiverealnumbers,withx+yandcxredefinedtoequal the usual xy and xc, is a vector space. What is the “zero vector”? (c) Suppose (x ,x )+(y ,y ) is defined to be (x +y ,x +y ). With the usual 1 2 1 2 1 2 2 1 cx=(cx ,cx ), which of the eight conditions are not satisfied? 1 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. Let P be the plane in 3-space with equation x+2y+z = 6. What is the equation of the plane P through the origin parallel to P? Are P and P subspaces of R3? 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "7. Which of the following are subspaces of R∞? (a) All sequences like (1,0,1,0,...) that include infinitely many zeros. (b) All sequences (x1,x2,...) with x =0 from some point onward. j (c) All decreasing sequences: x ≤x for each j. j+1 j (d) All convergent sequences: the x have a limit as j →∞. j (e) All arithmetic progressions: x −x is the same for all j. j+1 j (f) All geometric progressions (x ,kx ,k2x ,...) allowing all k and x . 1 1 1 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "8. Which of the following descriptions are correct? The solutions x of   (cid:34) (cid:35) (cid:34) (cid:35) x 1 1 1 1   0 Ax= x = 2 1 0 2 0 x 3 form (a) a plane. (b) a line. (c) a point. (d) a subspace. (e) the nullspace of A. (f) the column space of A."
    },
    {
        "chapter": "VectorSpaces",
        "question": "9. Showthatthesetofnonsingular2by2matricesisnotavectorspace. Showalsothat the set of singular 2 by 2 matrices is not a vector space. (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "10. The matrix A= 2 −2 is a “vector” in the space M of all 2 by 2 matrices. Write the 2 −2 zero vector in this space, the vector 1A, and the vector −A. What matrices are in the 2 smallest subspace containing A? (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "11. (a) Describe a subspace of M that contains A= 1 0 but not B= 0 0 . 0 0 0 −1 (b) If a subspace of M contains A and B, must it contain I? (c) Describe a subspace of M that contains no nonzero diagonal matrices."
    },
    {
        "chapter": "VectorSpaces",
        "question": "12. The functions f(x)=x2 and g(x)=5x are “vectors” in the vector space F of all real functions. The combination 3f(x)−4g(x) is the function h(x) = . Which rule is broken if multiplying f(x) by c gives the function f(cx)?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "13. Ifthesumofthe“vectors” f(x)andg(x)inFisdefinedtobe f(g(x)),thenthe“zero vector” is g(x) = x. Keep the usual scalar multiplication cf(x), and find two rules that are broken."
    },
    {
        "chapter": "VectorSpaces",
        "question": "14. Describe the smallest subspace of the 2 by 2 matrix space M that contains (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 0 1 1 0 1 0 (a) and . (b) and . 0 0 0 0 0 0 0 1 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 1 1 0 0 1 (c) . (d) , , . 0 0 0 0 0 1 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "15. Let P be the plane in R3 with equation x+y−2z = 4. The origin (0,0,0) is not in P! Find two vectors in P and check that their sum is not in P."
    },
    {
        "chapter": "VectorSpaces",
        "question": "16. P is the plane through (0,0,0) parallel to the plane P in Problem 15. What is the 0 equation for P ? Find two vectors in P and check that their sum is in P . 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "17. The four types of subspaces of R3 are planes, lines, R3 itself, or Z containing only (0,0,0). (a) Describe the three types of subspaces of R2. (b) Describe the five types of subspaces of R4."
    },
    {
        "chapter": "VectorSpaces",
        "question": "18. (a) The intersection of two planes through (0,0,0) is probably a but it could be a . It can’t be the zero vector Z! (b) The intersection of a plane through (0,0,0) with a line through (0,0,0) is prob- ably a but it could be a ."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.1 VectorSpacesandSubspaces 85 (c) If S and T are subspaces of R5, their intersection S∩T (vectors in both sub- spaces) is a subspace of R5. Check the requirements on x+y and cx."
    },
    {
        "chapter": "VectorSpaces",
        "question": "19. Suppose P is a plane through (0,0,0) and L is a line through (0,0,0). The smallest vector space containing both P and L is either or ."
    },
    {
        "chapter": "VectorSpaces",
        "question": "20. True or false for M= all 3 by 3 matrices (check addition using an example)? (a) The skew-symmetric matrices in M (with AT =−A) form a subspace. (b) The unsymmetric matrices in M (with AT (cid:54)=A) form a subspace. (c) The matrices that have (1,1,1) in their nullspace form a subspace. Problems 21–30 are about column spaces C(A) and the equation Ax=b."
    },
    {
        "chapter": "VectorSpaces",
        "question": "21. Describe the column spaces (lines or planes) of these particular matrices:       1 2 1 0 1 0       A=0 0 and B=0 2 and C =2 0. 0 0 0 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "22. Forwhichright-handsides(findaconditiononb ,b ,b )arethesesystemssolvable? 1 2 3          (cid:34) (cid:35) 1 4 2 x b 1 4 b 1 1 1        x   1 (a)  2 8 4 x =b . (b)  2 9  =b . 2 2 2 x 2 −1 −4 −2 x b −1 −4 b 3 3 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "23. Adding row 1 of A to row 2 produces B. Adding column 1 to column 2 producesC. A combination of the columns of is also a combination of the columns of A. Which two matrices have the same column ? (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 2 1 2 1 3 A= and B= and C = . 2 4 3 6 2 6"
    },
    {
        "chapter": "VectorSpaces",
        "question": "24. For which vectors (b ,b ,b ) do these systems have a solution? 1 2 3           1 1 1 x b 1 1 1 x b 1 1 1 1           0 1 1x =b  and 0 1 1x =b . 2 2 2 2 0 0 1 x b 0 0 0 x b 3 3 3 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "25. (Recommended) If we add an extra column b to a matrix A, then the column space gets larger unless . Give an example in which the column space gets larger and an example in which it doesn’t. Why is Ax = b solvable exactly when the column space doesn’t get larger by including b?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "26. The columns of AB are combinations of the columns of A. This means: The column space of AB is contained in (possibly equal to) the column space of A. Give an example where the column spaces of A and AB are not equal."
    },
    {
        "chapter": "VectorSpaces",
        "question": "27. If A is any 8 by 8 invertible matrix, then its column space is . Why?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "28. True or false (with a counterexample if false)? (a) The vectors b that are not in the column space C(A) form a subspace. (b) If C(A) contains only the zero vector, then A is the zero matrix. (c) The column space of 2A equals the column space of A. (d) The column space of A−I equals the column space of A."
    },
    {
        "chapter": "VectorSpaces",
        "question": "29. Construct a 3 by 3 matrix whose column space contains (1,1,0) and (1,0,1) but not (1,1,1). Construct a 3 by 3 matrix whose column space is only a line."
    },
    {
        "chapter": "VectorSpaces",
        "question": "30. If the 9 by 12 system Ax=b is solvable for every b, then C(A)= ."
    },
    {
        "chapter": "VectorSpaces",
        "question": "31. Why isn’t R2 a subspace of R3?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 Solving Ax=0 and Ax=b Chapter 1 concentrated on square invertible matrices. There was one solution to Ax=b and it was x=−A−1b. That solution was found by elimination (not by computing A−1). A rectangular matrix brings new possibilities—U may not have a full set of pivots. This section goes onward from U to a reduced form R—the simplest matrix that elimina- tion can give. R reveals all solutions immediately. For an invertible matrix, the nullspace contains only x=0 (multiply Ax=0 by A−1). The column space is the whole space (Ax=b has a solution for every b). The new ques- tions appear when the nullspace contains more than the zero vector and/or the column space contains less than all vectors:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Anyvectorx inthenullspacecanbeaddedtoaparticularsolutionx . Thesolutions n p to all linear equations have this form, x=x +x : p n Complete solution Ax =b and Ax =0 produce A(x +x )=b. p n p n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. When the column space doesn’t contain every b in Rm, we need the conditions on b that make Ax=b solvable. A3by4examplewillbeagoodsize. WewillwritedownallsolutionstoAx=0. We will find the conditions for b to lie in the column space (so that Ax=b is solvable). The 1 by 1 system 0x=b, one equation and one unknown, shows two possibilities: 0x = b has no solution unless b = 0. The column space of the 1 by 1 zero matrix contains only b=0. 0x=0 has infinitely many solutions. The nullspace contains all x. A particular solution is x =0, and the complete solution is x=x +x =0+(any x). p p n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 SolvingAx=0andAx=b 87 (cid:163) (cid:164) Simple, I admit. If you move up to 2 by 2, it’s more interesting. The matrix 1 1 is not 2 2 invertible: y+z=b and 2y+2z=b usually have no solution. 1 2 There is no solution unless b = 2b . The column space of A contains only 2 1 those b’s, the multiples of (1,2). When b = 2b there are infinitely many solutions. A particular solution to 2 1 y+z = 2 and 2y+2z = 4 is x = (1,1). The nullspace of A in Figure 2.2 p contains (−1,1) and all its multiples x =(−c,c): n (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) Complete y + z = 2 1 −1 1−c is solved by x +x = +c = . p n solution 2y + 2z = 4 1 1 1+c z all xn line of all solutions x = xp +xn 1 −1 1 = shortest particular solution xp 1 b b(cid:2) (cid:3) (cid:2) (cid:3) 2 = MATLAB’s particular solution A\\b 0 (cid:2) (cid:3) y b b nullspace Axn = 0 (cid:163) (cid:164) (cid:163) (cid:164) Figure2.2: TheparallellinesofsolutionstoAx =0and 11 [y]= 2 . n 22 z 4 Echelon FormU and Row Reduced Form R We start by simplifying this 3 by 4 matrix, first toU and then further to R:   1 3 3 2   Basic example A= 2 6 9 7. −1 −3 3 4 The pivot a =1 is nonzero. The usual elementary operations will produce zeros in the 11 first column below this pivot. The bad news appears in column 2:   1 3 3 2   No pivot in column 2 A→0 0 3 3. 0 0 6 6 The candidate for the second pivot has become zero: unacceptable. We look below that zero for a nonzero entry—intending to carry out a row exchange. In this case the entry below it is also zero. If A were square, this would signal that the matrix was singular. Witharectangularmatrix,wemustexpecttroubleanyway,andthereisnoreasontostop. Allwecandoistogoontothenextcolumn,wherethepivotentryis3. Subtractingtwice the second row from the third, we arrive atU:   1 3 3 2   Echelon matrixU U =0 0 3 3. 0 0 0 0 Strictly speaking, we proceed to the fourth column. A zero is in the third pivot position, and nothing can be done. U is upper triangular, but its pivots are not on the main diago- nal. The nonzero entries ofU have a “staircase pattern,” or echelon form. For the 5 by 8 case in Figure 2.3, the starred entries may or may not be zero.     • ∗ ∗ ∗ ∗ ∗ ∗ ∗ 1 0 ∗ 0 ∗ ∗ ∗ 0          0 • ∗ ∗ ∗ ∗ ∗ ∗   0 1 ∗ 0 ∗ ∗ ∗ 0          U = 0 0 0 • ∗ ∗ ∗ ∗  R= 0 0 0 1 ∗ ∗ ∗ 0           0 0 0 0 0 0 0 •   0 0 0 0 0 0 0 1      0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Figure2.3: Theentriesofa5by8echelonmatrixU anditsreducedformR. We can always reach this echelon formU, with zeros below the pivots:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. The pivots are the first nonzero entries in their rows."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Below each pivot is a column of zeros, obtained by elimination."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Eachpivotliestotherightofthepivotintherowabove. Thisproducesthestaircase pattern, and zero rows come last. Since we started with A and ended with U, the reader is certain to ask: Do we have A = LU as before? There is no reason why not, since the elimination steps have not changed. Each step still subtracts a multiple of one row from a row beneath it. The inverse of each step adds back the multiple that was subtracted. These inverses come in the right order to put the multipliers directly into L:   1 0 0   Lower triangular L= 2 1 0 and A=LU. −1 2 1 Note that L is square. It has the same number of rows as A andU. The only operation not required by our example, but needed in general, is row ex- change by a permutation matrix P. Since we keep going to the next column when no pivots are available, there is no need to assume that A is nonsingular. Here is PA = LU for all matrices:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 SolvingAx=0andAx=b 89 2B For any m by n matrix A there is a permutation P, a lower triangular L with unit diagonal, and an m by n echelon matrixU, such that PA=LU. Now comes R. We can go further than U, to make the matrix even simpler. Divide the second row by its pivot 3, so that all pivots are 1. Then use the pivot row to produce zero above the pivot. This time we subtract a row from a higher row. The final result (the best form we can get) is the reduced row echelon form R:       1 3 3 2 1 3 3 2 1 3 0 -1       0 0 3 3−→0 0 1 1−→0 0 1 1=R. 0 0 0 0 0 0 0 0 0 0 0 0 This matrix R is the final result of elimination on A. MATLAB would use the command R = rref(A). Of course rref(R) would give R again! What is the row reduced form of a square invertible matrix? In that case R is the identity matrix. There is a full set of pivots, all equal to 1, with zeros above and below. So rref(A) = I, when A is invertible. For a 5 by 8 matrix with four pivots, Figure 2.3 shows the reduced form R. It still contains an identity matrix, in the four pivot rows and four pivot columns. From R we will quickly find the nullspace of A. Rx = 0 has the same solutions as Ux = 0 and Ax=0. Pivot Variables and Free Variables Our goal is to read off all the solutions to Rx=0. The pivots are crucial:       u Nullspace of R 1 3 0 −1   0  v   (pivot columns Rx=0 0 1 1  =0. w in boldface) 0 0 0 0 0 y The unknowns u, v, w, y go into two groups. One group contains the pivot variables, those that correspond to columns with pivots. The first and third columns contain the pivots, so u and w are the pivot variables. The other group is made up of the free variables, corresponding to columns without pivots. These are the second and fourth columns, so v and y are free variables. TofindthemostgeneralsolutiontoRx=0(or,equivalently,toAx=0)wemayassign arbitrary values to the free variables. Suppose we call these values simply v and y. The pivot variables are completely determined in terms of v and y: u+3v−y=0 yields u=−3v+y Rx=0 (1) w+y=0 yields w = −y There is a “double infinity” of solutions, with v and y free and independent. The com- plete solution is a combination of two special solutions:       −3v+y −3 1 Nullspace contains        v   1   0  all combinations x= =v +y . (2)  −y   0  −1 of special solutions y 0 1 Please look again at this complete solution to Rx = 0 and Ax = 0. The special solution (−3,1,0,0) has free variables v = 1, y = 0. The other special solution (1,0,−1,1) has v=0andy=1. Allsolutionsarelinearcombinationsofthesetwo. Thebestwaytofind all solutions to Ax=0 is from the special solutions:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. After reaching Rx=0, identify the pivot variables and free variables."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Giveonefreevariablethevalue1,settheotherfreevariablesto0,andsolveRx=0 for the pivot variables. This x is a special solution."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Everyfreevariableproducesitsown“specialsolution”bystep2. Thecombinations of special solutions form the nullspace—all solutions to Ax=0. Within the four-dimensional space of all possible vectors x, the solutions to Ax = 0 form a two-dimensional subspace—the nullspace of A, In the example, N(A) is gener- ated by the special vectors (−3,1,0,0) and (1,0,−1,1). The combinations of these two vectors produce the whole nullspace. Here is a little trick. The special solutions are especially easy from R. The numbers 3 and 0 and −1 and 1 lie in the “nonpivot columns” of R. Reverse their signs to find the pivot variables (not free) in the special solutions. I will put the two special solutions from equation (2) into a nullspace matrix N, so you see this neat pattern:   −3 1 not free Nullspace matrix    1 0  free (columns are N =   0 −1 not free special solutions) 0 1 free The free variables have values 1 and 0. When the free columns moved to the right- hand side of equation (2), their coefficients 3 and 0 and −1 and 1 switched sign. That determined the pivot variables in the special solutions (the columns of N). Thisistheplacetorecognizeoneextremelyimportanttheorem. Supposeamatrixhas more columns than rows, n>m. Since m rows can hold at most m pivots, there must be at least n−m free variables. There will be even more free variables if some rows of R reduce to zero; but no matter what, at least one variable must be free. This free variable can be assigned any value, leading to the following conclusion: 2C If Ax = 0 has more unknowns than equations (n > m), it has at least one special solution: There are more solutions than the trivial x=0."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 SolvingAx=0andAx=b 91 Theremustbeinfinitelymanysolutions,sinceanymultiplecxwillalsosatisfyA(cx)="
    },
    {
        "chapter": "VectorSpaces",
        "question": "0. The nullspace contains the line through x. And if there are additional free variables, the nullspace becomes more than just a line in n-dimensional space. The nullspace has the same “dimension” as the number of free variables and special solutions. This central idea—the dimension of a subspace—is made precise in the next section. Wecountthefreevariablesforthenullspace. Wecountthepivotvariablesforthecolumn space! Solving Ax=b,Ux=c, and Rx=d The case b (cid:54)= 0 is quite different from b = 0. The row operations on A must act also on the right-hand side (on b). We begin with letters (b ,b ,b ) to find the solvability 1 2 3 condition—for b to lie in the column space. Then we choose b = (1,5,5) and find all solutions x. For the original example Ax=b=(b ,b ,b ), apply to both sides the operations that 1 2 3 led from A toU. The result is an upper triangular systemUx=c:       u 1 3 3 2   b 1  v   Ux=c 0 0 3 3 = b −2b . (3) 2 1 w 0 0 0 0 b −2b +5b 3 2 1 y The vector c on the right-hand side, which appeared after the forward elimination steps, is just L−1b as in the previous chapter. Start now withUx=c. It is not clear that these equations have a solution. The third equation is very much in doubt, because its left-hand side is zero. The equations are inconsistent unless b − 3 2b +5b = 0. Even though there are more unknowns than equations, there may be no 2 1 solution. Weknowanotherwayofansweringthesamequestion: Ax=bcanbesolvedif and only if b lies in the column space of A. This subspace comes from the four columns of A (not ofU!):         Columns of A 1 3 3 2         “span” the  2 ,  6 , 9, 7. column space −1 −3 3 4 Even though there are four vectors, their combinations only fill out a plane in three- dimensional space. Column 2 is three times column 1. The fourth column equals the third minus the first. These dependent columns, the second and fourth, are exactly the ones without pivots. The column space C(A) can be described in two different ways. On the one hand, it is the plane generated by columns 1 and 3. The other columns lie in that plane, and contribute nothing new. Equivalently, it is the plane of all vectors b that satisfy b −2b +5b = 0; this is the constraint if the system is to be solvable. Every column 3 2 1 satisfies this constraint, so it is forced on b! Geometrically, we shall see that the vector (5,−2,1) is perpendicular to each column. If b belongs to the column space, the solutions of Ax = b are easy to find. The last equation in Ux = c is 0 = 0. To the free variables v and y, we may assign any values, as before. The pivot variables u and w are still determined by back-substitution. For a specific example with b −2b +5b =0, choose b=(1,5,5): 3 2 1       u 1 3 3 2   1  v   Ax=b  2 6 9 7 =5. w −1 −3 3 4 5 y Forward elimination producesU on the left and c on the right:       u 1 3 3 2   1  v   Ux=c 0 0 3 3 =3. w 0 0 0 0 0 y The last equation is 0=0, as expected. Back-substitution gives 3w+3y=3 or w=1−y u+3v+3w+2y=1 or u=−2−3v+y. Again there is a double infinity of solutions: v and y are free, u and w are not:         u −2 −3 1         Complete solution v  0   1   0  x= = +v +y . (4) x=x +x w  1   0  −1 p n y 0 0 1 This has all solutions to Ax =0, plus the new x =(−2,0,1,0). That x is a particular p p solution to Ax = b. The last two terms with v and y yield more solutions (because they satisfy Ax = 0). Every solution to Ax = b is the sum of one particular solution and a solution to Ax=0: x =x +x complete particular nullspace The particular solution in equation (4) comes from solving the equation with all free variables set to zero. That is the only new part, since the nullspace is already computed. When you multiply the highlighted equation by A, you get Ax =b+0. complete Geometrically, the solutions again fill a two-dimensional surface—but it is not a sub- space. It does not contain x = 0. It is parallel to the nullspace we had before, shifted by the particular solution x as in Figure 2.2. Equation (4) is a good way to write the p answer:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Reduce Ax=b toUx=c."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 SolvingAx=0andAx=b 93"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. With free variables =0, find a particular solution to Ax =b andUx =c. p p"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Find the special solutions to Ax = 0 (or Ux = 0 or Rx = 0). Each free variable, in turn, is 1. Then x=x +(any combination x of special solutions). p n WhentheequationwasAx=0,theparticularsolutionwasthezerovector! Itfitsthepat- tern, but x =0 was not written in equation (2). Now x is added to the nullspace particular p solutions, as in equation (4). Question: How does the reduced form R make this solution even clearer? You will see it in our example. Subtract equation 2 from equation 1, and then divide equation 2 by its pivot. On the left-hand side, this produces R, as before. On the right-hand side, these operations change c=(1,3,0) to a new vector d =(−2,1,0):       u 1 3 0 −1   −2 Reduced equation  v   0 0 1 1  = 1 . (5) Rx=d w 0 0 0 0 0 y Our particular solution x , (one choice out of many) has free variables v = y = 0. p Columns 2 and 4 can be ignored. Then we immediately have u=−2 and w=1, exactly as in equation (4). The entries of d go directly into x . This is because the identity p matrix is sitting in the pivot columns of R! Let me summarize this section, before working a new example. Elimination reveals the pivot variables and free variables. If there are r pivots, there are r pivot variables and n−r free variables. That important number r will be given a name—it is the rank of the matrix. 2D Suppose elimination reduces Ax = b toUx = c and Rx = d, with r pivot rows and r pivot columns. The rank of those matrices is r. The last m−r rows ofU and R are zero, so there is a solution only if the last m−r entries of c and d are also zero. The complete solution is x = x +x . One particular solution x has all free p n p variables zero. Its pivot variables are the first r entries of d, so Rx =d. p The nullspace solutions x are combinations of n−r special solutions, with n one free variable equal to 1. The pivot variables in that special solution can be found in the corresponding column of R (with sign reversed). You see how the rank r is crucial. It counts the pivot rows in the “row space” and the pivot columns in the column space. There are n−r special solutions in the nullspace. There are m−r solvability conditions on b or c or d. Another Worked Example The full picture uses elimination and pivot columns to find the column space, nullspace, and rank. The 3 by 4 matrix A has rank 2: 1x + 2x + 3x + 5x = b 1 2 3 4 1 Ax=b is 2x + 4x + 8x + 12x = b (6) 1 2 3 4 2 3x + 6x + 7x + 13x = b 1 2 3 4 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Reduce [A b] to [U c], to reach a triangular systemUx=c."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Find the condition on b , b , b to have a solution. 1 2 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Describe the column space of A: Which plane in R3?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. Describe the nullspace of A: Which special solutions in R4?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. Find a particular solution to Ax=(0,6,−6) and the complete x +x . p n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. Reduce [U c] to [R d]: Special solutions from R and x from d. p Solution. (Notice how the right-hand side is included as an extra column!)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. The multipliers in elimination are 2 and 3 and −1, taking [A b] to [U c].       1 2 3 5 b 1 2 3 5 b 1 2 3 5 b 1 1 1        2 4 8 12 b → 0 0 2 2 b −2b → 0 0 2 2 b −2b . 2 2 1 2 1 3 6 7 13 b 0 0 −2 −2 b −3b 0 0 0 0 b +b −5b 3 3 1 3 2 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. The last equation shows the solvability condition b +b −5b =0. Then 0=0. 3 2 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. ThecolumnspaceofAistheplanecontainingallcombinationsofthepivotcolumns (1,2,3) and (3,8,7). Seconddescription: Thecolumnspacecontainsallvectorswithb +b −5b =0. 3 2 1 ThatmakesAx=bsolvable, so bisin thecolumn space. Allcolumns ofA passthis test b +b −5b =0. This is the equation for the plane (in the first description of 3 2 1 the column space)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. The special solutions in N have free variables x =1, x =0 and x =0, x =1: 2 4 2 4   Nullspace matrix −2 −2   Special solutions to Ax=0  1 0  N = . Back-substitution inUx=0  0 −1 Just switch signs in Rx=0 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 SolvingAx=0andAx=b 95"
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. Choose b = (0,6,−6), which has b +b −5b = 0. Elimination takes Ax = b to 3 2 1 Ux=c=(0,6,0). Back-substitute with free variables =0:   −9    0  free Particular solution to Ax =(0,6,−6) x =  p p  3  0 free The complete solution to Ax=(0,6,−6) is (this x ) + (all x ). p n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Constructasystemwithmoreunknownsthanequations,butnosolution. Changethe right-hand side to zero and find all solutions x . n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Reduce A and B to echelon form, to find their ranks. Which variables are free?     1 2 0 1 1 2 3     A=0 1 1 0 B=4 5 6. 1 2 0 1 7 8 9 Find the special solutions to Ax=0 and Bx=0. Find all solutions."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Find the echelon formU, the free variables, and the special solutions: (cid:34) (cid:35) (cid:34) (cid:35) 0 1 0 3 b 1 A= , b= . 0 2 0 6 b 2 Ax = b is consistent (has a solution) when b satisfies b = . Find the complete 2 solution in the same form as equation (4)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. Carry out the same steps as in the previous problem to find the complete solution of Mx=b:     0 0 b 1     1 2 b  2 M = , b= . 0 0 b  3 3 6 b 4"
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. Write the complete solutions x=x +x to these systems, as in equation (4): p n     (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) u u 1 2 2   1 1 2 2   1 v= v= . 2 4 5 4 2 4 4 4 w w"
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. Describe the set of attainable right-hand sides b (in the column space) for     (cid:34) (cid:35) 1 0 b 1   u   0 1 =b , 2 v 2 3 b 3 by finding the constraints on b that turn the third equation into 0=0 (after elimina- tion). What is the rank, and a particular solution?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "7. Find the value of c that makes it possible to solve Ax=b, and solve it: u + v + 2w = 2 2u + 3v − w = 5 3u + 4v + w = c."
    },
    {
        "chapter": "VectorSpaces",
        "question": "8. Under what conditions on b and b (if any) does Ax=b have a solution? 1 2 (cid:34) (cid:35) (cid:34) (cid:35) 1 2 0 3 b 1 A= , b= . 2 4 0 7 b 2 Find two vectors in the nullspace of A, and the complete solution to Ax=b."
    },
    {
        "chapter": "VectorSpaces",
        "question": "9. (a) Find the special solutions toUx=0. ReduceU to R and repeat:       x 1 1 2 3 4   0  x    2 Ux=0 0 1 2 =0. x  3 0 0 0 0 0 x 4 (b) If the right-hand side is changed from (0,0,0) to (a,b,0), what are all solutions?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "10. Find a 2 by 3 system Ax=b whose complete solution is     1 1     x=2+w3. 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 SolvingAx=0andAx=b 97 Find a 3 by 3 system with these solutions exactly when b +b =b . 1 2 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "11. Write a 2 by 2 system Ax=b with many solutions x but no solution x . (Therefore n p the system has no solution.) Which b’s allow an x ? p"
    },
    {
        "chapter": "VectorSpaces",
        "question": "12. Which of these rules give a correct definition of the rank of A? (a) The number of nonzero rows in R. (b) The number of columns minus the total number of rows. (c) The number of columns minus the number of free columns. (d) The number of 1s in R."
    },
    {
        "chapter": "VectorSpaces",
        "question": "13. Find the reduced row echelon forms R and the rank of these matrices: (a) The 3 by 4 matrix of all 1s. (b) The 4 by 4 matrix with a =(−1)ij. ij (c) The 3 by 4 matrix with a =(−1)j. ij"
    },
    {
        "chapter": "VectorSpaces",
        "question": "14. Find R for each of these (block) matrices, and the special solutions:   (cid:34) (cid:35) 0 0 0 (cid:104) (cid:105)   A A A=0 0 3 B= A A C = . A 0 2 4 6"
    },
    {
        "chapter": "VectorSpaces",
        "question": "15. If the r pivot variables come first, the reduced R must look like (cid:34) (cid:35) I F I is r by r R= 0 0 F is r by n−r What is the nullspace matrix N containing the special solutions?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "16. Suppose all r pivot variables come last. Describe the four blocks in the m by n reduced echelon form (the block B should be r by r): (cid:34) (cid:35) A B R= . C D What is the nullspace matrix N of special solutions? What is its shape?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "17. (Silly problem) Describe all 2 by 3 matrices A and A with row echelon forms R 1 2 1 and R , such that R +R is the row echelon form of A +A . Is it true that R =A 2 1 2 1 2 1 1 and R =A in this case? 2 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "18. If A has r pivot columns, then AT has r pivot columns. Give a 3 by 3 example for which the column numbers are different for A and AT."
    },
    {
        "chapter": "VectorSpaces",
        "question": "19. What are the special solutions to Rx=0 and RTy=0 for these R?     1 0 2 3 0 1 2     R=0 1 4 5 R=0 0 0. 0 0 0 0 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "20. IfAhasrankr,thenithasanrbyrsubmatrixSthatisinvertible. Findthatsubmatrix S from the pivot rows and pivot columns of each A:   (cid:34) (cid:35) (cid:34) (cid:35) 0 1 0 1 2 3 1 2 3   A= A= A=0 0 0. 1 2 4 2 4 6 0 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "21. Explain why the pivot rows and pivot columns of A (not R) always give an r by r invertible submatrix of A."
    },
    {
        "chapter": "VectorSpaces",
        "question": "22. Find the ranks of AB and AM (rank 1 matrix times rank 1 matrix): (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 2 2 1 4 1 b A= and B= and M = . 2 4 3 1.5 6 c bc"
    },
    {
        "chapter": "VectorSpaces",
        "question": "23. Multiplying the rank 1 matrices A = uvT and B = wzT gives uzT times the number . AB has rank 1 unless =0."
    },
    {
        "chapter": "VectorSpaces",
        "question": "24. Every column of AB is a combination of the columns of A. Then the dimensions of the column spaces give rank(AB)≤rank(A). Problem: Prove also that rank(AB)≤rank(B)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "25. (Important)SupposeAandBarenbynmatrices,andAB=I. Provefromrank(AB)≤ rank(A)thattherankofAisn. SoAisinvertibleandBmustbeitstwo-sidedinverse. Therefore BA=I (which is not so obvious!)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "26. If A is 2 by 3 and C is 3 by 2, show from its rank that CA (cid:54)= I. Give an example in which AC =I. For m<n, a right inverse is not a left inverse."
    },
    {
        "chapter": "VectorSpaces",
        "question": "27. SupposeAandBhavethesamereduced-rowechelonformR. Explainhowtochange A to B by elementary row operations. So B equals an matrix times A."
    },
    {
        "chapter": "VectorSpaces",
        "question": "28. Every m by n matrix of rank r reduces to (m by r) times (r by n): A=(pivot columns of A)(first r rows of R)=(COL)(ROW). Write the 3 by 4 matrix A at the start of this section as the product of the 3 by 2 matrix from the pivot columns and the 2 by 4 matrix from R:   1 3 3 2   A= 2 6 9 7 −1 −3 3 4"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 SolvingAx=0andAx=b 99"
    },
    {
        "chapter": "VectorSpaces",
        "question": "29. Suppose A is an m by n matrix of rank r. Its reduced echelon form is R. Describe exactly the reduced row echelon form of RT (not AT)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "30. (Recommended) Execute the six steps following equation (6) to find the column space and nullspace of A and the solution to Ax=b:       2 4 6 4 b 4 1       A=2 5 7 6 b=b =3. 2 2 3 5 2 b 5 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "31. For every c, find R and the special solutions to Ax=0:   (cid:34) (cid:35) 1 1 2 2   1−c 2 A=2 2 4 4 and A= . 0 2−c 1 c 2 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "32. What is the nullspace matrix N (of special solutions) for A, B,C? (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) I I A= I I and B= and C = I I I . 0 0 Problems 33–36 are about the solution of Ax=b. Follow the steps in the text to x and x . Reduce the augmented matrix [A b]. p n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "33. Find the complete solutions of       x x+3y+3z=1 1 3 1 2   1  y   2x+6y+9z=5 and 2 6 4 8 =3. z −x−3y+3z=5 0 0 2 4 1 t"
    },
    {
        "chapter": "VectorSpaces",
        "question": "34. Under what condition on b , b , b is the following system solvable? Include b as a 1 2 3 fourth column in [A b]. Find all solutions when that condition holds: x+2y−2z=b 1 2x+5y−4z=b 2 4x+9y−8z=b . 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "35. What conditions on b , b , b , b make each system solvable? Solve for x: 1 2 3 4           1 2 b 1 2 3 b (cid:34) (cid:35) 1 1       x 1   2 4 x b  2 4 6   b  1 2 2   =   x = . 2 2 5 x b  2 5 7  b  2 3 3 x 3 3 9 b 3 9 12 b 4 4"
    },
    {
        "chapter": "VectorSpaces",
        "question": "36. Which vectors (b ,b ,b ) are in the column space of A? Which combinations of the 1 2 3 rows of A give zero?     1 2 1 1 1 1     (a) A=2 6 3 (b) A=1 2 4. 0 2 5 2 4 8"
    },
    {
        "chapter": "VectorSpaces",
        "question": "37. Why can’t a 1 by 3 system have x =(2,4,0) and x = any multiple of (1,1,1)? p n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "38. (a) If Ax=b has two solutions x and x , find two solutions to Ax=0. 1 2 (b) Then find another solution to Ax=b."
    },
    {
        "chapter": "VectorSpaces",
        "question": "39. Explain why all these statements are false: (a) The complete solution is any linear combination of x and x . p n (b) A system Ax=b has at most one particular solution. (c) The solution x with all free variables zero is the shortest solution (minimum p length (cid:107)x(cid:107)). (Find a 2 by 2 counterexample.) (d) If A is invertible there is no solution x in the nullspace. n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "40. Suppose column 5 of U has no pivot. Then x is a variable. The zero vector 5 (is) (is not) the only solution to Ax = 0. If Ax = b has a solution, then it has solutions."
    },
    {
        "chapter": "VectorSpaces",
        "question": "41. If you know x (free variables = 0) and all special solutions for Ax = b, find x and p p all special solutions for these systems: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) x A b Ax=2b A A =b x = . X A b"
    },
    {
        "chapter": "VectorSpaces",
        "question": "42. If Ax = b has infinitely many solutions, why is it impossible for Ax = B (new right- hand side) to have only one solution? Could Ax=B have no solution?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "43. Choose the number q so that (if possible) the ranks are (a) 1, (b) 2, (c) 3:   (cid:34) (cid:35) 6 4 2   3 1 3 A=−3 −2 −1 and B= . q 2 q 9 6 q"
    },
    {
        "chapter": "VectorSpaces",
        "question": "44. Give examples of matrices A for which the number of solutions to Ax=b is (a) 0 or 1, depending on b. (b) ∞, regardless of b. (c) 0 or ∞, depending on b. (d) 1, regardless of b."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.2 SolvingAx=0andAx=b 101"
    },
    {
        "chapter": "VectorSpaces",
        "question": "45. Write all known relations between r and m and n if Ax=b has (a) no solution for some b. (b) infinitely many solutions for every b. (c) exactly one solution for some b, no solution for other b. (d) exactly one solution for every b."
    },
    {
        "chapter": "VectorSpaces",
        "question": "46. Apply Gauss-Jordan elimination (right-hand side becomes extra column) toUx = 0 andUx=c. Reach Rx=0 and Rx=d: (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) 1 2 3 0 1 2 3 5 U 0 = and U c = . 0 0 4 0 0 0 4 8 Solve Rx =0 to find x (its free variable is x =1). Solve Rx =d to find x (its free n 2 p variable is x =0). 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "47. Apply elimination with the extra column to reach Rx=0 and Rx=d:     3 0 6 0 3 0 6 9 (cid:104) (cid:105) (cid:104) (cid:105)     U 0 =0 0 2 0 and U c =0 0 2 4. 0 0 0 0 0 0 0 5 Solve Rx=0 (free variable =1). What are the solutions to Rx=d?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "48. Reduce toUx=c (Gaussian elimination) and then Rx=d:       x 1 1 0 2 3   2  x    2 Ax=1 3 2 0 = 5 =b. x  3 2 0 4 9 10 x 4 Find a particular solution x and all nullspace solutions x . p n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "49. Find A and B with the given property or explain why you can’t. (cid:104) (cid:105) (cid:163) (cid:164) 1 (a) The only solution to Ax= 2 is x= 0 . 1 3 (cid:104) (cid:105) (cid:163) (cid:164) 1 (b) The only solution to Bx= 0 is x= 2 . 1 3 (cid:163) (cid:164) (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "50. The complete solution to Ax= 1 is x= 1 +c 0 . Find A. 3 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "51. The nullspace of a 3 by 4 matrix A is the line through (2,3,1,0). (a) What is the rank of A and the complete solution to Ax=0? (b) What is the exact row reduced echelon form R of A?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "52. Reduce these matrices A and B to their ordinary echelon formsU:     1 2 2 4 6 2 4 2     (a) A=1 2 3 6 9 (b) B=0 4 4. 0 0 1 2 3 0 8 8 Find a special solution for each free variable and describe every solution to Ax = 0 and Bx = 0. Reduce the echelon forms U to R, and draw a box around the identity matrix in the pivot rows and pivot columns."
    },
    {
        "chapter": "VectorSpaces",
        "question": "53. True or False? (Give reason if true, or counterexample to show it is false.) (a) A square matrix has no free variables. (b) An invertible matrix has no free variables. (c) An m by n matrix has no more than n pivot variables. (d) An m by n matrix has no more than m pivot variables."
    },
    {
        "chapter": "VectorSpaces",
        "question": "54. Is there a 3 by 3 matrix with no zero entries for whichU =R=I?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "55. Put as many 1s as possible in a 4 by 7 echelon matrix U and in a reduced form R whose pivot columns are 2, 4, 5."
    },
    {
        "chapter": "VectorSpaces",
        "question": "56. Suppose column 4 of a 3 by 5 matrix is all 0s. Then x is certainly a variable. 4 The special solution for this variable is the vector x= ."
    },
    {
        "chapter": "VectorSpaces",
        "question": "57. Suppose the first and last columns of a 3 by 5 matrix are the same (nonzero). Then is a free variable. Find the special solution for this variable."
    },
    {
        "chapter": "VectorSpaces",
        "question": "58. The equation x−3y−z = 0 determines a plane in R3. What is the matrix A in this equation? Which are the free variables? The special solutions are (3,1,0) and . The parallel plane x−3y−z=12 contains the particular point (12,0,0). All points on this plane have the following form (fill in the first components):         x         y=0+y1+z0. z 0 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "59. Suppose column 1 + column 3 + column 5 = 0 in a 4 by 5 matrix with four pivots. Which column is sure to have no pivot (and which variable is free)? What is the special solution? What is the nullspace? Problems 60–66 ask for matrices (if possible) with specific properties."
    },
    {
        "chapter": "VectorSpaces",
        "question": "60. Construct a matrix whose nullspace consists of all combinations of (2,2,1,0) and (3,1,0,1)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "61. Construct a matrix whose nullspace consists of all multiples of (4,3,2,1)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.3 LinearIndependence,Basis,andDimension 103"
    },
    {
        "chapter": "VectorSpaces",
        "question": "62. Construct a matrix whose column space contains (1,1,5) and (0,3.1) and whose nullspace contains (1,1,2)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "63. Construct a matrix whose column space contains (1,1,0) and (0,1,1) and whose nullspace contains (1,0,1) and (0,0,1)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "64. Construct a matrix whose column space contains (1,1,1) and whose nullspace is the line of multiples of (1,1,1,1)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "65. Construct a 2 by 2 matrix whose nullspace equals its column space."
    },
    {
        "chapter": "VectorSpaces",
        "question": "66. Why does no 3 by 3 matrix have a nullspace that equals its column space?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "67. The reduced form R of a 3 by 3 matrix with randomly chosen entries is almost sure to be . What R is virtually certain if the random A is 4 by 3?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "68. Show by example that these three statements are generally false: (a) A and AT have the same nullspace. (b) A and AT have the same free variables. (c) If R is the reduced form rref(A) then RT is rref(AT)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "69. If the special solutions to Rx=0 are in the columns of these N, go backward to find the nonzero rows of the reduced matrices R:       2 3 0       N =1 0 and N =0 and N =  (empty 3 by 1). 0 1 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "70. Explain why A and −A always have the same reduced echelon form R."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.3 Linear Independence, Basis, and Dimension By themselves, the numbers m and n give an incomplete picture of the true size of a linear system. The matrix in our example had three rows and four columns, but the third row was only a combination of the first two. After elimination it became a zero row, It had no effect on the homogeneous problem Ax = 0. The four columns also failed to be independent, and the column space degenerated into a two-dimensional plane. The important number that is beginning to emerge (the true size) is the rank r. The rank was introduced as the number of pivots in the elimination process. Equivalently, the final matrixU has r nonzero rows. This definition could be given to a computer. But it would be wrong to leave it there because the rank has a simple and intuitive meaning: The rank counts the number of genuinely independent rows in the matrix A. We want definitions that are mathematical rather than computational. The goal of this section is to explain and use four ideas:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Linear independence or dependence."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Spanning a subspace."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Basis for a subspace (a set of vectors)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. Dimension of a subspace (a number). The first step is to define linear independence. Given a set of vectors v ,...,v , we 1 k look at their combinations c v +c v +···+c v . The trivial combination, with all 1 1 2 2 k k weights c =0, obviously produces the zero vector: 0v +···+0v =0. The question is i 1 k whether this is the only way to produce zero. If so, the vectors are independent. If any other combination of the vectors gives zero, they are dependent. 2E Supposec v +···+c v =0onlyhappenswhenc =···=c =0. Then 1 1 k k 1 k the vectors v ,...,v are linearly independent. If any c’s are nonzero, the v’s 1 k are linearly dependent. One vector is a combination of the others. Linear dependence is easy to visualize in three-dimensional space, when all vectors go out from the origin. Two vectors are dependent if they lie on the same line. Three vectors are dependent if they lie in the same plane. A random choice of three vectors, without any special accident, should produce linear independence (not in a plane). Four vectors are always linearly dependent in R3. Example 1. If v = zero vector, then the set is linearly dependent. We may choose 1 c =3 and all other c =0; this is a nontrivial combination that produces zero. 1 i Example 2. The columns of the matrix   1 3 3 2   A= 2 6 9 5 −1 −3 3 0 arelinearlydependent,sincethesecondcolumnisthreetimesthefirst. Thecombination of columns with weights −3, 1, 0, 0 gives a column of zeros. The rows are also linearly dependent; row 3 is two times row 2 minus five times row"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. (Thisisthesameasthecombinationofb ,b ,b ,thathadtovanishontheright-hand 1 2 3 side in order for Ax = b to be consistent. Unless b −2b +5b = 0, the third equation 3 2 1 would not become 0=0.) Example 3. The columns of this triangular matrix are linearly independent:   3 4 2   No zeros on the diagonal A=0 1 5. 0 0 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.3 LinearIndependence,Basis,andDimension 105 Look for a combination of the columns that makes zero:         3 4 2 0         Solve Ac=0 c 0+c 1+c 5=0. 1 2 3 0 0 2 0 We have to show that c , c , c are all forced to be zero. The last equation gives 1 2 3 c =0. Thenthenextequationgivesc =0,andsubstitutingintothefirstequationforces 3 2 c =0. The only combination to produce the zero vector is the trivial combination. The 1 nullspace of A contains only the zero vector c =c =c =0. 1 2 3 The columns of A are independent exactly when N(A)={zero vector}. A similar reasoning applies to the rows of A, which are also independent. Suppose c (3,4,2)+c (0,1,5)+c (0,0,2)=(0,0,0). 1 2 3 From the first components we find 3c =0 or c =0. Then the second components give 1 1 c =0, and finally c =0. 2 3 The nonzero rows of any echelon matrixU must be independent. Furthermore, if we pick out the columns that contain the pivots, they also are linearly independent. In our earlier example, with   1 3 3 2 Two independent rows   U =0 0 3 1, Two independent columns 0 0 0 0 the pivot columns 1 and 3 are independent. No set of three columns is independent, and certainly not all four. It is true that columns 1 and 4 are also independent, but if that last 1 were changed to 0 they would be dependent. It is the columns with pivots that are guaranteed to be independent. The general rule is this: 2F The r nonzero rows of an echelon matrix U and a reduced matrix R are linearly independent. So are the r columns that contain pivots. Example 4. The columns of the n by n identity matrix are independent:   1 0 · 0   0 1 · 0 I = . · · · 0 0 0 0 1 These columns e ,...,e represent unit vectors in the coordinate directions; in R4, 1 n         1 0 0 0         0 1 0 0 e = , e = , e = , e = . 1 2 3 4 0 0 1 0 0 0 0 1 Most sets of four vectors in R4 are independent. Those e’s might be the safest. Tocheckanysetofvectorsv ,...,v forindependence,puttheminthecolumnsofA. 1 n Then solve the system Ac=0; the vectors are dependent if there is a solution other than c=0. Withnofreevariables(rank n), thereisnonullspaceexceptc=0; thevectorsare independent. If the rank is less than n, at least one free variable can be nonzero and the columns are dependent. Onecasehasspecialimportance. Letthenvectorshavemcomponents,sothatAisan mbynmatrix. Supposenowthatn>m. Therearetoomanycolumnstobeindependents There cannot be n pivots, since there are not enough rows to hold them. The rank will be less than n. Every system Ac = 0 with more unknowns than equations has solutions c(cid:54)=0. 2G A set of n vectors in Rm must be linearly dependent if n>m. The reader will recognize this as a disguised form of 2C: Every m by n system Ax = 0 has nonzero solutions if n>m. Example 5. These three columns in R2 cannot be independent: (cid:34) (cid:35) 1 2 1 A= . 1 3 2 To find the combination of the columns producing zero we solve Ac=0: (cid:34) (cid:35) 1 2 1 A→U = . 0 1 1 If we give the value 1 to the free variable c , then back-substitution in Uc = 0 gives 3 c = −1, c = 1. With these three weights, the first column minus the second plus the 2 1 third equals zero: Dependence. Spanning a Subspace Now we define what it means for a set of vectors to span a space. The column space of A is spanned by the columns. Their combinations produce the whole space: 2H If a vector space V consists of all linear combinations of w ,...,w , then 1 (cid:96) these vectors span the space. Every vector v in V is some combination of the w’s: Every v comes from w’s v=c w +···+c w for some coefficients c . 1 1 (cid:96) (cid:96) i It is permitted that a different combination of w’s could give the same vector v. The c’s need not be unique, because the spanning set might be excessively large—it could include the zero vector, or even all vectors."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.3 LinearIndependence,Basis,andDimension 107 Example 6. The vectors w =(1,0,0), w =(0,1,0), and w =(−2,0,0) span a plane 1 2 3 (the x-y plane) in R3. The first two vectors also span this plane, whereas w and w span 1 3 only a line. Example7. ThecolumnspaceofAisexactlythespacethatisspannedbyitscolumns. The row space is spanned by the rows. The definition is made to order. Multiplying A by any x gives a combination of the columns; it is a vector Ax in the column space. The coordinate vectors e ,...,e coming from the identity matrix span Rn. Every 1 n vector b = (b ,...,b ) is a combination of those columns. In this example the weights 1 n are the components b themselves: b = b e +···+b e . But the columns of other i 1 1 n n matrices also span Rn! Basis for a Vector Space To decide if b is a combination of the columns, we try to solve Ax = b. To decide if the columns are independent, we solve Ax = 0. Spanning involves the column space, and independence involves the nullspace. The coordinate vectors e ,...,e span Rn 1 n and they are linearly independent. Roughly speaking, no vectors in that set are wasted. This leads to the crucial idea of a basis. 2I A basis for V is a sequence of vectors having two properties at once:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. The vectors are linearly independent (not too many vectors)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. They span the space V (not too few vectors). This combination of properties is absolutely fundamental to linear algebra. It means that every vector in the space is a combination of the basis vectors, because they span. It also means that the combination is unique: If v = a v +···+a v and also v = 1 1 k k b v +···+b v , then subtraction gives 0 = ∑(a −b )v . Now independence plays its 1 1 k k i i i part; every coefficient a −b must be zero. Therefore a = b . There is one and only i i i i one way to write v as a combination of the basis vectors. We had better say at once that the coordinate vectors e ,...,e are not the only basis 1 n for Rn. Some things in linear algebra are unique, but not this. A vector space has infinitely many different bases. Whenever a square matrix is invertible, its columns are independent—and they are a basis for Rn. The two columns of this nonsingular matrix are a basis for R2: (cid:34) (cid:35) 1 1 A= 2 3 Every two-dimensional vector is a combination of those (independent!) columns. Example 8. The x-y plane in Figure 2.4 is just R2. The vector v by itself is linearly 1 independent, but it fails to span R2. The three vectors v , v , v certainly span R2, but 1 2 3 arenotindependent. Anytwoofthesevectors, sayv andv , havebothproperties—they 1 2 span, and they are independent. So they form a basis. Notice again that a vector space does not have a unique basis. y v3 v2 x v1 Figure2.4: Aspanningsetv ,v ,v . Basesv ,v andv ,v andv ,v . 1 2 3 1 2 1 3 2 3 Example 9. ThesefourcolumnsspanthecolumnspaceofU,buttheyarenotindepen- dent:   1 3 3 2   Echelon matrix U =0 0 3 1. 0 0 0 0 There are many possibilities for a basis, but we propose a specific choice: The columns that contain pivots (in this case the first and third, which correspond to the basic vari- ables) are a basis for the column space. These columns are independent, and it is easy to see that they span the space. In fact, the column space of U is just the x-y plane within R3. C(U) is not the same as the column space C(A) before elimination—but the number of independent columns didn’t change. Tosummarize: Thecolumnsofanymatrixspanitscolumnspace. Iftheyareindepen- dent, theyareabasisforthecolumnspace—whetherthematrixissquareorrectangular. If we are asking the columns to be a basis for the whole space Rn, then the matrix must be square and invertible. Dimension of a Vector Space A space has infinitely many different bases, but there is something common to all of these choices. The number of basis vectors is a property of the space itself: 2J Any two bases for a vector space V contain the same number of vec- tors. This number, which is shared by all bases and expresses the number of “degrees of freedom” of the space, is the dimension of V. We have to prove this fact: All possible bases contain the same number of vectors. The x-y plane in Figure 2.4 has two vectors in every basis; its dimension is 2. In three"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Show that v , v , v are independent but v , v , v , v are dependent: 1 2 3 1 2 3 4         1 1 1 2         v =0 v =1 v =1 v =3. 1 2 3 4 0 0 1 4 Solve c v +···+c v =0 or Ac=0. The v’s go in the columns of A. 1 1 4 4"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Find the largest possible number of independent vectors among             1 1 1 0 0 0             −1  0   0   1   1   0  v =  v =  v =  v =  v =  v = . 1 2 3 4 5 6  0  −1  0  −1  0   1  0 0 −1 0 −1 −1 This number is the of the space spanned by the v’s."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Prove that if a=0, d =0, or f =0 (3 cases), the columns ofU are dependent:   a b c   U =0 d e. 0 0 f"
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. Ifa,d, f inProblem3areallnonzero,showthattheonlysolutiontoUx=0isx=0. ThenU has independent columns."
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. Decide the dependence or independence of (a) the vectors (1,3,2), (2,1,3), and (3.2,1). (b) the vectors (1,−3,2), (2,1,−3), and (−3,2,1)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.3 LinearIndependence,Basis,andDimension 111"
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. ChoosethreeindependentcolumnsofU. Thenmaketwootherchoices. Dothesame for A. You have found bases for which spaces?     2 3 4 1 2 3 4 1     0 6 7 0 0 6 7 0 U =  and A= . 0 0 0 9 0 0 0 9 0 0 0 0 4 6 8 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "7. If w , w , w are independent vectors, show that the differences v = w −w , v = 1 2 3 1 2 3 2 w −w , and v = w −w are dependent. Find a combination of the v’s that gives 1 3 3 1 2 zero."
    },
    {
        "chapter": "VectorSpaces",
        "question": "8. Ifw ,w ,w areindependentvectors,showthatthesumsv =w +w ,v =w +w , 1 2 3 1 2 3 2 1 3 and v =w +w are independent. (Write c v +c v +c v =0 in terms of the w’s. 3 1 2 1 1 2 2 3 3 Find and solve equations for the c’s.)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "9. Suppose v , v , v , v are vectors in R3. 1 2 3 4 (a) These four vectors are dependent because . (b) The two vectors v and v will be dependent if . 1 2 (c) The vectors v and (0,0,0) are dependent because . 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "10. Find two independent vectors on the plane x+2y−3z−t =0 in R4. Then find three independent vectors. Why not four? This plane is the nullspace of what matrix? Problems 11–18 are about the space spanned by a set of vectors. Take all linear combinations of the vectors"
    },
    {
        "chapter": "VectorSpaces",
        "question": "11. Describe the subspace of R3 (is it a line or a plane or R3?) spanned by (a) the two vectors (1,1,−1) and (−1,−1,1). (b) the three vectors (0,1,1) and (1,1,0) and (0,0,0). (c) the columns of a 3 by 5 echelon matrix with 2 pivots. (d) all vectors with positive components."
    },
    {
        "chapter": "VectorSpaces",
        "question": "12. Thevectorbisinthesubspace spannedbythecolumns of Awhenthereisa solution to . The vector c is in the row space of A when there is a solution to . True or false: If the zero vector is in the row space, the rows are dependent."
    },
    {
        "chapter": "VectorSpaces",
        "question": "13. Find the dimensions of (a) the column space of A, (b) the column space ofU, (c) the row space of A, (d) the row space ofU. Which two of the spaces are the same?     1 1 0 1 1 0     A=1 3 1  and U =0 2 1. 3 1 −1 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "14. Choose x = (x ,x ,x ,x ) in R4. It has 24 rearrangements like (x ,x ,x ,x ) and 1 2 3 4 2 1 3 4 (x ,x ,x ,x ). Those 24 vectors, including x itself, span a subspace S. Find specific 4 3 1 2 vectors x so that the dimension of S is: (a) 0, (b) 1, (c) 3, (d) 4."
    },
    {
        "chapter": "VectorSpaces",
        "question": "15. v+wandv−warecombinationsofvandw. Writevandwascombinationsofv+w and v−w. The two pairs of vectors the same space. When are they a basis for the same space?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "16. Decide whether or not the following vectors are linearly independent, by solving c v +c v +c v +c v =0: 1 1 2 2 3 3 4 4         1 1 0 0         1 0 0 1 v = , v = , v = , v = . 1 2 3 4 0 1 1 0 0 0 1 1 Decide also if they span R4, by trying to solve c v +···+c v =(0,0,0,1). 1 1 4 4"
    },
    {
        "chapter": "VectorSpaces",
        "question": "17. Suppose the vectors to be tested for independence are placed into the rows instead of the columns of A, How does the elimination process from A to U decide for or against independence?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "18. To decide whether b is in the sub space spanned by w ,...,w , let the vectors w be 1 n the columns of A and try to solve Ax=b. What is the result for (a) w =(1,1,0), w =(2,2,1), w =(0,0,2), b=(3,4,5)? 1 2 3 (b) w =(1,2,0), w =(2,5,0), w =(0,0,2), w =(0,0,0), and any b? 1 2 3 4 Problems 19–37 are about the requirements for a basis."
    },
    {
        "chapter": "VectorSpaces",
        "question": "19. Ifv ,...,v arelinearlyindependent,thespacetheyspanhasdimension . These 1 n vectors are a for that space. If the vectors are the columns of an m by n matrix, then m is than n."
    },
    {
        "chapter": "VectorSpaces",
        "question": "20. Find a basis for each of these subspaces of R4: (a) All vectors whose components are equal. (b) All vectors whose components add to zero. (c) All vectors that are perpendicular to (1,1,0,0) and (1,0,1,1). (cid:163) (cid:164) (d) The column space (in R2) and nullspace (in R5) ofU = 1 0 1 0 1 . 0 1 0 1 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "21. Find three different bases for the column space ofU above. Then find two different bases for the row space ofU."
    },
    {
        "chapter": "VectorSpaces",
        "question": "22. Suppose v ,v ,...,v are six vectors in R4. 1 2 6 (a) Those vectors (do)(do not)(might not) span R4."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.3 LinearIndependence,Basis,andDimension 113 (b) Those vectors (are)(are not)(might be) linearly independent. (c) Any four of those vectors (are)(are not)(might be) a basis for R4. (d) If those vectors are the columns of A, then Ax = b (has) (does not have) (might not have) a solution."
    },
    {
        "chapter": "VectorSpaces",
        "question": "23. The columns of A are n vectors from Rm. If they are linearly independent, what is the rank of A? If they span Rm, what is the rank? If they are a basis for Rm, what then?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "24. Findabasisfortheplanex−2y+3z=0inR3. Thenfindabasisfortheintersection of that plane with the xy-plane. Then find a basis for all vectors perpendicular to the plane."
    },
    {
        "chapter": "VectorSpaces",
        "question": "25. Suppose the columns of a 5 by 5 matrix A are a basis for R5. (a) The equation Ax=0 has only the solution x=0 because . (b) If b is in R5 then Ax=b is solvable because . Conclusion: A is invertible. Its rank is 5."
    },
    {
        "chapter": "VectorSpaces",
        "question": "26. Suppose S is a five-dimensional subspace of R6. True or false? (a) Every basis for S can be extended to a basis for R6 by adding one more vector. (b) Every basis for R6 can be reduced to a basis for S by removing one vector."
    },
    {
        "chapter": "VectorSpaces",
        "question": "27. U comes from A by subtracting row 1 from row 3:     1 3 2 1 3 2     A=0 1 1 and U =0 1 1. 1 3 2 0 0 0 Findbasesforthetwocolumnspaces. Findbasesforthetworowspaces. Findbases for the two nullspace."
    },
    {
        "chapter": "VectorSpaces",
        "question": "28. True or false (give a good reason)? (a) If the columns of a matrix are dependent, so are the rows. (b) The column space of a 2 by 2 matrix is the same as its row space. (c) The column space of a 2 by 2 matrix has the same dimension as its row space. (d) The columns of a matrix are a basis for the column space."
    },
    {
        "chapter": "VectorSpaces",
        "question": "29. For which numbers c and d do these matrices have rank 2?   (cid:34) (cid:35) 1 2 5 0 5   c d A=0 0 c 2 2 and B= . d c 0 0 0 d 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "30. By locating the pivots, find a basis for the column space of   0 5 4 3   0 0 2 1 U = . 0 0 0 0 0 0 0 0 Express each column that is not in the basis as a combination of the basic columns, Find also a matrix A with this echelon formU, but a different column space."
    },
    {
        "chapter": "VectorSpaces",
        "question": "31. Find a counterexample to the following statement: If v , v , v , v is a basis for the 1 2 3 4 vector space R4, and if W is a subspace, then some subset of the v’s is a basis for W."
    },
    {
        "chapter": "VectorSpaces",
        "question": "32. Find the dimensions of these vector spaces: (a) The space of all vectors in R4 whose components add to zero. (b) The nullspace of the 4 by 4 identity matrix. (c) The space of all 4 by 4 matrices."
    },
    {
        "chapter": "VectorSpaces",
        "question": "33. Suppose V is known to have dimension k. Prove that (a) any k independent vectors in V form a basis; (b) any k vectors that span V form a basis. In other words, if the number of vectors is known to be correct, either of the two properties of a basis implies the other."
    },
    {
        "chapter": "VectorSpaces",
        "question": "34. Prove that if V and W are three-dimensional subspaces of R5, then V and W must have a nonzero vector in common. Hint: Start with bases for the two subspaces, making six vectors in all."
    },
    {
        "chapter": "VectorSpaces",
        "question": "35. True or false? (a) IfthecolumnsofAarelinearlyindependent,thenAx=bhasexactlyonesolution for every b. (b) A 5 by 7 matrix never has linearly independent columns,"
    },
    {
        "chapter": "VectorSpaces",
        "question": "36. If A is a 64 by 17 matrix of rank 11, how many independent vectors satisfy Ax =0? How many independent vectors satisfy ATy=0?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "37. Find a basis for each of these subspaces of 3 by 3 matrices: (a) All diagonal matrices. (b) All symmetric matrices (AT =A). (c) All skew-symmetric matrices (AT =−A). Problems 38–42 are about spaces in which the “vectors” are functions."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.4 TheFourFundamentalSubspaces 115 dy"
    },
    {
        "chapter": "VectorSpaces",
        "question": "38. (a) Find all functions that satisfy =0. dx dy (b) Choose a particular function that satisfies =3. dx dy (c) Find all functions that satisfy =3. dx"
    },
    {
        "chapter": "VectorSpaces",
        "question": "39. The cosine space F contains all combinations y(x) = Acosx+Bcos2x+Ccos3x. 3 Find a basis for the subspace that has y(0)=0."
    },
    {
        "chapter": "VectorSpaces",
        "question": "40. Find a basis for the space of functions that satisfy dy (a) −2y=0. dx dy y (b) − =0. dx x"
    },
    {
        "chapter": "VectorSpaces",
        "question": "41. Suppose y (x), y (x), y (x) are three different functions of x. The vector space they 1 2 3 span could have dimension 1, 2, or 3. Give an example of y , y , y to show each 1 2 3 possibility."
    },
    {
        "chapter": "VectorSpaces",
        "question": "42. Find a basis for the space of polynomials p(x) of degree ≤ 3. Find a basis for the subspace with p(1)=0."
    },
    {
        "chapter": "VectorSpaces",
        "question": "43. Writethe3by3identitymatrixasacombinationoftheotherfivepermutationmatri- ces! Then show that those five matrices are linearly independent. (Assume a combi nation gives zero, and check entries to prove each term is zero.) The five permuta- tions are a basis for the subspace of 3 by 3 matrices with row and column sums all equal."
    },
    {
        "chapter": "VectorSpaces",
        "question": "44. Review: Which of the following are bases for R3? (a) (1,2,0) and (0,1,−1). (b) (1,1,−1), (2,3,4), (4,1,−1), (0,1,−1). (c) (1,2,2), (−1,2,1), (0,8,0). (d) (1,2,2), (−1,2,1), (0,8,6)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "45. Review: SupposeAis5by4withrank4. ShowthatAx=bhasnosolutionwhenthe 5by5matrix[A b]isinvertible. ShowthatAx=bissolvablewhen[A b]issingular."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.4 The Four Fundamental Subspaces The previous section dealt with definitions rather than constructions. We know what a basisis,butnothowtofindone. Now,startingfromanexplicitdescriptionofasubspace, we would like to compute an explicit basis. Subspaces can be described in two ways. First, we may be given a set of vectors that span the space. (Example: The columns span the column space.) Second, we may be told which conditions the vectors in the space must satisfy. (Example: The nullspace consists of all vectors that satisfy Ax=0.) The first description may include useless vectors (dependent columns). The second description may include repeated conditions (dependent rows). We can’t write a basis by inspection, and a systematic procedure is necessary. The reader can guess what that procedure will be. When elimination on A produces an echelon matrix U or a reduced R, we will find a basis for each of the subspaces associated with A. Then we have to look at the extreme case of full rank: When the rank is as large as possible, r =n or r =m or r =m=n, the matrix has a left-inverse B or a right-inverseC or a two-sided A−1. To organize the whole discussion, we take each of the four subspaces in turn. Two of them are familiar and two are new."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. The column space of A is denoted by C(A). Its dimension is the rank r."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. The nullspace of A is denoted by N(A). Its dimension is n−r."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. The row space of A is the column space of AT. It is C(AT), and it is spanned by the rows of A. Its dimension is also r."
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. The left nullspace of A is the nullspace of AT. It contains all vectors y such that ATy=0, and it is written N(AT). Its dimension is . The point about the last two subspaces is that they come from AT. If A is an m by n matrix, you can see which “host” spaces contain the four subspaces by looking at the number of components: The nullspace N(A) and row space C(AT) are subspaces of Rn. The left nullspace N(AT) and column space C(A) are subspaces of Rm. The rows have n components and the columns have m. For a simple matrix like (cid:34) (cid:35) 1 0 0 A=U =R= , 0 0 0 the column space is the line through [1]. The row space is the line through [1 0 0]T. It 0 is in R3. The nullspace is a plane in R3 and the left nullspace is a line in R2:     (cid:34) (cid:35) 0 0     0 N(A) contains 1 and 0, N(AT) contains . 1 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.4 TheFourFundamentalSubspaces 117 Note that all vectors are column vectors. Even the rows are transposed, and the row space of A is the column space of AT, Our problem will be to connect the four spaces for U (after elimination) to the four spaces for A:     1 3 3 2 1 3 3 2 Basic     U =0 0 3 3 came from A= 2 6 9 7. example 0 0 0 0 −1 −3 3 4 For novelty, we take the four subspaces in a more interesting order."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. The row space of A For an echelon matrix likeU, the row space is clear. It contains allcombinationsoftherows,aseveryrowspacedoes—butherethethirdrowcontributes nothing. The first two rows are a basis for the row space. A similar rule applies to every echelonmatrixU orR, withr pivotsandr nonzerorows: Thenonzerorowsareabasis, and the row space has dimension r. That makes it easy to deal with the original matrix A. 2M The row space of A has the same dimension r as the row space of U, and it has the same bases, because the row spaces of A andU (and R) are the same. The reason is that each elementary operation leaves the row space unchanged. The rows inU are combinations of the original rows in A. Therefore the row space ofU contains nothing new. At the same time, because every step can be reversed, nothing is lost; the rows of A can be recovered fromU. It is true that A andU have different rows, but the combinations of the rows are identical: same space! NotethatwedidnotstartwiththemrowsofA,whichspantherowspace,anddiscard m−r of them to end up with a basis. According to 2L, we could have done so. But it might be hard to decide which rows to keep and which to discard, so it was easier just to take the nonzero rows ofU."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. The nullspace of A Elimination simplifies a system of linear equations without changing the solutions. The system Ax = 0 is reduced to Ux = 0, and this process is reversible. The nullspace of A is the same as the nullspace of U and R. Only r of the equations Ax=0 are independent. Choosing the n−r “special solutions” to Ax=0 provides a definite basis for the nullspace: 2N The nullspace N(A) has dimension n−r. The “special solutions” are a basis—each free variable is given the value 1, while the other free variables are 0. Then Ax = 0 or Ux = 0 or Rx = 0 gives the pivot variables by back- substitution. This is exactly the way we have been solving Ux = 0. The basic example above has pivots in columns 1 and 3. Therefore its free variables are the second and fourth v and y. The basis for the nullspace is     −3 1     v = 1  1  v = 0  0  Special solutions x = ; x = . 1 2 y = 0  0  y = 1 −1 0 1 Any combination c x +c x has c as its v component, and c as its y component. The 1 1 2 2 1 2 onlywaytohavec x +c x =0istohavec =c =0,sothesevectorsareindependent. 1 1 2 2 1 2 They also span the nullspace; the complete solution is vx +yx . Thus the n−r =4−2 1 2 vectors are a basis. The nullspace is also called the kernel of A, and its dimension n−r is the nullity."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. The column space of A The column space is sometimes called the range. This is consistent with the usual idea of the range, as the set of all possible values f(x); x is in the domain and f(x) is in the range. In our case the function is f(x)=Ax. Its domain consists of all x in Rn; its range is all possible vectors Ax, which is the column space. (In an earlier edition of this book we called it R(A).) Our problem is to find bases for the column spaces of U and A. Those spaces are different (just look at the matrices!) but their dimensions are the same. The first and third columns of U are a basis for its column space. They are the columns with pivots. Every other column is a combination of those two. Furthermore, the same is true of the original A—even though its columns are different. The pivot columns of A are a basis for its column space. The second column is three times the first, just as in U. The fourth column equals (column 3) − (column 1). The same nullspace is telling us those dependencies. The reason is this: Ax=0 exactly whenUx=0. The two systems are equivalent and have the same solutions. The fourth column of U was also (column 3) − (column 1). Every linear dependence Ax = 0 among the columns of A is matched by a dependence Ux=0 among the columns ofU, with exactly the same coefficients. If a set of columns of A is independent, then so are the corresponding columns ofU, and vice versa. To find a basis for the column space C(A), we use what is already done for U. The r columns containing pivots are a basis for the column space of U. We will pick those same r columns in A: 2O The dimension of the column space C(A) equals the rank r, which also equals the dimension of the row space: The number of independent columns equals the number of independent rows. A basis for C(A) is formed by the r columns of A that correspond, inU, to the columns containing pivots. The row space and the column space have the same dimension r! This is one of the most important theorems in linear algebra. It is often abbreviated as “row rank = column rank.” It expresses a result that, for a random 10 by 12 matrix, is not at all"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.4 TheFourFundamentalSubspaces 119 obvious. It also says something about square matrices: If the rows of a square matrix are linearly independent, then so are the columns (and vice versa). Again, that does not seem self-evident (at least, not to the author). To see once more that both the row and column spaces ofU have dimension r, con- sider a typical situation with rank r = 3. The echelon matrix U certainly has three independent rows:   d ∗ ∗ ∗ ∗ ∗ 1    0 0 0 d ∗ ∗  2 U = .  0 0 0 0 0 d  3 0 0 0 0 0 0 We claim that U also has three independent columns, and no more, The columns have onlythreenonzerocomponents. Ifwecanshowthatthepivotcolumns—thefirst,fourth, and sixth—are linearly independent, they must be a basis (for the column space of U, not A!). Suppose a combination of these pivot columns produced zero:         d ∗ ∗ 0 1         0 d  ∗ 0 2 c  +c  +c  = . 1 2 3 0 0 d  0 3 0 0 0 0 Workingupwardintheusualway,c mustbezerobecausethepivotd (cid:54)=0,thenc must 3 3 2 bezerobecaused (cid:54)=0,andfinallyc =0. Thisestablishesindependenceandcompletes 2 1 the proof. Since Ax=0 if and only ifUx=0, the first, fourth, and sixth columns of A— whatever the original matrix A was, which we do not even know in this example—are a basis for C(A). The row space and column space both became clear after elimination on A. Now comes the fourth fundamental subspace, which has been keeping quietly out of sight. Since the first three spaces were C(A), N(A), and C(AT), the fourth space must be N(AT), It is the nullspace of the transpose, or the left nullspace of A. ATy = 0 means yTA=0, and the vector appears on the left-hand side of A."
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. The left nullspace of A (= the nullspace of AT) If A is an m by n matrix, then AT is n by m. Its nullspace is a subspace of Rm; the vector y has m components. Written as yTA=0, those components multiply the rows of A to produce the zero row: (cid:104) (cid:105)(cid:104) (cid:105) (cid:104) (cid:105) yTA= y ··· y A = 0 ··· 0 . 1 m The dimension of this nullspace N(AT) is easy to find, For any matrix, the number of pivot variables plus the number of free variables must match the total number of columns. For A, that was r+(n−r)=n. In other words, rank plus nullity equals n: dimension of C(A)+dimension of N(A)=number of columns. This law applies equally to AT, which has m columns. AT is just as good a matrix as A. But the dimension of its column space is also r, so (cid:161) (cid:162) r+dimension N(AT) =m. (1) 2P The left nullspace N(AT) has dimension m−r. The m−r solutions to yTA = 0 are hiding somewhere in elimination. The rows of A combine to produce the m−r zero rows ofU. Start from PA=LU, or L−1PA=U. The lastm−r rowsoftheinvertiblematrixL−1Pmustbeabasisofy’sintheleftnullspace— because they multiply A to give the zero rows inU. In our 3 by 4 example, the zero row was row 3 − 2(row 2) + 5(row 1). Therefore the components of y are 5, −2, 1. This is the same combination as in b −2b +5b on 3 2 1 the right-hand side, leading to 0 = 0 as the final equation. That vector y is a basis for the left nullspace, which has dimension m−r = 3−2 = 1. It is the last row of L−1P, and produces the zero rowinU—and we can often see it without computing L−1. When desperate, it is always possible just to solve ATy=0. I realize that so far in this book we have given no reason to care about N(AT). It is correctbutnotconvincingifIwriteinitalicsthattheleftnullspaceisalsoimportant. The next section does better by finding a physical meaning for y from Kirchhoff’s Current Law. Now we know the dimensions of the four spaces. We can summarize them in a table, and it even seems fair to advertise them as the Fundamental Theorem of Linear Algebra, Part I"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. C(A)= column space of A; dimension r."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. N(A)= nullspace of A; dimension n−r."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. C(AT)= row space of A; dimension r."
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. N(AT)= left nullspace of A; dimension m−r. (cid:34) (cid:35) 1 2 Example 1. A= has m=n=2, and rank r =1. 3 6 (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. The column space contains all multiples of 1 . The second column is in the same 3 direction and contributes nothing new. (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. The nullspace contains all multiples of −2 . This vector satisfies Ax=0. 1 (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. The row space contains all multiples of 1 . I write it as a column vector, since 2 strictly speaking it is in the column space of AT. (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. The left nullspace contains all multiples of y = −3 . The rows of A with coeffi- 1 cients −3 and 1 add to zero, so ATy=0."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.4 TheFourFundamentalSubspaces 121 In this example all four subspaces are lines. That is an accident, coming from r =1 and n−r=1andm−r=1. Figure2.5showsthattwopairsoflinesareperpendicular. That is no accident! Figure2.5: Thefourfundamentalsubspaces(lines)forthesingularmatrixA. If you change the last entry of A from 6 to 7, all the dimensions are different. The column space and row space have dimension r = 2. The nullspace and left nullspace contain only the vectors x=0 and y=0. The matrix is invertible. Existence of Inverses We know that if A has a left-inverse (BA=I) and a right-inverse (AC =I), then the two inverses are equal: B = B(AC)(BA)C =C. Now, from the rank of a matrix, it is easy to decidewhichmatricesactuallyhavetheseinverses. Roughlyspeaking, aninverseexists only when the rank is as large as possible. The rank always satisfies r ≤ m and also r ≤ n. An m by n matrix cannot have more than m independent rows or n independent columns. There is not space for more than m pivots, or more than n. We want to prove that when r = m there is a right-inverse, and Ax = b always has a solution. When r = n there is a left-inverse, and the solution (if it exists) is unique. Only a square matrix can have both r = m and r = n, and therefore only a square matrixcanachievebothexistenceanduniqueness. Onlyasquarematrixhasatwo-sided inverse. 2Q EXISTENCE: Full row rank r =m. Ax=b has at least one solution x for every b if and only if the columns span Rm. Then A has a right-inverseC such that AC =I (m by m). This is possible only if m≤n. m UNIQUENESS: Full column rank r =n. Ax =b has at most one solution x for every b if and only if the columns are linearly independent. Then A has an n by m left-inverse B such that BA=I . This is possible only if m≥n. n In the existence case, one possible solution is x =Cb, since then Ax =ACb=b. But there will be other solutions if there are other right-inverses. The number of solutions when the columns span Rm is 1 or ∞. In the uniqueness case, if there is a solution to Ax=b, it has to be x=BAx=Bb. But there may be no solution. The number of solutions is 0 or 1. There are simple formulas for the best left and right inverses, if they exist: One-sided inverses B=(ATA)−1AT and C =AT(AAT)−1. Certainly BA = I and AC = I. What is not so certain is that ATA and AAT are actually invertible. We show in Chapter 3 that ATA does have an inverse if the rank is n, and AAT has an inverse when the rank is m. Thus the formulas make sense exactly when the rank is as large as possible, and the one-sided inverses are found. Example 2. Consider a simple 2 by 3 matrix of rank 2: (cid:34) (cid:35) 4 0 0 A= . 0 5 0 Since r =m=2, the theorem guarantees a right-inverseC:   (cid:34) (cid:35) 1 0 (cid:34) (cid:35) 4 0 0  4  1 0 AC =  0 1 = . 0 5 0 5 0 1 c c 31 32 There are many right-inverses because the last row ofC is completely arbitrary. This is a case of existence but not uniqueness. The matrix A has no left-inverse because the last column of BA is certain to be zero. The specific right-inverse C = AT(AAT)−1 chooses c and c to be zero: 31 32     4 0 (cid:34) (cid:35) 1 0   1 0 4  Best right-inverse AT(AAT)−1 =0 5 16 =0 1 =C. 0 1 5 0 0 25 0 0 This is the pseudoinverse—a way of choosing the best C in Section 6.3. The transpose of A yields an example with infinitely many left-inverses:   (cid:34) (cid:35) (cid:34) (cid:35) 4 0 1 0 b   1 0 BAT = 4 13 0 5= . 0 1 b 0 1 5 23 0 0 NowitisthelastcolumnofBthatiscompletelyarbitrary. Thebestleft-inverse(alsothe pseudoinverse) has b = b = 0. This is a “uniqueness case,” when the rank is r = n. 13 23 There are no free variables, since n−r =0. If there is a solution it will be the only one. You can see when this example has one solution or no solution:     (cid:34) (cid:35) 4 0 b 1   x   1 0 5 =b  is solvable exactly when b =0. 2 3 x 2 0 0 b 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.4 TheFourFundamentalSubspaces 123 A rectangular matrix cannot have both existence and uniqueness. If m is different from n, we cannot have r =m and r =n. A square matrix is the opposite. If m = n, we cannot have one property without the other. A square matrix has a left-inverse if and only if it has a right-inverse. There is only one inverse, namely B =C = A−1. Existence implies uniqueness and uniqueness impliesexistence, when the matrix is square. The condition for invertibilityis fullrank: r =m=n. Each of these conditions is a necessary and sufficient test:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. The columns span Rn, so Ax=b has at least one solution for every b."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. The columns are independent, so Ax=0 has only the solution x=0. This list can be made much longer, especially if we look ahead to later chapters. Every condition is equivalent to every other, and ensures that A is invertible."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. The rows of A span Rn."
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. The rows are linearly independent."
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. Elimination can be completed: PA=LDU, with all n pivots."
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. The determinant of A is not zero."
    },
    {
        "chapter": "VectorSpaces",
        "question": "7. Zero is not an eigenvalue of A."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. True or false: If m = n, then the row space of A equals the column space. If m < n, then the nullspace has a larger dimension than ."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Find the dimension and construct a basis for the four subspaces associated with each of the matrices (cid:34) (cid:35) (cid:34) (cid:35) 0 1 4 0 0 1 4 0 A= and U = . 0 2 8 0 0 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Find the dimension and a basis for the four fundamental subspaces for     1 2 0 1 1 2 0 1     A=0 1 1 0 and U =0 1 1 0. 1 2 0 1 0 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.4 TheFourFundamentalSubspaces 125"
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. Describe the four subspaces in three-dimensional space associated with   0 1 0   A=0 0 1. 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. If the product AB is the zero matrix, AB = 0, show that the column space of B is contained in the nullspace of A. (Also the row space of A is in the left nullspace of B, since each row of A multiplies B to give a zero row.)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. Suppose A is an m by n matrix of rank r. Under what conditions on those numbers does (a) A have a two-sided inverse: AA−1 =A−1A=I? (b) Ax=b have infinitely many solutions for every b?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "7. Why is there no matrix whose row space and nullspace both contain (1,1,1)?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "8. Suppose the only solution to Ax = 0 (m equations in n unknowns) is x = 0. What is the rank and why? The columns of A are linearly ."
    },
    {
        "chapter": "VectorSpaces",
        "question": "9. Find a 1 by 3 matrix whose nullspace consists of all vectors in R3 such that x + 1 2x +4x =0. Find a 3 by 3 matrix with that same nullspace. 2 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "10. If Ax =b always has at least one solution, show that the only solution to ATy =0 is y=0. Hint: What is the rank?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "11. If Ax = 0 has a nonzero solution, show that ATy = f fails to be solvable for some right-hand sides f. Construct an example of A and f."
    },
    {
        "chapter": "VectorSpaces",
        "question": "12. Find the rank of A and write the matrix as A=uvT:   (cid:34) (cid:35) 1 0 0 3   2 −2 A=0 0 0 0 and A= . 6 −6 2 0 0 6"
    },
    {
        "chapter": "VectorSpaces",
        "question": "13. If a, b, c are given with a(cid:54)=0, choose d so that (cid:34) (cid:35) a b A= =uvT c d has rank 1. What are the pivots?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "14. Find a left-inverse and/or a right-inverse (when they exist) for   (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 1 0   a b A= and M =1 1 and T = . 0 1 1 0 a 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "15. If the columns of A are linearly independent (A is m by n), then the rank is , the nullspace is , the row space is , and there exists a -inverse."
    },
    {
        "chapter": "VectorSpaces",
        "question": "16. (A paradox) Suppose A has a right-inverse B. Then AB = I leads to ATAB = AT or B(ATA)−1AT. ButthatsatisfiesBA=I;itisaleft-inverse. Whichstepisnotjustified?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "17. FindamatrixAthathasVasitsrowspace,andamatrixBthathasVasitsnullspace, if V is the subspace spanned by       1 1 1       1, 2, 5. 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "18. Find a basis for each of the four subspaces of      0 1 2 3 4 1 0 0 0 1 2 3 4      A=0 1 2 4 6=1 1 00 0 0 1 2. 0 0 0 1 2 0 1 1 0 0 0 0 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "19. If A has the same four fundamental subspaces as B, does A=cB?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "20. (a) If a 7 by 9 matrix has rank 5, what are the dimensions of the four subspaces? What is the sum of all four dimensions? (b) If a 3 by 4 matrix has rank 3, what are its column space and left nullspace?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "21. Construct a matrix with the required property, or explain why you can’t. (cid:104) (cid:105) (cid:104) (cid:105) (cid:163) (cid:164) (cid:163) (cid:164) 1 0 (a) Column space contains 1 , 0 , row space contains 1 , 2 . 2 5 (cid:104)0 (cid:105) 1 (cid:104) (cid:105) 1 3 (b) Column space has basis 2 , nullspace has basis 2 . 3 1 (c) Dimension of nullspace =1+ dimension of left nullspace. (cid:163) (cid:164) (cid:163) (cid:164) (d) Left nullspace contains 1 , row space contains 3 . 3 1 (e) Row space = column space, nullspace (cid:54)= left nullspace."
    },
    {
        "chapter": "VectorSpaces",
        "question": "22. Without elimination, find dimensions and bases for the four subspaces for     0 3 3 3 1 1     A=0 0 0 0 and B=4 4. 0 1 0 1 5 5"
    },
    {
        "chapter": "VectorSpaces",
        "question": "23. Suppose the 3 by 3 matrix A is invertible. Write bases for the four subspaces for A, and also for the 3 by 6 matrix B=[A A]."
    },
    {
        "chapter": "VectorSpaces",
        "question": "24. What are the dimensions of the four subspaces for A, B, and C, if I is the 3 by 3 identity matrix and 0 is the 3 by 2 zero matrix? (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) I I A= I 0 and B= and C = 0 . 0T 0T"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.4 TheFourFundamentalSubspaces 127"
    },
    {
        "chapter": "VectorSpaces",
        "question": "25. Which subspaces are the same for these matrices of different sizes? (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) A A A A (a) A and . (b) and . A A A A Prove that all three matrices have the same rank r."
    },
    {
        "chapter": "VectorSpaces",
        "question": "26. If the entries of a 3 by 3 matrix are chosen randomly between 0 and 1, what are the most likely dimensions of the four subspaces? What if the matrix is 3 by 5?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "27. (Important) A is an m by n matrix of rank r. Suppose there are right-hand sides b for which Ax=b has no solution. (a) What inequalities (< or ≤) must be true between m, n, and r? (b) How do you know that ATy=0 has a nonzero solution?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "28. Construct a matrix with (1,0,1) and (1,2,0) as a basis for its row space and its column space. Why can’t this be a basis for the row space and nullspace?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "29. Without computing A, find bases for the four fundamental subspaces:    1 0 0 1 2 3 4    A=6 1 00 1 2 3. 9 8 1 0 0 1 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "30. If you exchange the first two rows of a matrix A, which of the four subspaces stay the same? If y = (1,2,3,4) is in the left nullspace of A, write down a vector in the left nullspace of the new matrix."
    },
    {
        "chapter": "VectorSpaces",
        "question": "31. Explain why v=(1,0,−1) cannot be a row of A and also be in the nullspace."
    },
    {
        "chapter": "VectorSpaces",
        "question": "32. Describe the four subspaces of R3 associated with     0 1 0 1 1 0     A=0 0 1 and I+A=0 1 1. 0 0 0 0 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "33. (Left nullspace) Add the extra column b and reduce A to echelon form:     1 2 3 b 1 2 3 b (cid:104) (cid:105) 1 1     A b =4 5 6 b →0 −3 −6 b −4b . 2 2 1 7 8 9 b 0 0 0 b −2b +b 3 3 2 1 A combination of the rows of A has produced the zero row. What combination is it? (Look at b −2b +b on the right-hand side.) Which vectors are in the nullspace of 3 2 1 AT and which are in the nullspace of A?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "34. Following the method of Problem 33, reduce A to echelon form and look at zero rows. The b column tells which combinations you have taken of the rows:     1 2 b 1 1 2 b 1     2 3 b  2 (a) 3 4 b . (b)  . 2 2 4 b  3 4 6 b 3 2 5 b 4 Fromthebcolumnafterelimination,readoffm−r basisvectorsintheleftnullspace of A (combinations of rows that give zero)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "35. Suppose A is the sum of two matrices of rank one: A=uvT+wzT. (a) Which vectors span the column space of A? (b) Which vectors span the row space of A? (c) The rank is less than 2 if or if . (d) Compute A and its rank if u=z=(1,0,0) and v=w=(0,0,1)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "36. Without multiplying matrices, find bases for the row and column spaces of A:   (cid:34) (cid:35) 1 2   3 0 3 A=4 5 . 1 1 2 2 7 How do you know from these shapes that A is not invertible?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "37. True or false (with a reason or a counterexample)? (a) A and AT have the same number of pivots. (b) A and AT have the same left nullspace. (c) If the row space equals the column space then AT =A. (d) If AT =−A then the row space of A equals the column space."
    },
    {
        "chapter": "VectorSpaces",
        "question": "38. If AB = 0, the columns of B are in the nullspace of A. If those vectors are in Rn, prove that rank(A)+rank(B)≤n."
    },
    {
        "chapter": "VectorSpaces",
        "question": "39. Cantic-tac-toebecompleted(5onesand4zerosinA)sothatrank(A)=2butneither side passed up a winning move?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "40. Construct any 2 by 3 matrix of rank 1. Copy Figure 2.5 and put one vector in each subspace (two in the nullspace). Which vectors are orthogonal?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "41. Redraw Figure 2.5 for a 3 by 2 matrix of rank r = 2. Which subspace is Z (zero vector only)? The nullspace part of any vector x in R2 is x = . n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.5 GraphsandNetworks 129"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.5 Graphs and Networks Iamnotentirelyhappywiththe3by4matrixintheprevioussection. Fromatheoretical point of view it was very satisfactory; the four subspaces were computable and their dimensions r, n−r, r, m−r were nonzero. But the example was not produced by a genuine application. It did not show how fundamental those subspaces really are. This section introduces a class of rectangular matrices with two advantages. They are simple, and they are important. They are incidence matrices of graphs, and every entry is 1, −1, or 0. What is remarkable is that the same is true of L and U and basis vectors for all four subspaces. Those subspaces play a central role in network theory. We emphasize that the word “graph” does not refer to the graph of a function (like a parabolafory=x2). Thereisasecondmeaning, completelydifferent,whichiscloserto computersciencethantocalculus—anditiseasytoexplain. Thissectionisoptional,but it gives a chance to see rectangular matrices in action—and how the square symmetric matrix ATA turns up in the end. A graph consists of a set of vertices or nodes, and a set of edges that connect them. The graph in Figure 2.6 has 4 nodes and 5 edges. It does not have an edge between nodes 1 and 4 (and edges from a node to itself are forbidden). This graph is directed, because of the arrow in each edge. Theedgenodeincidencematrixis5by4,witharowforeveryedge. Iftheedgegoes from node j to node k, then that row has −1 in column j and +1 in column k. The incidence matrix A is shown next to the graph (and you could recover the graph if you only had A). Row 1 shows the edge from node 1 to node 2. Row 5 comes from the fifth edge, from node 3 to node 4. Figure2.6: Adirectedgraph(5edges,4nodes,2loops)anditsincidencematrixA. Notice the columns of A. Column 3 gives information about node 3—it tells which edges enter and leave. Edges 2 and 3 go in, edge 5 goes out (with the minus sign). A is sometimes called the connectivity matrix, or the topology matrix. When the graph has m edges and n nodes, A is m by n (and normally m > n). Its transpose is the “node-edge” incidence matrix. Each of the four fundamental subspaces has a meaning in terms of the graph. We can do linear algebra, or write about voltages and currents. We do both! Nullspace of A: Is there a combination of the columns that gives Ax = 0? Normally the answer comes from elimination, but here it comes at a glance. The columns add up to the zero column. The nullspace contains x = (1,1,1,1), since Ax = 0. The equation Ax=bdoesnothaveauniquesolution(ifithasasolutionatall). Any“constantvector” x =(c,c,c,c) can be added to any particular solution of Ax =b. The complete solution has this arbitrary constant c (like the +C when we integrate in calculus). This has a meaning if we think of x , x , x , x as the potentials (the voltages) at the 1 2 3 4 nodes. ThefivecomponentsofAxgivethedifferencesinpotentialacrossthefiveedges. The difference across edge 1 is x −x , from the ±1 in the first row. 2 1 The equation Ax =b asks: Given the differences b ,...,b , find the actual potentials 1 5 x ,...,x . But that is impossible to do! We can raise or lower all the potentials by the 1 4 sameconstantc,andthedifferenceswillnotchange—confirmingthatx=(c,c,c,c)isin thenullspaceofA. Thosearetheonlyvectorsinthenullspace,sinceAx=0meansequal potentials across every edge. The nullspace of this incidence matrix is one-dimensional. The rank is 4−1=3. Column Space: For which differences b ,...,b can we solve Ax = b? To find a 1 5 direct test, look back at the matrix. Row 1 plus row 3 equals row 2. On the right-hand side we need b +b =b , or no solution is possible. Similarly, row 3 plus row 5 is row 1 3 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. The right-hand side must satisfy b +b = b , for elimination to arrive at 0 = 0. To 3 5 4 repeat, if b is in the column space, then b −b +b =0 and b −b +b =0. (1) 1 2 3 3 4 5 Continuing the search, we also find that rows 1+4 equal rows 2+5. But this is nothing new; subtracting the equations in (1) already produces b +b = b +b . There are 1 4 2 5 two conditions on the five components, because the column space has dimension 5−2. Those conditions would come from elimination, but here they have a meaning on the graph. Loops: Kirchhoff’s Voltage Law says that potential differences around a loop must addtozero,AroundtheupperloopinFigure2.6,thedifferencessatisfy(x −x )+(x − 2 1 3 x )=(x −x ). Those differences are b +b =b . To circle the lower loop and arrive 2 3 1 1 3 2 back at the same potential, we need b +b =b . 3 5 4 2R The test for b to be in the column space is Kirchhoff’s Voltage Law: The sum of potential differences around a loop must be zero. Left Nullspace: TosolveATy=0,wefinditsmeaningonthegraph. Thevectoryhas five components, one for each edge. These numbers represent currents flowing along the five edges. Since AT is 4 by 5, the equations ATy = 0 give four conditions on those"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.5 GraphsandNetworks 131 five currents. They are conditions of “conservation” at each node: Flow in equals flow out at every node: −y −y =0 Total current to node 1 is zero 1 2 y −y −y =0 to node 2 ATy=0 1 3 4 y +y −y =0 to node 3 2 3 5 y +y =0 to node 4 4 5 The beauty of network theory is that both A and AT have important roles. SolvingATy=0meansfindingasetofcurrentsthatdonot“pileup”atanynode. The traffic keeps circulating, and the simplest solutions are currents around small loops. Our graph has two loops, and we send 1 amp of current around each loop: (cid:104) (cid:105) (cid:104) (cid:105) Loop vectors yT = 1 −1 1 0 0 and yT = 0 0 1 −1 1 . 1 2 Each loop produces a vector y in the left nullspace. The component +1 or −1 indicates whether the current goes with or against the arrow. The combinations of y and y fill 1 2 the left nullspace, so y and y are a basis (the dimension had to be m−r =5−3 =2). 1 2 In fact y −y =(1,−1,0,1,−1) gives the big loop around the outside of the graph. 1 2 The column space and left nullspace are closely related. The left nullspace contains y = (1,1,1,0,0), and the vectors in the column space satisfy b −b +b = 0. Then 1 1 2 3 yTb=0: Vectors in the column space and left nullspace are perpendicular! That is soon to become Part Two of the “Fundamental Theorem of Linear Algebra.” Row Space: The row space of A contains vectors in R4, but not all vectors. Its dimension is the rank r = 3. Elimination will find three independent rows, and we can also look to the graph. The first three rows are dependent (row 1 + row 3 = row 2, and those edges form a loop). Rows 1,2,4 are independent because edges 1,2,4 contain no loops. Rows 1, 2, 4 are a basis for the row space. In each row the entries add to zero. Every combination (f , f , f , f ) in the row space will have that same property: 1 2 3 4 f in row space f + f + f + f =0 x in nullspace x=c(1,1,1,1) (2) 1 2 3 4 Again this illustrates the Fundamental Theorem: The row space is perpendicular to the nullspace. If f is in the row space and x is in the nullspace then fTx=0. For AT, the basic law of network theory is Kirchhoff’s Current Law. The total flow intoeverynodeiszero. Thenumbers f , f , f , f arecurrentsourcesintothenodes. The 1 2 3 4 source f mustbalance−y −y ,whichistheflowleavingnode1(alongedges1and2). 1 1 2 That is the first equation in ATy = f. Similarly at the other three nodes—conservation of charge requires flow in = flow out. The beautiful thing is that AT is exactly the right matrix for the Current Law. 2S The equations ATy= f at the nodes express Kirchhoff’s Current Law: The net current into every node is zero. Flow in = Flow out. This law can only be satisfied if the total current from outside is f + f + f + 1 2 3 f =0. With f =0, thelawATy=0issatisfiedby acurrentthatgoesaround 4 a loop. Spanning Trees and Independent Rows Every component of y and y in the left nullspace is 1 or −1 or 0 (from loop flows). 1 2 Thesameistrueofx=(1,1,1,1)inthenullspace,andalltheentriesinPA=LDU! The key point is that every elimination step has a meaning for the graph. You can see it in the first step for our matrix A: subtract row 1 from row 2. This replaces edge 2 by a new edge “1 minus 2”: That elimination step destroys an edge and edge 1 − b row 1 1 1 0 0 − row 2 1 0 1 0 edge 2 edge 1−2 row 1−2 0 1 −1 0 creates a new edge. Here the new edge “1−2” is just the old edge 3 in the opposite direction. The next elimination step will produce zeros in row 3 of the matrix. This shows that rows 1, 2, 3 are dependent. Rows are dependent if the corresponding edges contain a loop. At the end of elimination we have a full set of r independent rows. Those r edges form a tree—a graph with no loops. Our graph has r =3, and edges 1, 2, 4 form one possible tree. The full name is spanning tree because the tree “spans” all nodes of the graph. Aspanningtreehasn−1edgesifthegraphisconnected,andincludingonemore edge will produce a loop. Inthelanguageoflinearalgebra, n−1istherankoftheincidencematrix A. Therow spacehasdimensionn−1. Thespanningtreefromeliminationgivesabasisforthatrow space—each edge in the tree corresponds to a row in the basis. Thefundamentaltheoremoflinearalgebraconnectsthedimensionsofthesubspaces: Nullspace: dimension 1, contains x=(1,...,1). Column space: dimension r =n−1, any n−1 columns are independent. Row space: dimension r =n−1, independent rows from any spanning tree. Left nullspace: dimension m−r =m−n+1, contains y’s from the loops. Those four lines give Euler’s formula, which in some way is the first theorem in topol- ogy. Itcountszero-dimensionalnodesminusone-dimensionaledgesplustwo-dimensional"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.5 GraphsandNetworks 133 loops. Now it has a linear algebra proof for any connected graph: (# of nodes)−(# of edges)+(# of loops)=(n)−(m)+(m−n+1)=1. (3) For a single loop of 10 nodes and 10 edges, the Euler number is 10−10+1. If those 10 nodes are each connected to an eleventh node in the center, then 11−20+10 is still 1. Everyvector f intherowspacehasxTf = f +···+ f =0—thecurrentsfromoutside 1 n add to zero. Every vector b in the column space has yTb = 0—the potential differences add to zero around all loops. In a moment we link x to y by a third law (Ohm’s law for each resistor). First we stay with the matrix A for an application that seems frivolous but is not. The Ranking of Football Teams At the end of the season, the polls rank college football teams. The ranking is mostly an average of opinions, and it sometimes becomes vague after the top dozen colleges. We want to rank all teams on a more mathematical basis. The first step is to recognize the graph. If team j played team k, there is an edge between them. The teams are the nodes, and the games are the edges. There are a few hundred nodes and a few thousand edges—which will be given a direction by an arrow from the visiting team to the home team. Figure 2.7 shows part of the Ivy League, and some serious teams, and also a college that is not famous for big time football. Fortunately for that college (from which I am writing these words) the graph is not connected. Mathematically speaking, we cannot prove that MIT is not number 1 (unless it happens to play a game against somebody). Harvard Yale Michigan USC Texas b b b b b b MIT ) b b b b b Princeton Purdue Ohio State Notre Dame Georgia Tech Figure2.7: Partofthegraphforcollegefootball. If football were perfectly consistent, we could assign a “potential” x to every team. j Then if visiting team v played home team h, the one with higher potential would win. In the ideal case, the difference b in the score would exactly equal the difference x −x in h v their potentials. They wouldn’t even have to play the game! There would be complete agreement that the team with highest potential is the best. This method has two difficulties (at least). We are trying to find a number x for every team, and we want x −x = b , for every game. That means a few thousand equations h v i and only a few hundred unknowns. The equations x −x = b go into a linear system h v i Ax=b, in which A is an incidence matrix. Every game has a row, with +1 in column h and −1 in column v—to indicate which teams are in that game. First difficulty: If b is not in the column space there is no solution. The scores must fit perfectly or exact potentials cannot be found. Second difficulty: If A has nonzero vectors in its nullspace, the potentials x are not well determined. In the first case x does not exist; in the second case x is not unique. Probably both difficulties are present. The nullspace always contains the vector of 1s, since A looks only at the differences x −x . To determine the potentials we can arbitrarily assign zero potential to Harvard. h v (I am speaking mathematically, not meanly.) But if the graph is not connected, every separatepieceofthegraphcontributesavectortothenullspace. Thereiseventhevector with x = 1 and all other x = 0. We have to ground not only Harvard but one team MIT j in each piece. (There is nothing unfair in assigning zero potential; if all other potentials are below zero then the grounded team ranks first.) The dimension of the nullspace is the number of pieces of the graph—and there will be no way to rank one piece against another, since they play no games. The column space looks harder to describe. Which scores fit perfectly with a set of potentials? Certainly Ax = b is unsolvable if Harvard beats Yale, Yale beats Princeton, and Princeton beats Harvard. More than that, the score differences in that loop of games have to add to zero: Kirchhoff’s law for score differences b +b +b =0. HY YP PH Thisisalsoalawoflinearalgebra. Ax=bcanbesolvedwhenbsatisfiesthesamelinear dependencies as the rows of A. Then elimination leads to 0=0. In reality, b is almost certainly not in the column space. Football scores are not that consistent. To obtain a ranking we can use least squares: Make Ax as close as possible tob. ThatisinChapter3,andwementiononlyoneadjustment. Thewinnergetsabonus of 50 or even 100 points on top of the score difference. Otherwise winning by 1 is too close to losing by 1. This brings the computed rankings very close to the polls, and Dr. Leake (Notre Dame) gave a full analysis in Management Science in Sports (1976). After writing that subsection, I found the following in the New York Times: In its final rankings for 1985, the computer placed Miami (10-2) in the sev- enth spot above Tennessee (9-1-2). A few days after publication, packages containing oranges and angry letters from disgruntled Tennessee fans began arriving at the Times sports department. The irritation stems from the fact that Tennessee thumped Miami 35-7 in the Sugar Bowl. Final AP and UPI polls ranked Tennessee fourth, with Miami significantly lower. Yesterday morning nine cartons of oranges arrived at the loading dock. They were sent to Bellevue Hospital with a warning that the quality and contents of the oranges were uncertain."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.5 GraphsandNetworks 135 So much for that application of linear algebra. Networks and Discrete Applied Mathematics A graph becomes a network when numbers c ,...,c are assigned to the edges. The 1 m number c can be the length of edge i, or its capacity, or its stiffness (if it contains a i spring), or its conductance (if it contains a resistor). Those numbers go into a diagonal matrixC, which is m by m. C reflects “material properties,” in contrast to the incidence matrix A—which gives information about the connections. Our description will be in electrical terms. On edge i, the conductance is c and the i resistanceis1/c . Ohm’sLawsaysthatthecurrenty throughtheresistorisproportional i i to the voltage drop e : i Ohm’s Law y =c e (current)=(conductance)(voltage drop). i i i This is also written E = IR, voltage drop equals current times resistance. As a vector equation on all edges at once, Ohm’s Law is y=Ce. We need Kirchhoff’s Voltage Law and Current Law to complete the framework: KVL: The voltage drops around each loop add to zero. KCL: The currents y (and f ) into each node add to zero. i i The voltage law allows us to assign potentials x ,...,x to the nodes. Then the dif- 1 n ferences around a loop give a sum like (x −x1)+(x −x )+(x −x ) = 0, in which 2 3 2 1 3 everything cancels. The current law asks us to add the currents into each node by the multiplication ATy. If there are no external sources of current, Kirchhoff’s Current Law is ATy=0. The other equation is Ohm’s Law, but we need to find the voltage drop e across the resistor. The multiplication Ax gave the potential difference between the nodes. Reversing the signs, −Ax gives the drop in potential. Part of that drop may be due to a battery in the edge of strength b . The rest of the drop is e=b−Ax across the resistor: i Ohm’s Law y=C(b−Ax) or C−1y+Ax=b. (4) The fundamental equations of equilibrium combine Ohm and Kirchhoff into a cen- tral problem of applied mathematics. These equations appear everywhere: C−1y + Ax = b Equilibrium equations (5) ATy = f. Thatisalinearsymmetricsystem, fromwhichehasdisappeared. Theunknownsarethe currents y and the potentials x. You see the symmetric block matrix: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) C−1 A y b Block form = . (6) AT 0 x f For block elimination the pivot isC−1, the multiplier is ATC, and subtraction knocks out AT below the pivot. The result is (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) C−1 A y b = 0 −ATCA x f −ATCb The equation for x alone is in the bottom row, with the symmetric matrix ATCA: Fundamental equation ATCAx=ATCbłf. (7) Then back-substitution in the first equation produces y. Nothing mysterious—substitute y=C(b−Ax) into ATy= f to reach (7). Important Remark One potential must be fixed in advance: x = 0. The nth node n is grounded, and the nth column of the original incidence matrix is removed. The re- sultingmatrixiswhatwenowmeanbyA: itsn−1columnsareindependent. Thesquare matrix ATCA, which is the key to solving equation (7) for x, is an invertible matrix of order n−1:      (cid:104) (cid:105)      AT  C  A = ATCA  (n−1)×m m×m m×(n−1) (n−1)×(n−1) Example 1. Suppose a battery b and a current source f (and five resistors) connect 3 2 four nodes. Node 4 is grounded and the potential x = 0 is fixed. The first thing is the 4 current law ATy= f at nodes 1, 2, 3:   −y − y − y = 0 −1 0 −1 0 −1 1 3 5   y − y = f and AT = 1 −1 0 0 0 . 1 2 2 y + y − y = 0 0 1 1 −1 0 2 3 4 Noequationiswrittenfornode4,wherethecurrentlawisy +y + f =0. Thisfollows 4 5 2 from adding the other three equations. The other equation is C−1y+Ax = b. The potentials x are connected to the currents y by Ohm’s Law. The diagonal matrixC contains the five conductances c = 1/R . The i i"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.5 GraphsandNetworks 137 right-hand side accounts for the battery of strength b in edge 3. The block form has 3 C−1y+Ax=b above ATy= f:      R −1 1 0 y 0 1 1       R 2 0 −1 1 y 2 0       R −1 0 1 y  b  (cid:34) (cid:35)(cid:34) (cid:35) 3 3 3      C−1 A y  R 0 0 −1y  0  4  4   = = AT 0 x   R 5 −1 0 0   y 5   0       −1 0 −1 0 −1 x  0 1       1 −1 0 0 0 x  f  2 2 0 1 1 −1 0 x 0 3 The system is 8 by 8, with five currents and three potentials. Elimination of y’s reduces to the 3 by 3 system ATCAx = ATCb− f. The matrix ATCA contains the reciprocals c = 1/R (because in elimination you divide by the pivots). We also show the fourth i i row and column, from the grounded node, outside the 3 by 3 matrix:   c +c +c −c −c −c (node 1) 1 3 5 1 3 5   ATCA= −c c +c −c  0 (node 2) 1 1 2 2 −c −c c +c +c −c (node 3) 3 2 2 3 4 4 −c 0 −c c +c (node 4) 5 4 4 5 The first entry is 1+1+1, or c +c +c when C is included, because edges 1, 3, 5 1 3 5 touch node 1. The next diagonal entry is 1+1 or c +c , from the edges touching node 1 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. For the 3-node triangular graph in the figure following, write the 3 by 3 incidence matrix A. Find a solution to Ax=0 and describe all other vectors in the nullspace of A. Find a solution to ATy = 0 and describe all other vectors in the left nullspace of A. node 1 x1 b b y5 edge 1 edge 3 y1 y2 y4 xb 4 y6 b b b b node 2 edge 2 node 3 x2 y3 x3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. For the same 3 by 3 matrix, show directly from the columns that every vector b in the column space will satisfy b +b −b =0. Derive the same thing from the three 1 2 3 rows—the equations in the system Ax = b. What does that mean about potential differences around a loop?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Show directly from the rows that every vector f in the row space will satisfy f + 1 f + f =0. DerivethesamethingfromthethreeequationsATy= f. Whatdoesthat 2 3 mean when the f’s are currents into the nodes?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. Compute the 3 by 3 matrix ATA, and show that it is symmetric but singular—what vectors are in its nullspace? Removing the last column of A (and last row of AT) leaves the 2 by 2 matrix in the upper left corner; show that it is not singular."
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. Put the diagonal matrix C with entries c , c , c in the middle and compute ATCA. 1 2 3 Show again that the 2 by 2 matrix in the upper left corner is invertible."
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. Write the 6 by 4 incidence matrix A for the second graph in the figure. The vector (1,1,1,1) is in the nullspace of A, but now there will be m−n+1 = 3 independent vectors that satisfy ATy = 0. Find three vectors y and connect them to the loops in the graph."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.5 GraphsandNetworks 139"
    },
    {
        "chapter": "VectorSpaces",
        "question": "7. If that second graph represents six games between four teams, and the score dif- ferences are b ,...,b , when is it possible to assign potentials x ,...,x so that the 1 6 1 4 potential differences agree with the b’s? You are finding (from Kirchhoff or from elimination) the conditions that make Ax=b solvable."
    },
    {
        "chapter": "VectorSpaces",
        "question": "8. Write down the dimensions of the four fundamental subspaces for this 6 by 4 inci- dence matrix, and a basis for each subspace."
    },
    {
        "chapter": "VectorSpaces",
        "question": "9. Compute ATA and ATCA, where the 6 by 6 diagonal matrixC has entries c ,...,c . 1 6 How can you tell from the graph where the c’s will appear on the main diagonal of ATCA?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "10. Draw a graph with numbered and directed edges (and numbered nodes) whose inci- dence matrix is   −1 1 0 0   −1 0 1 0  A= .  0 1 0 −1 0 0 −1 1 Is this graph a tree? (Are the rows of A independent?) Show that removing the last edge produces a spanning tree. Then the remaining rows are a basis for ?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "11. With the last column removed from the preceding A, and with the numbers 1. 2, 2, 1 on the diagonal ofC, write out the 7 by 7 system C−1y + Ax = 0 ATy = f. Eliminating y , y , y , y leaves three equations ATCAx = −f for x , x , x . Solve 1 2 3 4 1 2 3 the equations when f = (1,1,6). With those currents entering nodes 1, 2, 3 of the network, what are the potentials at the nodes and currents on the edges?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "12. If A is a 12 by 7 incidence matrix from a connected graph, what is its rank? How many free variables are there in the solution to Ax = b? How many free variables are there in the solution to ATy = f? How many edges must be removed to leave a spanning tree?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "13. In the graph above with 4 nodes and 6 edges, find all 16 spanning trees."
    },
    {
        "chapter": "VectorSpaces",
        "question": "14. If MIT beats Harvard 35-0, Yale ties Harvard, and Princeton beats Yale 7-6, what score differences in the other 3 games (H-P MIT-P, MIT-Y) will allow potential dif- ferences that agree with the score differences? If the score differences are known for the games in a spanning tree, they are known for all games."
    },
    {
        "chapter": "VectorSpaces",
        "question": "15. In our method for football rankings, should the strength of the opposition be consid- ered — or is that already built in?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "16. If there is an edge between every pair of nodes (a complete graph), how many edges are there? The graph has n nodes, and edges from a node to itself are not allowed."
    },
    {
        "chapter": "VectorSpaces",
        "question": "17. For both graphs drawn below, verify Euler’s formula: (# of nodes) − (# of edges) + (# of loops) = 1."
    },
    {
        "chapter": "VectorSpaces",
        "question": "18. Multiply matrices to find ATA, and guess how its entries come from the graph: (a) The diagonal of ATA tells how many into each node. (b) The off-diagonals −1 or 0 tell which pairs of nodes are ."
    },
    {
        "chapter": "VectorSpaces",
        "question": "19. Why does the nullspace of ATA contain (1,1,1,1)? What is its rank?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "20. Why does a complete graph with n = 6 nodes have m = 15 edges? A spanning tree connecting all six nodes has edges. There are nn−2 =64 spanning trees!"
    },
    {
        "chapter": "VectorSpaces",
        "question": "21. The adjacency matrix of a graph has M = 1 if nodes i and j are connected by an ij edge (otherwise M = 0). For the graph in Problem 6 with 6 nodes and 4 edges, ij write down M and also M2. Why does (M2) count the number of 2-step paths from ij node i to node j?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 Linear Transformations We know how a matrix moves subspaces around when we multiply by A. The nullspace goes into the zero vector. All vectors go into the column space, since Ax is always a combination of the columns. You will soon see something beautiful—that A takes its rowspace into its column space, and on those spacesof dimension r itis 100 percent in- vertible. ThatistherealactionofA. Itispartlyhiddenbynullspacesandleftnullspaces, which lie at right angles and go their own way (toward zero). Whatmattersnowiswhathappensinsidethespace—whichmeansinsiden-dimensional space, if A is n by n. That demands a closer look. Suppose x is an n-dimensional vector. When A multiplies x, it transforms that vector into a new vector Ax. This happens at every point x of the n-dimensional space Rn. The whole space is transformed, or “mapped into itself,” by the matrix A. Figure 2.9 illustrates four transformations that come from matrices:"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 LinearTransformations 141 (cid:34) (cid:35) 1. Amultipleoftheidentitymatrix, A=cI, stretcheseveryvector by the same factor c. The whole space expands or contracts (or c 0 A= somehow goes through the origin and out the opposite side, when 0 c c is negative). (cid:34) (cid:35) 2. Arotationmatrixturnsthewholespacearoundtheorigin. This 0 −1 example turns all vectors through 90°, transforming every point A= (x,y) to (−y,x). 1 0 (cid:34) (cid:35) 3. A reflection matrix transforms every vector into its image on the opposite side of a mirror. In this example the mirror is the 0 1 A= 45° line y = x, and a point like (2,2) is unchanged. A point like 1 0 (2,−2) is reversed to (−2,2). On a combination like v=(2,2)+ (2,−2)=(4,0), the matrix leaves one part and reverses the other part. The output is Av=(2,2)+(−2,2)=(0,4) That reflection matrix is also a permutation matrix! It is alge- braically so simple, sending (x,y) to (y,x), that the geometric pic- ture was concealed. (cid:34) (cid:35) 4. A projection matrix takes the whole space onto a lower- dimensional subspace (not invertible). The example transforms 1 0 A= eachvector(x,y)intheplanetothenearestpoint(x,0)onthehor- 0 0 izontal axis. That axis is the column space of A. The y-axis that projects to (0,0) is the nullspace. (cx,cy) (−y,x) (y,x) (x,y) (x,y) (x,0) (x,y) (x,y) stretching 90° rotation reflection (45° mirror) projection on axis Figure2.9: Transformationsoftheplanebyfourmatrices. Those examples could be lifted into three dimensions. There are matrices to stretch the earth or spin it or reflect it across the plane of the equator (forth pole transforming to south pole). There is a matrix that projects everything onto that plane (both poles to the center). It is also important to recognize that matrices cannot do everything, and some transformations T(x) are not possible with Ax: (i) It is impossible to move the origin, since A0=0 for every matrix. (ii) If the vector x goes to x(cid:48), then 2x must go to 2x(cid:48). in general cx must go to cx(cid:48), since A(cx)=c(Ax). (iii) If the vectors x and y go to x(cid:48) and y(cid:48), then their sum x+y must go to x(cid:48)+y(cid:48)—since A(x+y)=Ax+Ay. Matrix multiplication imposes those rules on the transformation. The second rule con- tains the first (take c = 0 to get A0 = 0). We saw rule (iii) in action when (4,0) was reflected across the 45° line. It was split into (2,2)+(2,−2) and the two parts were reflected separately. The same could be done for projections: split, project separately, and add the projections. These rules apply to any transformation that comes from a matrix. Their importance has earned them a name: Transformations that obey rules (i)–(iii) are called linear transformations. The rules can be combined into one requirement: 2T For all numbers c and d and all vectors x and y, matrix multiplication satisfies the rule of linearity: A(cx+dy)=c(Ax)+d(Ay). (1) Every transformation T(x) that meets this requirement is a linear transforma- tion. Any matrix leads immediately to a linear transformation. The more interesting question is in the opposite direction: Does every linear transformation lead to a matrix? The object of this section is to find the answer, yes. This is the foundation of an approach to linear algebra—starting with property (1) and developing its consequences—that is much more abstract than the main approach in this book. We preferred to begin directly with matrices, and now we see how they represent linear transformations. AtransformationneednotgofromRn tothesamespaceRn. Itisabsolutelypermitted totransformvectorsinRn tovectorsinadifferentspaceRm. Thatisexactlywhatisdone byanmbynmatrix! Theoriginalvectorxhasncomponents,andthetransformedvector Ax has m components. The rule of linearity is equally satisfied by rectangular matrices, so they also produce linear transformations. Having gone that far, there is no reason to stop. The operations in the linearity con- dition (1) are addition and scalar multiplication, but x and y need not be column vectors in Rn. Those are not the only spaces. By definition, any vector space allows the com- binations cx+dy—the “vectors” are x and y, but they may actually be polynomials or matrices or functions x(t) and y(t). As long as the transformation satisfies equation (1), it is linear. We take as examples the spaces P , in which the vectors are polynomials p(t) of n degreen. Theylooklike p=a +a t+···+a tn, andthedimensionofthevectorspace 0 1 n is n+1 (because with the constant term, there are n+1 coefficients). Example 1. The operation of differentiation, A=d/dt, is linear: d Ap(t)= (a +a t+···+a tn)=a +···+na tn−1. (2) 0 1 n 1 n dt The nullspace of this A is the one-dimensional space of constants: da /dt = 0. The 0 column space is the n-dimensional space P ; the right-hand side of equation (2) is n−1 always in that space. The sum of nullity (= 1) and rank (= n) is the dimension of the original space P . n"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 LinearTransformations 143 Example 2. Integration from 0 tot is also linear (it takes P to P ): n n+1 (cid:90) t a Ap(t)= (a +···+a tn)dt =a t+···+ n tn+1. (3) 0 n 0 n+1 0 This time there is no nullspace (except for the zero vector, as always!) but integration does not produce all polynomials in P . The right side of equation (3) has no constant n+1 term. Probably the constant polynomials will be the left nullspace. Example 3. Multiplication by a fixed polynomial like 2+3t is linear: Ap(t)=(2+3t)(a +···+a tn)=2a +···+3a tn+1. 0 n 0 n Again this transforms P to P , with no nullspace except p=0. n n+1 In these examples (and in almost all examples), linearity is not difficult to verify. It hardly even seems interesting. If it is there, it is practically impossible to miss. Nev- ertheless, it is the most important property a transformation can have1. Of course most transformations are not linear—for example, to square the polynomial (Ap = p2), or to add 1 (Ap = p+1), or to keep the positive coefficients (A(t−t2) =t). It will be linear transformations, and only those, that lead us back to matrices. Transformations Represented by Matrices Linearity has a crucial consequence: If we know Ax for each vector in a basis, then we know Ax for each vector in the entire space. Suppose the basis consists of the n vectors x ,...,x . Every other vector x is a combination of those particular vectors (they span 1 n the space). Then linearity determines Ax: Linearity If x=c x +···+c x then Ax=c (Ax )+···+c (Ax ). (4) 1 1 n n 1 1 n n The transformation T(x) = Ax has no freedom left, after it has decided what to do with thebasisvectors. Therestisdeterminedbylinearity. Therequirement(1)fortwovectors x and y leads to condition (4) for n vectors x ,...,x . The transformation does have a 1 n free hand with the vectors in the basis (they are independent). When those are settled, the transformation of every vector is settled. Example 4. What linear transformation takes x and x to Ax and Ax ? 1 2 1 2     (cid:34) (cid:35) (cid:34) (cid:35) 2 4 1   0   x = goes to Ax =3; x = goes to Ax =6. 1 1 2 2 0 1 4 8 It must be multiplication T(x)=Ax by the matrix   2 4   A=3 6. 4 8 1Invertibilityisperhapsinsecondplaceasanimportantproperty. Starting with a different basis (1,1) and (2,−1), this same A is also the only linear transformation with     (cid:34) (cid:35) (cid:34) (cid:35) 6 0 1   2   A = 9  and A =0. 1 −1 12 0 Next we find matrices that represent differentiation and integration. First we must decide on a basis. For the polynomials of degree 3 there is a natural choice for the four basis vectors: Basis for P p =1, p =t, p =t2, p =t3. 3 1 2 3 4 That basis is not unique (it never is), but some choice is necessary and this is the most convenient. The derivatives of those four basis vectors are 0, 1, 2t, 3t2: Action of d/dt Ap =0, Ap = p , Ap =2p , Ap =3p . (5) 1 2 1 3 2 4 3 “d/dt” is acting exactly like a matrix, but which matrix? Suppose we were in the usual four-dimensional space with the usual basis—the coordinate vectors p = (1,0,0,0), 1 p =(0,1,0,0), p =(0,0,1,0), p =(0,0,0,1). Thematrixisdecidedbyequation(5): 2 3 4   0 1 0 0   0 0 2 0 Differentiation matrix A = . diff 0 0 0 3 0 0 0 0 Ap is its first column, which is zero. Ap is the second column, which is p . Ap is 1 2 1 3 2p and Ap is 3p . The nullspace contains p (the derivative of a constant is zero). 2 4 3 1 The column space contains p , p , p (the derivative of a cubic is a quadratic). The 1 2 3 derivative of a combination like p = 2+t−t2−t3 is decided by linearity, and there is nothingnewaboutthat—itisthewaywealldifferentiate. Itwouldbecrazytomemorize the derivative of every polynomial. The matrix can differentiate that p(t), because matrices build in linearity!      0 1 0 0 2 1      dp 0 0 2 0 1  −2 =Ap−→  = −→1−2t−3t2. dt 0 0 0 3−1 −3 0 0 0 0 −1 0 In short, the matrix carries all the essential information. If the basis is known, and the matrix is known, then the transformation of every vector is known. The coding of the information is simple. To transform a space to itself, one basis is enough. A transformation from one space to another requires a basis for each."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 LinearTransformations 145 2U Suppose the vectors x ,...,x are a basis for the space V, and vectors 1 n y ,...,y are a basis for W. Each linear transformation T from V to W is 1 m represented by a matrix A. The jth column is found by applying T to the jth basis vector x , and writing T(x ) as a combination of the y’s: j j Column j of A T(x )=Ax =a y +a y +···+a y . (6) j j 1j 1 2j 2 mj m For the differentiation matrix, column 1 came from the first basis vector p = 1. Its 1 derivativeiszero,socolumn1iszero. Thelastcolumncamefrom(d/dt)t3=3t2. Since 3t2=0p +0p +3p +0p ,thelastcolumncontained0,0,3. 0. Therule(6)constructs 1 2 3 4 the matrix, a column at a time. We do the same for integration. That goes from cubics to quartics, transforming V = P into W = P , so we need a basis for W. The natural choice is y = 1, y =t, 3 4 1 2 y =t2, y =t3, y =t4, spanning the polynomials of degree 4. The matrix A will be m 3 4 5 by n, or 5 by 4. It comes from applying integration to each basis vector of V: (cid:90) (cid:90) t t 1 1 1dt =t or Ax =y , ..., t3dt = t4 or Ax = y . 1 2 4 5 4 4 0 0   0 0 0 0   1 0 0 0   Integration matrix A =0 1 0 0. int  2  0 0 1 0 3 0 0 0 1 4 Differentiation and integration are inverse operations. Or at least integration followed by differentiation brings back the original function. To make that happen for matrices, we need the differentiation matrix from quartics down to cubics, which is 4 by 5:     0 1 0 0 0 1     0 0 2 0 0  1  A =  and A A = . diff diff int 0 0 0 3 0  1  0 0 0 0 4 1 Differentiation is a left-inverse of integration. Rectangular matrices cannot have two- sidedinverses! Intheoppositeorder,A A =I cannotbetrue. The5by5producthas int diff zeros in column 1. The derivative of a constant is zero. In the other columns A A is int diff the identity and the integral of the derivative oftn istn. Rotations Q, Projections P, and Reflections H Thissectionbeganwith90°rotations,projectionsontothex-axis,andreflectionsthrough the 45° line. Their matrices were especially simple: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 0 −1 1 0 0 1 Q= P= H = . 1 0 0 0 1 0 (rotation) (projection) (reflection) The underlying linear transformations of the x-y plane are also simple. But rotations through other angles, projections onto other lines, and reflections in other mirrors are almostaseasytovisualize,Theyarestilllineartransformations,providedthattheorigin (cid:163) (cid:164) is fixed: A0=0. They must be represented by matrices. Using the natural basis 1 and (cid:163) (cid:164) 0 0 , we want to discover those matrices. 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. Rotation Figure 2.10 shows rotation through an angleθ. It also shows the effect on the two basis vectors. The first one goes to (cosθ,sinθ), whose length is still 1; it lies on the “θ-line.” The second basis vector (0,1) rotates into (−sinθ,cosθ). By rule (6), those numbers go into the columns of the matrix (we use c and s for cosθ and sinθ). This family of rotations Q is a perfect chance to test the correspondence θ between transformations and matrices: Does the inverse of Q equal Q (rotation backward throughθ)? Yes. θ −θ (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) c −s c s 1 0 Q Q = = . θ −θ s c −s c 0 1 Does the square of Q equal Q (rotation through a double angle)? Yes. θ 2θ (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) c −s c −s c2−s2 −2cs cos2θ −sin2θ Q2 = = = . θ s c s c 2cs c2−s2 sin2θ cos2θ Does the product of Q and Q equal Q (rotation through θ then ϕ)? θ ϕ θ+ϕ Yes. (cid:34) (cid:35) (cid:34) (cid:35) cosθcosϕ−sinθsinϕ ··· cos(θ+ϕ) ··· Q Q = = . θ ϕ sinθcosϕ+cosθsinϕ ··· sin(θ+ϕ) ··· The last case contains the first two. The inverse appears when ϕ is −θ, and the square appears when ϕ is +θ. All three questions were decided by trigonometric identities (and theygive a newway to remember those identities). It wasno accident that all the answers were yes. Matrix multiplication is defined exactly so that the product of the matrices corresponds to the product of the transformations. 2V Suppose A and B are linear transformations from V to W and from U to V. Their product AB starts with a vector u in U, goes to Bu in V, and"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 LinearTransformations 147 c −s   R = [− cs ] 0 1 s c  0 1 b c (cid:2) (cid:3) (cid:2) (cid:3) c c [s] c[s] 2 1 c cs θ   b P = b s cs s2  s[sc ] θ θ b 1 1 1 0 0 (cid:2) (cid:3) (cid:2) (cid:3) Figure2.10: Rotationthroughθ(left). Projectionontotheθ-line(right). finishes with ABu in W. This “composition” AB is again a linear transfor- mation (from U to W). Its matrix is the product of the individual matrices representing A and B. For A A , the composite transformation was the identity (and A A annihilated diff int int diff all constants). For rotations, the order of multiplication does not matter. Then U = V = W is the x-y plane, and Q Q is the same as Q Q . For a rotation and a θ ϕ ϕ θ reflection, the order makes a difference. Technical note: To construct the matrices, we need bases for V and W, and then for U and V. By keeping the same basis for V, the product matrix goes correctly from the basis in U to the basis in W. If we distinguish the transformation A from its matrix (call that [A]), then the product rule 2V becomes extremely concise: [AB] = [A][B]. The rule for multiplying matrices in Chapter 1 was totally determined by this requirement—it must match the product of linear transformations."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Projection Figure 2.10 also shows the projection of (1,0) onto the θ-line. The lengthoftheprojectionisc=cosθ. Noticethatthepointofprojectionisnot(c,s),as Imistakenlythought;thatvectorhaslength1(itistherotation),sowemustmultiply byc. Similarlytheprojectionof(0,1)haslengths,andfallsats(c,s)=(cs,s2),That gives the second column of the projection matrix P: (cid:34) (cid:35) c2 cs Projection ontoθ-line P= . cs s2 This matrix has no inverse, because the transformation has no inverse. Points on the perpendicularlineareprojectedontotheorigin;thatlineisthenullspaceofP. Points on theθ-line are projected to themselves! Projecting twice is the same as projecting once, and P2 =P: (cid:34) (cid:35) (cid:34) (cid:35) 2 c2 cs c2(c2+s2) cs(c2+s2) P2 = = =P. cs s2 cs(c2+s2) s2(c2+s2) 0 c 1 2c2 − 1 (cid:20) (cid:21) b 2c(cid:20) (cid:21)− (cid:20) (cid:21) = (cid:20) (cid:21) 1 b s 0 2cs 2c2 − 1 2cs H = 2P −I = (cid:20) 2cs 2s2 − 1(cid:21) Image+original = 2×projection b b 1 Hx+x = 2Px (cid:20) (cid:21) 0 θ b c 0 2cs b2s(cid:20) s(cid:21)− (cid:20) 1(cid:21) = (cid:20) 2s2 − 1(cid:21) Figure2.11: Reflectionthroughtheθ-line: thegeometryandthematrix. Of course c2+s2 =cos2θ+sin2θ=1. A projection matrix equals its own square."
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. Reflection Figure 2.11 shows the reflection of (1,0) in theθ-line. The length of the reflection equals the length of the original, as it did after rotation—but here the θ- line stays where it is. The perpendicular line reversesdirection; all points go straight through the mirror, Linearity decides the rest. (cid:34) (cid:35) 2c2−1 2cs Reflection matrix H = . 2cs 2s2−1 This matrix H has the remarkable property H2 = I. Two reflections bring back the original. A reflection is its own inverse, H = H−1, which is clear from the geometry but less clear from the matrix. One approach is through the relationship of reflections to projections: H = 2P−I. This means that Hx+x = 2Px—the image plus the original equals twice the projection. It also confirms that H2 =I: H2 =(2P−I)2 =4P2−4P+I =I, since P2 =P. Other transformations Ax can increase the length of x; stretching and shearing are in theexercises. Eachexamplehasamatrixtorepresentit—whichisthemainpointofthis section. But there is also the question of choosing a basis, and we emphasize that the matrixdependsonthechoiceofbasis. Supposethefirstbasisvectorisontheθ-lineand the second basis vector is perpendicular: (cid:163) (cid:164) (i) The projection matrix is back to P = 1 0 . This matrix is constructed as always: 0 0 its first column comes from the first basis vector (projected to itself). The second column comes from the basis vector that is projected to zero."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. What matrix has the effect of rotating every vector through 90° and then projecting theresultontothex-axis? Whatmatrixrepresentsprojectionontothex-axisfollowed by projection onto the y-axis?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. Does the product of 5 reflections and 8 rotations of the x-y plane produce a rotation or a reflection? (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3. The matrix A = 2 0 produces a stretching in the x-direction. Draw the circle x2+ 0 1 y2 = 1 and sketch around it the points (2x,y) that result from multiplication by A. What shape is that curve?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "4. Every straight line remains straight after a linear transformation. If z is halfway between x and y, show that Az is halfway between Ax and Ay. (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "5. The matrix A = 1 0 yields a shearing transformation, which leaves the y-axis un- 3 1 changed. Sketch its effect on the x-axis, by indicating what happens to (1,0) and (2,0) and (−1,0)—and how the whole axis is transformed."
    },
    {
        "chapter": "VectorSpaces",
        "question": "6. What 3 by 3 matrices represent the transformations that (a) project every vector onto the x-y plane? (b) reflect every vector through the x-y plane? (c) rotate the x-y plane through 90°, leaving the z-axis alone? (d) rotate the x-y plane, then x-z, then y-z, through 90°? (e) carry out the same three rotations, but each one through 180°?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "7. On the space P of cubic polynomials, what matrix represents d2/dt2? Construct 3 the 4 by 4 matrix from the standard basis 1, t, t2, t3. Find its nullspace and column space. What do they mean in terms of polynomials?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "8. From the cubics P to the fourth-degree polynomials P , what matrix represents 3 4 multiplication by 2+3t? The columns of the 5 by 4 matrix A come from applying the transformation to 1,t,t2,t3."
    },
    {
        "chapter": "VectorSpaces",
        "question": "9. The solutions to the linear differential equation d2u/dt2 = u form a vector space (sincecombinationsofsolutionsarestillsolutions). Findtwoindependentsolutions, to give a basis for that solution space."
    },
    {
        "chapter": "VectorSpaces",
        "question": "10. With initial values u = x and du/dt = y at t = 0, what combination of basis vectors in Problem 9 solves u(cid:48)(cid:48) = u? This transformation from initial values to solution is linear. What is its 2 by 2 matrix (using x =1, y=0 and x =0, y=1 as basis for V, and your basis for W)?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "11. Verify directly from c2+s2 =1 that reflection matrices satisfy H2 =1."
    },
    {
        "chapter": "VectorSpaces",
        "question": "12. Suppose A is a linear transformation from the x-y plane to itself. Why does A−1(x+ y) = A−1x+A−1y? If A is represented by the matrix M, explain why A−1 is repre- sented by M−1."
    },
    {
        "chapter": "VectorSpaces",
        "question": "13. The product (AB)C of linear transformations starts with a vector x and produces u=Cx. Then rule 2V applies AB to u and reaches (AB)Cx. (a) Is this result the same as separately applyingC then B then A? (b) IstheresultthesameasapplyingBCfollowedbyA? Parenthesesareunnecessary and the associative law (AB)C =A(BC) holds for linear transformations. This is the best proof of the same law for matrices."
    },
    {
        "chapter": "VectorSpaces",
        "question": "14. Prove that T2 is a linear transformation if T is linear (from R3 to R3)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "15. The space of all 2 by 2 matrices has the four basis “vectors” (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 0 1 0 0 0 0 , , , . 0 0 0 0 1 0 0 1 For the linear transformation of transposing, find its matrix A with respect to this basis. Why is A2 =I?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "16. Find the 4 by 4 cyclic permutation matrix: (x ,x ,x ,x ) is transformed to Ax = 1 2 3 4 (x ,x ,x ,x ). What is the effect of A2? Show that A3 =A−1. 2 3 4 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "17. Find the 4 by 3 matrix A that represents a right shift: (x ,x ,x ) is transformed to 1 2 3 (0,x ,x ,x ). Find also the left shift matrix B from R4 back to R3, transforming 1 2 3 (x ,x ,x ,x ) to (x ,x ,x ). What are the products AB and BA? 1 2 3 4 2 3 4"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 LinearTransformations 151"
    },
    {
        "chapter": "VectorSpaces",
        "question": "18. In the vector space P of all p(x) = a +a x+a x2+a x3, let S be the subset of 3 0 1 2 3 (cid:82) 1 polynomials with p(x)dx=0. Verify that S is a subspace and find a basis. 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "19. A nonlinear transformation is invertible if T(x) = b has exactly one solution for every b. The example T(x) = x2 is not invertible because x2 = b has two solutions forpositivebandnosolutionfornegativeb. Whichofthefollowingtransformations (from the real numbers R1 to the real numbers R1) are invertible? None are linear, not even (c). (a) T(x)=x3. (b) T(x)=ex. (c) T(x)=x+11. (d) T(x)=cosx."
    },
    {
        "chapter": "VectorSpaces",
        "question": "20. What is the axis and the rotation angle for the transformation that takes (x ,x ,x ) 1 2 3 into (x ,x ,x )? 2 3 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "21. A linear transformation must leave the zero vector fixed: T(0)=0. Prove this from T(v+w)=T(v)+T(w) by choosing w= . Prove it also from the requirement T(cv)=cT(v) by choosing c="
    },
    {
        "chapter": "VectorSpaces",
        "question": "22. Which of these transformations is not linear? The input is v=(v ,v ). 1 2 (a) T(v)=(v ,v ). (b) T(v)=(v ,v ). 2 1 1 1 (c) T(v)=(0,v ). (d) T(v)=(0,1). 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "23. If S and T are linear with S(v)=T(v)=v, then S(T(v))=v or v2?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "24. Suppose T(v)=v, except that T(0,v )=(0,0). Show that this transformation satis- 2 fies T(cv)=cT(v) but not T(v+w)=T(v)+T(w)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "25. Which of these transformations satisfy T(v+w) = T(v)+T(w), and which satisfy T(cv)=cT(v)? (a) T(v)=v/(cid:107)v(cid:107). (b) T(v)=v +v +v . 1 2 3 (c) T(v)=(v ,2v ,3v ). (d) T(v)=largest component of v. 1 2 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "26. For these transformations of V=R2 to W=R2, find T(T(v)). (a) T(v)=−v. (b) T(v)=v+(1,1). (c) T(v)=90° rotation=(−v ,v ). 2 1 (cid:181) (cid:182) v +v v +v 1 2 1 2 (d) T(v)=projection= , . 2 2"
    },
    {
        "chapter": "VectorSpaces",
        "question": "27. The “cyclic” transformation T is defined by T(v ,v ,v ) = (v ,v ,v ). What is 1 2 3 2 3 1 T(T(T(v)))? What is T100(v)?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "28. Find the range and kernel (those are new words for the column space and nullspace) of T. (a) T(v ,v )=(v ,v ). (b) T(v ,v ,v )=(v ,v ). 1 2 2 1 1 2 3 1 2 (c) T(v ,v )=(0,0). (d) T(v ,v )=(v ,v ). 1 2 1 2 1 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "29. A linear transformation from V to W has an inverse from W to V when the range is all of W and the kernel contains only v = 0. Why are these transformations not invertible? (a) T(v ,v )=(v ,v ) W=R2. 1 2 2 2 (b) T(v ,v )=(v ,v ,v +v ) W=R3. 1 2 1 2 1 2 (c) T(v ,v )=v W=R1. 1 2 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "30. Suppose a linear T transforms (1,1) to (2,2) and (2,0) to (0,0). Find T(v) when (a) v=(2,2). (b) v=(3,1). (c) v=(−1,1). (d) v=(a,b). Problems 31–35 may be harder. The input space V contains all 2 by 2 matrices M. (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "31. M is any 2 by 2 matrix and A = 1 2 . The linear transformation T is defined by 3 4 T(M)=AM. What rules of matrix multiplication show that T is linear? (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "32. Suppose A = 1 2 . Show that the identity matrix I is not in the range of T. Find a 3 6 nonzero matrix M such that T(M)=AM is zero."
    },
    {
        "chapter": "VectorSpaces",
        "question": "33. Suppose T transposes every matrix M. Try to find a matrix A that gives AM = MT for every M. Show that no matrix A will do it. To professors: Is this a linear transformation that doesn’t come from a matrix?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "34. ThetransformationT thattransposeseverymatrixisdefinitelylinear. Whichofthese extra properties are true? (a) T2 = identity transformation. (b) The kernel of T is the zero matrix. (c) Every matrix is in the range of T. (d) T(M)=−M is impossible. (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "VectorSpaces",
        "question": "35. Suppose T(M) = 1 0 [M] 0 0 . Find a matrix with T(M) (cid:54)= 0. Describe all ma- 0 0 0 1 trices with T(M) = 0 (the kernel of T) and all output matrices T(M) (the range of T). Problems 36–40 are about changing the basis"
    },
    {
        "chapter": "VectorSpaces",
        "question": "36. (a) What matrix transforms (1,0) into (2,5) and transforms (0,1) to (1,3)? (b) What matrix transforms (2,5) to (1,0) and (1,3) to (0,1)?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 LinearTransformations 153 (c) Why does no matrix transform (2,6) to (1,0) and (1,3) to (0,1)?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "37. (a) What matrix M transforms (1,0) and (0,1) to (r,t) and (s,u)? (b) What matrix N transforms (a,c) and (b,d) to (1,0) and (0,1)? (c) What condition on a, b, c, d will make part (b) impossible?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "38. (a) How do M and N in Problem 37 yield the matrix that transforms (a,c) to (r,t) and (b,d) to (s,u)? (b) What matrix transforms (2,5) to (1,1) and (1,3) to (0,2)?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "39. If you keep the same basis vectors but put them in a different order, the change-of- basis matrix M is a matrix. If you keep the basis vectors in order but change their lengths, M is a matrix."
    },
    {
        "chapter": "VectorSpaces",
        "question": "40. The matrix that transforms (1,0) and (0,1) to (1,4) and (1,5) is M = . The combination a(1,4)+b(1,5) that equals (1,0) has (a,b) = ( , ). How are those new coordinates of (1,0) related to M or M−1?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "41. What are the three equations for A, B, C if the parabola Y = A+Bx+Cx2 equals 4 at x = a, 5 at x = b, and 6 at x = c? Find the determinant of the 3 by 3 matrix. For which numbers a, b, c will it be impossible to find this parabolaY?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "42. Suppose v , v , v are eigenvectors for T. This means T(v ) = λv for i = 1,2,3. 1 2 3 i i i What is the matrix for T when the input and output bases are the v’s?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "43. Every invertible linear transformation can have I as its matrix. For the output basis just choose w =T(v ). Why must T be invertible? i i"
    },
    {
        "chapter": "VectorSpaces",
        "question": "44. Suppose T is reflection across the x-axis and S is reflection across the y-axis. The domain V is the x-y plane. If v =(x,y) what is S(T(v))? Find a simpler description of the product ST."
    },
    {
        "chapter": "VectorSpaces",
        "question": "45. Suppose T is reflection across the 45° line, and S is reflection across the y-axis, If v = (2,1) then T(v) = (1,2). Find S(T(v)) and T(S(v)). This shows that generally ST (cid:54)=TS."
    },
    {
        "chapter": "VectorSpaces",
        "question": "46. Show that the product ST of two reflections is a rotation. Multiply these reflection matrices to find the rotation angle: (cid:34) (cid:35) (cid:34) (cid:35) cos2θ sin2θ cos2α sin2α . sin2θ −cos2θ sin2α −cos2α"
    },
    {
        "chapter": "VectorSpaces",
        "question": "47. The 4 by 4 Hadamard matrix is entirely +1 and −1:   1 1 1 1   1 −1 1 −1 H = . 1 1 −1 −1 1 −1 −1 1 Find H−1 and write v=(7,5,3,1) as a combination of the columns of H."
    },
    {
        "chapter": "VectorSpaces",
        "question": "48. Suppose we have two bases v ,...,v and w ,...,w for Rn. If a vector has coeffi- 1 n 1 n cients b in one basis and c in the other basis, what is the change-of-basis matrix in i i b=Mc? Start from b v +···+b v =Vb=c w +···+c w =Wc. 1 1 n n 1 1 n n Your answer represents T(v) = v with input basis of v’s and output basis of w’s. Because of different bases, the matrix is not I."
    },
    {
        "chapter": "VectorSpaces",
        "question": "49. True or false: If we know T(v) for n different nonzero vectors in R2, then we know T(v) for every vector in Rn."
    },
    {
        "chapter": "VectorSpaces",
        "question": "50. (Recommended) Suppose all vectors x in the unit square 0≤x ≤1, 0≤x ≤1 are 1 2 transformed to Ax (A is 2 by 2). (a) What is the shape of the transformed region (all Ax)? (b) For which matrices A is that region a square? (c) For which A is it a line? (d) For which A is the new area still 1? Review Exercises"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.1 Find a basis for the following subspaces of R4: (a) The vectors for which x1=2x . 4 (b) The vectors for which x +x +x =0 and x +x =0. 1 2 3 3 4 (c) The subspace spanned by (1,1,1,1), (1,2,3,4), and (2,3,4,5)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.2 By giving a basis, describe a two-dimensional subspace of R3 that contains none of the coordinate vectors (1,0,0), (0,1,0), (0,0,1)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.3 True or false, with counterexample if false: (a) If the vectors x ,...,x span a subspace S, then dimS=m. 1 m (b) The intersection of two subspaces of a vector space cannot be empty. (c) If Ax=Ay, then x=y. (d) The row space of A has a unique basis that can be computed by reducing A to echelon form. (e) If a square matrix A has independent columns, so does A2."
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 LinearTransformations 155"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.4 What is the echelon formU of A?   1 2 0 2 1   A=−1 −2 1 1 0 . 1 2 −3 −7 −2 What are the dimensions of its four fundamental subspaces?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.5 Find the rank and the nullspace of     0 0 1 0 0 1 2     A=0 0 1 and B=0 0 1 2. 1 1 1 1 1 1 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.6 Find bases for the four fundamental subspaces associated with (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 2 0 0 1 1 0 0 A= , B= , C = . 3 6 1 2 0 1 0 1"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.7 What is the most general solution to u+v+w=1, u−w=2?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.8 (a) Construct a matrix whose nullspace contains the vector x=(1,1,2). (b) Construct a matrix whose left nullspace contains y=(1,5). (c) Construct a matrix whose column space is spanned by (1,1,2) and whose row space is spanned by (1,5). (d) If you are given any three vectors in R6 and any three vectors in R5, is there a 6 by 5 matrix whose column space is spanned by the first three and whose row space is spanned by the second three?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.9 In the vector space of 2 by 2 matrices, (a) is the set of rank 1 matrices a subspace? (b) what subspace is spanned by the permutation matrices? (c) what subspace is spanned by the positive matrices (all a >0)? ij (d) what subspace is spanned by the invertible matrices?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.10 Invent a vector space that contains all linear transformations from Rn to Rn. You have to decide on a rule for addition. What is its dimension?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.11 (a) Find the rank of A, and give a basis for its nullspace.    1 1 2 0 1 2 1    2 1 0 0 2 2 0 0 A=LU =  . 2 1 2 0 0 0 0 0 1 3 2 4 1 0 0 0 0 0 0 (b) The first 3 rows ofU are a basis for the row space of A—true or false? Columns 1, 3, 6 ofU are a basis for the column space of A—true or false? The four rows of A are a basis for the row space of A—true or false? (c) Find as many linearly independent vectors b as possible for which Ax=b has a solution. (d) In elimination on A, what multiple of the third row is subtracted to knock out the fourth row?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.12 If A is an n by n−1 matrix, and its rank is n−2, what is the dimension of its nullspace?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.13 Use elimination to find the triangular factors in A=LU, if   a a a a   a b b b A= . a b c c a b c d Under what conditions on the numbers a, b, c, d are the columns linearly indepen- dent?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.14 Do the vectors (1,1,3), (2,3,6), and (1,4,3) form a basis for R3?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.15 What do you know about C(A) when the number of solutions to Ax=b is (a) 0 or 1, depending on b. (b) ∞, independent of b. (c) 0 or ∞, depending on b. (d) 1, regardless of b."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.16 In the previous exercise, how is r related to m and n in each example?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.17 If x is a vector in Rn, and xTy=0 for every y, prove that x=0."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.18 If A is an n by n matrix such that A2 =A and rankA=n, prove that A=I."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.19 What subspace of 3 by 3 matrices is spanned by the elementary matrices E , with ij 1s on the diagonal and at most one nonzero entry below?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.20 How many 5 by 5 permutation matrices are there? Are they linearly independent? Do they span the space of all 5 by 5 matrices? No need to write them all down."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.21 What is the rank of the n by n matrix with every entry equal to 1? How about the “checkerboard matrix,” with a =0 when i+ j is even, a =1 when i+ j is odd? ij ij"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2.6 LinearTransformations 157"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.22 (a) Ax=b has a solution under what conditions on b, for the following A and b?     1 2 0 3 b 1     A=0 0 0 0 and b=b . 2 2 4 0 1 b 3 (b) Find a basis for the nullspace of A. (c) Find the general solution to Ax=b, when a solution exists. (d) Find a basis for the column space of A. (e) What is the rank of AT?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.23 Howcanyouconstructamatrixthattransformsthecoordinatevectorse ,e ,e into 1 2 3 three given vectors v ,v ,v ? When will that matrix be invertible? 1 2 3"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.24 If e ,e ,e are in the column space of a 3 by 5 matrix, does it have a left-inverse? 1 2 3 Does it have a right-inverse?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.25 Suppose T is the linear transformation on R3 that takes each point (u,v,w) to (u+ v+w,u+v,u), Describe what T−1 does to the point (x,y,z)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.26 True or false? (a) Every subspace of R4 is the nullspace of some matrix. (b) If A has the same nullspace as AT, the matrix must be square. (c) The transformation that takes x to mx+b is linear (from R1 to R1)."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.27 Find bases for the four fundamental subspaces of     1 2 0 3   1 (cid:104) (cid:105) 0 2 2 2   A =  and A =1 1 4 . 1 2 0 0 0 0 1 0 0 0 4"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.28 (a) IftherowsofAarelinearlyindependent(Aismbyn)thentherankis , the column space is , and the left nullspace is . (b) IfAis8by10withatwo-dimensionalnullspace,showthatAx=bcanbesolved for every b."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.29 Describe the linear transformations of the x-y plane that are represented with stan- dard basis (1,0) and (0,1) by the matrices (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 0 0 1 A = , A = , A = . 1 2 3 0 −1 2 1 −1 0"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.30 (a) If A is square, show that the nullspace of A2 contains the nullspace of A. (b) Show also that the column space of A2 is contained in the column space of A."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.31 When does the rank-1 matrix A=uvT have A2 =0?"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.32 (a) Find a basis for the space of all vectors in R6 with x +x =x +x =x +x . 1 2 3 4 5 6 (b) Find a matrix with that subspace as its nullspace. (c) Find a matrix with that subspace as its column space."
    },
    {
        "chapter": "VectorSpaces",
        "question": "1.33 Suppose the matrices in PA=LU are       0 1 0 0 0 0 1 −3 2 1 0 0 0 2 −1 4 2 1       1 0 0 02 −1 4 2 1 0 1 0 00 0 1 −3 2   =  . 0 0 0 14 −2 9 1 4 1 1 1 00 0 0 0 2 0 0 1 0 2 −1 5 −1 5 2 1 0 1 0 0 0 0 0 (a) What is the rank of A? (b) What is a basis for the row space of A? (c) True or false: Rows 1, 2, 3 of A are linearly independent. (d) What is a basis for the column space of A? (e) What is the dimension of the left nullspace of A? (f) What is the general solution to Ax=0? 3 Chapter Orthogonality"
    },
    {
        "chapter": "VectorSpaces",
        "question": "3.1 Orthogonal Vectors and Subspaces A basis is a set of independent vectors that span a space. Geometrically, it is a set of coordinate axes. A vector space is defined without those axes, but every time I think of the x-y plane or three-dimensional space or Rn, the axes are there. They are usually per- pendicular! The coordinate axes that the imagination constructs are practically always orthogonal. In choosing a basis, we tend to choose an orthogonal basis. The idea of an orthogonal basis is one of the foundations of linear algebra. We need a basis to convert geometric constructions into algebraic calculations, and we need an orthogonal basis to make those calculations simple. A further specialization makes the basis just about optimal: The vectors should have length 1. For an orthonormal basis (orthogonal unit vectors), we will find"
    },
    {
        "chapter": "VectorSpaces",
        "question": "1. the length (cid:107)x(cid:107) of a vector;"
    },
    {
        "chapter": "VectorSpaces",
        "question": "2. the test xTy=0 for perpendicular vectors; and"
    },
    {
        "chapter": "Orthogonality",
        "question": "3. how to create perpendicular vectors from linearly independent vectors. More than just vectors, subspaces can also be perpendicular. We will discover, so beautifully and simply that it will be a delight to see, that the fundamental subspaces meet at right angles. Those four subspaces are perpendicular in pairs, two in Rm and two in Rn. That will complete the fundamental theorem of linear algebra. The first step is to find the length of a vector. It is denoted by (cid:107)x(cid:107), and in two dimensions it comes from the hypotenuse of a right triangle (Figure 3.1a). The square of the length was given a long time ago by Pythagoras: (cid:107)x(cid:107)2 =x2+x2. 1 2 In three-dimensional space, x=(x ,x ,x ) is the diagonal of a box (Figure 3.1b). Its 1 2 3 length comes from two applications of the Pythagorean formula. The two-dimensional case takes care of (x ,x ,0)=(1,2,0) across the base. This forms a right angle with the 1 2 verticalside(0,0,x )=(0,0,3). Thehypotenuseoftheboldtriangle(Pythagorasagain) 3 (0,0,3) √ (1,2) (1,2,3) has length 14 (0,2) b kxk2 = x2 1 +x2 2 +x2 3 2 2 5 = 1 +2 x √5 2 2 2 2 14 = 1 +2 +3 1 (0,2,0) (1,0) √ (1,0,0) (1,2,0) has length 5 (a) (b) Figure3.1: Thelengthofvectors(x ,x )and(x ,x ,x ). 1 2 1 2 3 is the length (cid:107)x(cid:107) we want: (cid:113) Length in 3D (cid:107)x(cid:107)2 =12+22+32 and (cid:107)x(cid:107)= x2+x2+x2. 1 2 3 The extension to x =(x ,...,x ) in n dimensions is immediate. By Pythagoras n−1 1 n times, the length (cid:107)x(cid:107) in Rn is the positive square root of xTx: Length squared (cid:107)x(cid:107)2 =x2+x2+···+x2 =xTx. (1) 1 2 n √ The sum of squares matches xTx—and the length of x=(1,2,−3) is 14:   1 (cid:104) (cid:105)   xTx= 1 2 −3  2 =12+22+(−3)2 =14. −3 Orthogonal Vectors How can we decide whether two vectors x and y are perpendicular? What is the test for orthogonality in Figure 3.2? In the plane spanned by x and y, those vectors are orthogonal provided they form a right triangle. We go back to a2+b2 =c2: Sides of a right triangle (cid:107)x(cid:107)2+(cid:107)y(cid:107)2 =(cid:107)x−y(cid:107)2. (2) Applying the length formula (1), this test for orthogonality in Rn becomes (cid:161) (cid:162) (cid:161) (cid:162) x2+···+x2 + y2+···+y2 =(x −y )2+···+(x −y )2. 1 n 1 n 1 1 n n The right-hand side has an extra −2x y from each (x −y )2: i i i i (cid:161) (cid:162) (cid:161) (cid:162) right-hand side= x2+···+x2 −2(x y +···+x y )+ y2+···+y2 . 1 n 1 1 n n 1 n"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.1 OrthogonalVectorsandSubspaces 161 √ − 1 25 Right angle y = (cid:20) (cid:21) 4 T 2√ √ x = (cid:20) (cid:21) x y = 0 2 5 20 b T T x y < 0 x y > 0 T x y = 0 greater than 90° less than 90° Figure3.2: Arighttrianglewith5+20=25. Dottedangle100°,dashedangle30°. We have a right triangle when that sum of cross-product terms x y is zero: i i Orthogonal vectors xTy=x y +···+x y =0. (3) 1 1 n n This sum is xTy=∑x y =yTx, the row vector xT times the column vector y: i i   y (cid:104) (cid:105) 1  .  Inner product xTy= x 1 ··· x n  . . =x 1y 1+···+x ny n. (4) y n Thisnumberissometimescalledthescalarproductordotproduct,anddenotedby(x,y) or x·y. We will use the name inner product and keep the notation xTy. 3A TheinnerproductxTyiszeroifandonlyifxandyareorthogonalvectors. If xTy > 0, their angle is less than 90°. If xTy < 0, their angle is greater than 90°. The length squared is the inner product of x with itself: xTx=x2+···+x2 =(cid:107)x(cid:107)2. The 1 n only vector with length zero—the only vector orthogonal to itself—is the zero vector. This vector x=0 is orthogonal to every vector in Rn. √ Example 1. (2,2,−1) is orthogonal to (−1,2,2). Both have length 4+4+1=3. Useful fact: If nonzero vectors v ,...,v are mutually orthogonal (every vector is 1 k perpendicular to every other), then those vectors are linearly independent. Proof. Suppose c v +···+c v = 0. To show that c must be zero, take the inner 1 1 k k 1 product of both sides with v . Orthogonality of the v’s leaves only one term: 1 vT(c v +···+c v )=c vTv =0. (5) 1 1 1 k k 1 1 1 The vectors are nonzero, so vTv (cid:54)=0 and therefore c =0. The same is true of every c . 1 1 1 i The only combination of the v’s producing zero has all c =0: independence! i The coordinate vectors e ,...,e in Rn are the most important orthogonal vectors. 1 n Those are the columns of the identity matrix. They form the simplest basis for Rn, and theyareunitvectors—eachhaslength(cid:107)e (cid:107)=1. Theypointalongthecoordinateaxes. If i these axes are rotated, the result is a new orthonormal basis: a new system of mutually orthogonal unit vectors. In R2 we have cos2θ+sin2θ=1: Orthonormal vectors in R2 v =(cosθ,sinθ) and v =(−sinθ,cosθ). 1 2 Orthogonal Subspaces We come to the orthogonality of two subspaces. Every vector in one subspace must be orthogonal to every vector in the other subspace. Subspaces of R3 can have dimension 0, 1, 2, or 3. The subspaces are represented by lines or planes through the origin— and in the extreme cases, by the origin alone or the whole space. The subspace {0} is orthogonal to all subspaces. A line can be orthogonal to another line, or it can be orthogonal to a plane, but a plane cannot be orthogonal to a plane. I have to admit that the front wall and side wall of a room look like perpendicular planes in R3. But by our definition, that is not so! There are lines v and w in the front and side walls that do not meet at a right angle. The line along the corner is in both walls, and it is certainly not orthogonal to itself. 3B Two subspaces V and W of the same space Rn are orthogonal if every vector v in V is orthogonal to every vector w in W: vTw=0 for all v and w. Example 2. Suppose V is the plane spanned by v =(1,0,0,0) and v =(1,1,0,0). If 1 2 W is the line spanned by w = (0,0,4,5), then w is orthogonal to both v’s. The line W will be orthogonal to the whole plane V. In this case, with subspaces of dimension 2 and 1 in R4, there is room for a third subspace. The line L through z = (0,0,5,−4) is perpendicular to V and W. Then the dimensions add to 2+1+1=4. What space is perpendicular to all of V, W, and L? The important orthogonal subspaces don’t come by accident, and they come two at a time. In fact orthogonal subspaces are unavoidable: They are the fundamental sub- spaces! The first pair is the nullspace and row space. Those are subspaces of Rn—the rows have n components and so does the vector x in Ax = 0. We have to show, using Ax=0, that the rows of A are orthogonal to the nullspace vector x. 3C Fundamental theorem of orthogonality The row space is orthogonal to the nullspace (in Rn). The column space is orthogonal to the left nullspace (in Rm). First Proof. Suppose x is a vector in the nullspace. Then Ax = 0, and this system of m"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.1 OrthogonalVectorsandSubspaces 163 equations can be written out as rows of A multiplying x:      ··· row 1 ··· x 0 1      Every row is ··· row 2 ···x 2 0 Ax= . . .  . =.. (6) orthogonal to x  . . . . . .  . .  . . ··· row m ··· x 0 n The main point is already in the first equation: row 1 is orthogonal to x. Their inner product is zero; that is equation 1. Every right-hand side is zero, so x is orthogonal to every row. Therefore x is orthogonal to every combination of the rows. Each x in the nullspace is orthogonal to each vector in the row space, so N(A)⊥C(AT). The other pair of orthogonal subspaces comes from ATy=0, or yTA=0:   c c o o    l l  (cid:104) (cid:105) (cid:104) (cid:105)   u ··· u yTA= y ··· y   = 0 ··· 0 . (7) 1 m m m   n n   1 n The vector y is orthogonal to every column. The equation says so, from the zeros on the right-hand side. Therefore y is orthogonal to every combination of the columns. It is orthogonal to the column space, and it is a typical vector in the left nullspace: N(AT)⊥C(A). This is the same as the first half of the theorem, with A replaced by AT. Second Proof. The contrast with this “coordinate-free proof” should be useful to the reader. It shows a more “abstract” method of reasoning. I wish I knew which proof is clearer, and more permanently understood. If x is in the nullspace then Ax=0. If v is in the row space, it is a combination of the rows: v=ATz for some vector z. Now, in one line: Nullspace ⊥ Row space vTx=(ATz)Tx=zTAx=zT0=0. (8) Example 3. Suppose A has rank 1, so its row space and column space are lines:   1 3   Rank-1 matrix A=2 6. 3 9 Therowsaremultiplesof(1,3). Thenullspacecontainsx=(−3,1),whichisorthogonal to all the rows. The nullspace and row space are perpendicular lines in R2: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) 3 3 3 1 3 =0 and 2 6 =0 and 3 9 =0. −1 −1 −1 In contrast, the other two subspaces are in R3. The column space is the line through (1,2,3). The left nullspace must be the perpendicular plane y +2y +3y = 0. That 1 2 3 equation is exactly the content of yTA=0. Thefirsttwosubspaces(thetwolines)haddimensions1+1=2inthespaceR2. The second pair (line and plane) had dimensions 1+2 = 3 in the space R3. In general, the row space and nullspace have dimensions that add to r+(n−r) = n. The other pair adds to r+(m−r)=m. Something more than orthogonality is occurring, and I have to ask your patience about that one further point: the dimensions. It is certainly true that the null space is perpendicular to the row space—but it is not thewholetruth. N(A)containseveryvectororthogonaltotherowspace. Thenullspace was formed from all solutions to Ax=0. Definition. GivenasubspaceVofRn,thespaceofallvectorsorthogonaltoViscalled the orthogonal complement of V. It is denoted by V⊥ = “V perp.” Usingthisterminology,thenullspaceistheorthogonalcomplementoftherowspace: N(A)=(C(AT))⊥. Atthesametime,therowspacecontainsallvectorsthatareorthog- onaltothenullspace. Avectorzcan’tbeorthogonaltothenullspacebutoutsidetherow space. Adding z as an extra row of A would enlarge the row space, but we know that there is a fixed formula r+(n−r)=n: Dimension formula dim(row space)+dim(nullspace)=number of columns. Every vector orthogonal to the nullspace is in the row space: C(AT)=(N(A))⊥. ThesamereasoningappliedtoAT producesthedualresult: TheleftnullspaceN(AT) and the column space C(A) are orthogonal complements. Their dimensions add up to (m−r)+r = m, This completes the second half of the fundamental theorem of linear algebra. The first half gave the dimensions of the four subspaces. including the fact that row rank = column rank. Now we know that those subspaces are perpendicular. More than that, the subspaces are orthogonal complements. 3D Fundamental Theorem of Linear Algebra, Part II The nullspace is the orthogonal complement of the row space in Rn. The left nullspace is the orthogonal complement of the column space in Rm. To repeat, the row space contains everything orthogonal to the nullspace. The column spacecontainseverythingorthogonaltotheleftnullspace. Thatisjustasentence,hidden in the middle of the book, but it decides exactly which equations can be solved! Looked at directly, Ax = b requires b to be in the column space. Looked at indirectly. Ax = b requires b to be perpendicular to the left nullspace. 3E Ax=b is solvable if and only if yTb=0 whenever yTA=0."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.1 OrthogonalVectorsandSubspaces 165 The direct approach was “b must be a combination of the columns.” The indirect ap- proach is “b must be orthogonal to every vector that is orthogonal to the columns.” That doesn’t sound like an improvement (to put it mildly). But if only one or two vectors are orthogonal to the columns. it is much easier to check those one or two conditions yTb = 0. A good example is Kirchhoff’s Voltage Law in Section 2.5. Testing for zero around loops is much easier than recognizing combinations of the columns. When the left-hand sides of Ax=b add to zero, the right-hand sides must, too:   x 1−x 2 =b 1 1 −1 0   x 2−x 3 =b 2 is solvable if and only if b 1+b 2+b 3 =0. Here A= 0 1 −1. x −x =b −1 0 1 3 1 3 This test b +b +b = 0 makes b orthogonal to y = (1,1,1) in the left nullspace. By 1 2 3 the Fundamental Theorem, b is a combination of the columns! The Matrix and the Subspaces We emphasize that V and W can be orthogonal without being complements. Their dimensions can be too small. The line V spanned by (0,1,0) is orthogonal to the line W spanned by (0,0,1), but V is not W⊥. The orthogonal complement of W is a two- dimensional plane, and the line is only part of W⊥. When the dimensions are right, orthogonal subspaces are necessarily orthogonal complements: If W=V⊥ then V=W⊥ and dimV+dimW=n. In other words V⊥⊥ = V. The dimensions of V and W are right, and the whole space Rn is being decomposed into two perpendicular parts (Figure 3.3). W W 3 Two orthogonal axes in R Line W perpendicular to plane V ⊥ Not orthogonal complements Orthogonal complements V = W V V Figure3.3: OrthogonalcomplementsinR3: aplaneandaline(nottwolines). Splitting Rn into orthogonal parts will split every vector into x=v+w. The vector v is the projection onto the subspace V. The orthogonal component w is the projection of x onto W. The next sections show how to find those projections of x. They lead to what is probably the most important figure in the book (Figure 3.4). Figure 3.4 summarizes the fundamental theorem of linear algebra. It illustrates the true effect of a matrix—what is happening inside the multiplication Ax. The nullspace Figure3.4: ThetrueactionAx=A(x +x )ofanymbynmatrix. row null is carried to the zero vector. Every Ax is in the column space. Nothing is carried to the left nullspace. The real action is between the row space and column space, and you see it by looking at a typical vector x. It has a “row space component” and a “nullspace component,” with x=x +x . When multiplied by A, this is Ax=Ax +Ax : r n r n The nullspace component goes to zero: Ax =0. n The row space component goes to the column space: Ax =Ax. r Of course everything goes to the column space—the matrix cannot do anything else. I tried to make the row and column spaces the same size, with equal dimension r. 3F From the row space to the column space, A is actually invertible. Every vector b in the column space comes from exactly one vector x in the row r space. Proof. Every b in the column space is a combination Ax of the columns. In fact, b is Ax , with x in the row space, since the nullspace component gives Ax = 0, If another r r n vector x(cid:48) in the row space gives Ax(cid:48) = b, then A(x −x(cid:48)) = b−b = 0. This puts x −x(cid:48) r r r r r r in the nullspace and the row space, which makes it orthogonal to itself. Therefore it is zero, and x −x(cid:48). Exactly one vector in the row space is carried to b. r r Every matrix transforms its row space onto its column space. On those r-dimensional spaces A is invertible. On its nullspace A is zero. When A is diagonal, you see the invertible submatrix holding the r nonzeros. AT goes in the opposite direction, from Rm to Rn and from C(A) back to C(AT). Of course the transpose is not the inverse! AT moves the spaces correctly, but not the"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. Find the lengths and the inner product of x=(1,4,0,2) and y=(2,−2,1,3)."
    },
    {
        "chapter": "Orthogonality",
        "question": "2. Giveanexamplein R2 oflinearlyindependentvectorsthatarenot orthogonal. Also, give an example of orthogonal vectors that are not independent."
    },
    {
        "chapter": "Orthogonality",
        "question": "3. Two lines in the plane are perpendicular when the product of their slopes is −1. Apply this to the vectors x = (x ,x ) and y = (y ,y ), whose slopes are x /x and 1 2 1 2 2 1 y /y , to derive again the orthogonality condition xTy=0. 2 1"
    },
    {
        "chapter": "Orthogonality",
        "question": "4. How do we know that the ith row of an invertible matrix B is orthogonal to the jth column of B−1, if i(cid:54)= j?"
    },
    {
        "chapter": "Orthogonality",
        "question": "5. Which pairs are orthogonal among the vectors v , v , v , v ? 1 2 3 4         1 4 1 1          2  0 −1 1 v = , v = , v = , v = . 1 2 3 4 −2 4 −1 1 1 0 −1 1"
    },
    {
        "chapter": "Orthogonality",
        "question": "6. Find all vectors in R3 that are orthogonal to (1,1,1) and (1,−1,0). Produce an orthonormal basis from these vectors (mutually orthogonal unit vectors)."
    },
    {
        "chapter": "Orthogonality",
        "question": "7. Find a vector x orthogonal to the row space of A, and a vector y orthogonal to the column space, and a vector z orthogonal to the nullspace:   1 2 1   A=2 4 3. 3 6 4"
    },
    {
        "chapter": "Orthogonality",
        "question": "8. IfVandWareorthogonalsubspaces,showthattheonlyvectortheyhaveincommon is the zero vector: V∩W={0}."
    },
    {
        "chapter": "Orthogonality",
        "question": "9. Find the orthogonal complement of the plane spanned by the vectors (1,1,2) and (1,2,3), by taking these to be the rows of A and solving Ax=0. Remember that the complement is a whole line."
    },
    {
        "chapter": "Orthogonality",
        "question": "10. Constructahomogeneousequationinthreeunknownswhosesolutionsarethelinear combinations of the vectors (1,1,2) and (1,2,3). This is the reverse of the previous exercise, but the two problems are really the same."
    },
    {
        "chapter": "Orthogonality",
        "question": "11. The fundamental theorem is often stated in the form of Fredholm’s alternative: For any A and b, one and only one of the following systems has a solution: (i) Ax=b. (ii) ATy=0, yTb(cid:54)=0. Either b is in the column space C(A) or there is a y in N(AT) such that yTb (cid:54)= 0. Show that it is contradictory for (i) and (ii) both to have solutions."
    },
    {
        "chapter": "Orthogonality",
        "question": "12. Find a basis for the orthogonal complement of the row space of A: (cid:34) (cid:35) 1 0 2 A= . 1 1 4 Split x=(3,3,3) into a row space component x and a nullspace component x . r n"
    },
    {
        "chapter": "Orthogonality",
        "question": "13. Illustrate the action of AT by a picture corresponding to Figure 3.4, sending C(A) back to the row space and the left nullspace to zero."
    },
    {
        "chapter": "Orthogonality",
        "question": "14. Show that x−y is orthogonal to x+y if and only if (cid:107)x(cid:107)=(cid:107)y(cid:107)."
    },
    {
        "chapter": "Orthogonality",
        "question": "15. Findamatrixwhoserowspacecontains(1,2,1)andwhosenullspacecontains(1,−2,1), or prove that there is no such matrix."
    },
    {
        "chapter": "Orthogonality",
        "question": "16. Find all vectors that are perpendicular to (1,4,4,1) and (2,9,8,2)."
    },
    {
        "chapter": "Orthogonality",
        "question": "17. If V is the orthogonal complement of W in Rn, is there a matrix with row space V and nullspace W? Starting with a basis for V, construct such a matrix."
    },
    {
        "chapter": "Orthogonality",
        "question": "18. If S={0} is the subspace of R4 containing only the zero vector, what is S⊥? If S is spanned by (0,0,0,1), what is S⊥? What is (S⊥)⊥?"
    },
    {
        "chapter": "Orthogonality",
        "question": "19. Why are these statements false? (a) If V is orthogonal to W, then V⊥ is orthogonal to W⊥. (b) V orthogonal to W and W orthogonal to Z makes V orthogonal to Z."
    },
    {
        "chapter": "Orthogonality",
        "question": "20. Let S be a subspace of Rn. Explain what (S⊥)⊥ =S means and why it is true."
    },
    {
        "chapter": "Orthogonality",
        "question": "21. Let P be the plane in R2 with equation x+2y−z = 0. Find a vector perpendicular to P. What matrix has the plane P as its nullspace, and what matrix has P as its row space?"
    },
    {
        "chapter": "Orthogonality",
        "question": "22. Let S be the subspace of R4 containing all vectors with x +x +x +x =0. Find a 1 2 3 4 basis for the space S⊥, containing all vectors orthogonal to S."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.1 OrthogonalVectorsandSubspaces 169"
    },
    {
        "chapter": "Orthogonality",
        "question": "23. Construct an unsymmetric 2 by 2 matrix of rank 1. Copy Figure 3.4 and put one vector in each subspace. Which vectors are orthogonal?"
    },
    {
        "chapter": "Orthogonality",
        "question": "24. Redraw Figure 3.4 for a 3 by 2 matrix of rank r = 2. Which subspace is Z (zero vector only)? The nullspace part of any vector x in R2 is x = . n"
    },
    {
        "chapter": "Orthogonality",
        "question": "25. Construct a matrix with the required property or say why that is impossible. (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) 1 2 1 (a) Column space contains 2 and −3 , nullspace contains 1 . −3 5 1 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) 1 2 1 (b) Row space contains 2 and −3 , nullspace contains 1 . (cid:104) (cid:105) −3 (cid:104)5 (cid:105) (cid:104) (cid:105) 1 1 1 0 (c) Ax= 1 has a solution and AT 0 = 0 . 1 0 0 (d) Every row is orthogonal to every column (A is not the zero matrix). (e) The columns add up to a column of 0s, the rows add to a row of 1s."
    },
    {
        "chapter": "Orthogonality",
        "question": "26. If AB=0 then the columns of B are in the of A. The rows of A are in the of B. Why can’t A and B be 3 by 3 matrices of rank 2?"
    },
    {
        "chapter": "Orthogonality",
        "question": "27. (a) If Ax=b has a solution and ATy=0, then y is perpendicular to . (b) If ATy=c has a solution and Ax=0, then x is perpendicular to ."
    },
    {
        "chapter": "Orthogonality",
        "question": "28. This is a system of equations Ax=b with no solution: x+2y+2z=5 2x+2y+3z=5 3x+4y+5z=9. Find numbers y , y , y to multiply the equations so they add to 0 = 1. You have 1 2 3 found a vector y in which subspace? The inner product yTb is 1."
    },
    {
        "chapter": "Orthogonality",
        "question": "29. In Figure 3.4, how do we know that Ax is equal to Ax? How do we know that this (cid:163) r (cid:164) (cid:163) (cid:164) vector is in the column space? If A= 1 1 and x= 1 what is x ? 1 1 0 r"
    },
    {
        "chapter": "Orthogonality",
        "question": "30. If Ax is in the nullspace of AT then Ax =0. Reason: Ax is also in the of A and the spaces are . Conclusion: ATA has the same nullspace as A."
    },
    {
        "chapter": "Orthogonality",
        "question": "31. Suppose A is a symmetric matrix (AT =A). (a) Why is its column space perpendicular to its nullspace? (b) If Ax = 0 and Az = 5z, which subspaces contain these “eigenvectors” x and z? Symmetric matrices have perpendicular eigenvectors (see Section 5.5)."
    },
    {
        "chapter": "Orthogonality",
        "question": "32. (Recommended) Draw Figure 3.4 to show each subspace for (cid:34) (cid:35) (cid:34) (cid:35) 1 2 1 0 A= and B= . 3 6 3 0"
    },
    {
        "chapter": "Orthogonality",
        "question": "33. Find the pieces x and x , and draw Figure 3.4 properly, if r n   (cid:34) (cid:35) 1 −1   2 A=0 0  and x= . 0 0 0 Problems 34–44 are about orthogonal subspaces."
    },
    {
        "chapter": "Orthogonality",
        "question": "34. Put bases for the orthogonal subspaces V and W into the columns of matricesV and W. Why doesVTW = zero matrix? This matches vTw=0 for vectors."
    },
    {
        "chapter": "Orthogonality",
        "question": "35. The floor and the wall are not orthogonal subspaces because they share a nonzero vector (along the line where they meet). Two planes in R3 cannot be orthogonal! Find a vector in both column spaces C(A) and C(B):     1 2 5 4     A=1 3 and B=6 3. 1 2 5 1 This will be a vector Ax and also Bx(cid:98). Think 3 by 4 with the matrix [A B]."
    },
    {
        "chapter": "Orthogonality",
        "question": "36. Extend Problem 35 to a p-dimensional subspace V and a q-dimensional subspace W of Rn. What inequality on p+q guarantees that V intersects W in a nonzero vector? These subspaces cannot be orthogonal."
    },
    {
        "chapter": "Orthogonality",
        "question": "37. Prove that every y in N(AT) is perpendicular to every Ax in the column space, using the matrix shorthand of equation (8). Start from ATy=0."
    },
    {
        "chapter": "Orthogonality",
        "question": "38. IfSisthesubspaceofR3 containingonlythezerovector,whatisS⊥? IfSisspanned by (1,1,1), what is S⊥? If S is spanned by (2,0,0) and (0,0,3), what is S⊥?"
    },
    {
        "chapter": "Orthogonality",
        "question": "39. Suppose S only contains (1,5,1) and (2,2,2) (not a subspace). Then S⊥ is the nullspace of the matrix A= . S⊥ is a subspace even if S is not."
    },
    {
        "chapter": "Orthogonality",
        "question": "40. Suppose L is a one-dimensional subspace (a line) in R3. Its orthogonal complement L⊥ is the perpendicular to L. Then (L⊥)⊥ is a perpendicular to L⊥. In fact (L⊥)⊥ is the same as ."
    },
    {
        "chapter": "Orthogonality",
        "question": "41. Suppose V is the whole space R4. Then V⊥ contains only the vector . Then (V⊥)⊥ is . So (V⊥)⊥ is the same as ."
    },
    {
        "chapter": "Orthogonality",
        "question": "42. Suppose S is spanned by the vectors (1,2,2,3) and (1,3,3,2). Find two vectors that span S⊥. This is the same as solving Ax=0 for which A?"
    },
    {
        "chapter": "Orthogonality",
        "question": "43. If P is the plane of vectors in R4 satisfying x +x +x +x = 0, write a basis for 1 2 3 4 P⊥. Construct a matrix that has P as its nullspace."
    },
    {
        "chapter": "Orthogonality",
        "question": "44. If a subspace S is contained in a subspace V, prove that S⊥ contains V⊥. Problems 45–50 are about perpendicular columns and rows."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.2 CosinesandProjectionsontoLines 171"
    },
    {
        "chapter": "Orthogonality",
        "question": "45. Suppose an n by n matrix is invertible: AA−1 = I. Then the first column of A−1 is orthogonal to the space spanned by which rows of A?"
    },
    {
        "chapter": "Orthogonality",
        "question": "46. Find ATA if the columns of A are unit vectors, all mutually perpendicular."
    },
    {
        "chapter": "Orthogonality",
        "question": "47. Construct a 3 by 3 matrix A with no zero entries whose columns are mutually per- pendicular. Compute ATA. Why is it a diagonal matrix?"
    },
    {
        "chapter": "Orthogonality",
        "question": "48. The lines 3x+y=b and 6x+2y=b are . They are the same line if . In 1 2 that case (b ,b ) is perpendicular to the vector . The nullspace of the matrix is 1 2 the line 3x+y= . One particular vector in that nullspace is ."
    },
    {
        "chapter": "Orthogonality",
        "question": "49. Why is each of these statements false? (a) (1,1,1)isperpendicularto(1,1,−2),sotheplanesx+y+z=0andx+y−2z= 0 are orthogonal subspaces. (b) The subspace spanned by (1,1,0,0,0) and (0,0,0,1,1) is the orthogonal com- plement of the subspace spanned by (1,−1,0,0,0) and (2,−2,3,4,−4). (c) Two subspaces that meet only in the zero vector are orthogonal."
    },
    {
        "chapter": "Orthogonality",
        "question": "50. Find a matrix with v = (1,2,3) in the row space and column space. Find another matrixwithvinthenullspaceandcolumnspace. Whichpairsofsubspacescanvnot be in?"
    },
    {
        "chapter": "Orthogonality",
        "question": "51. Suppose A is 3 by 4, B is 4 by 5, and AB=0. Prove rank(A)+rank(B)≤4."
    },
    {
        "chapter": "Orthogonality",
        "question": "52. The command N = null(A) will produce a basis for the nullspace of A. Then the command B = null(N’) will produce a basis for the of A."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.2 Cosines and Projections onto Lines Vectors with xTy = 0 are orthogonal. Now we allow inner products that are not zero, and angles that are not right angles. We want to connect inner products to angles, and also to transposes. In Chapter 1 the transpose was constructed by flipping over a matrix as if it were some kind of pancake. We have to do better than that. One fact is unavoidable: The orthogonal case is the most important. Suppose we want to find the distance from a point b to the line in the direction of the vector a. We are looking along that line for the point p closest to b. The key is in the geometry: The lineconnectingbto p(thedottedlineinFigure3.5)isperpendiculartoa. Thisfactwill allow us to find the projection p. Even though a and b are not orthogonal, the distance problem automatically brings in orthogonality. The situation is the same when we are given a plane (or any subspace S) instead of a line. Again the problem is to find the point p on that subspace that is closest to b. This b e = b−p a projection of b b p = θ onto line through a Figure3.5: Theprojection pisthepoint(onthelinethrougha)closesttob. point pistheprojectionofbontothesubspace. AperpendicularlinefrombtoSmeets thesubspaceat p. Geometrically,thatgivesthedistancebetweenpointsbandsubspaces S. But there are two questions that need to be asked:"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. Does this projection actually arise in practical applications?"
    },
    {
        "chapter": "Orthogonality",
        "question": "2. If we have a basis for the subspace S, is there a formula for the projection p? The answers are certainly yes. This is exactly the problem of the least-squares solu- tion to an overdetermined system. The vector b represents the data from experiments or questionnaires, and it contains too many errors to be found in the subspace S. When we try to write b as a combination of the basis vectors for S, it cannot be done—the equations are inconsistent, and Ax=b has no solution. The least-squares method selects p as the best choice to replace b. There can be no doubt of the importance of this application. In economics and statistics, least squares enters regression analysis. In geodesy, the U.S. mapping survey tackled 2.5 million equations in 400,000 unknowns. Aformulafor piseasywhenthesubspaceisaline. Wewillprojectbontoainseveral differentways,andrelatetheprojection ptoinnerproductsandangles. Projectionontoa higher dimensional subspace is by far the most important case; it corresponds to a least- squares problem with several parameters, and it is solved in Section 3.3. The formulas are even simpler when we produce an orthogonal basis for S. inner products and cosines We pick up the discussion of inner products and angles. You will soon see that it is not the angle, but the cosine of the angle, that is directly related to inner products. We look back to trigonometry in the two-dimensional case to find that relationship. Suppose the vectors a and b make anglesαandβ with the x-axis (Figure 3.6). The length (cid:107)a(cid:107) is the hypotenuse in the triangle OaQ. So the sine and cosine ofαare a a 2 1 sinα= , cosα= . (cid:107)a(cid:107) (cid:107)a(cid:107)"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.2 CosinesandProjectionsontoLines 173 y b = (b1,b2) cosθ (cid:20) (cid:21) kbk kb−ak sinθ β kak a = (a1,a2) θ θ 1 α b (cid:20) (cid:21) b x u i = cosθ 0 O Q · Figure3.6: Thecosineoftheangleθ=β−αusinginnerproducts. For the angle β, the sine is b /(cid:107)b(cid:107) and the cosine is b /(cid:107)b(cid:107) . The cosine of θ=β−α 2 1 comes from an identity that no one could forget: a b +a b 1 1 2 2 Cosine formula cosθ=cosβcosα+sinβsinα= . (1) (cid:107)a(cid:107)(cid:107)b(cid:107) The numerator in this formula is exactly the inner product of a and b. It gives the relationship between aTb and cosθ: 3G The cosine of the angle between any nonzero vectors a and b is aTb Cosine ofθ cosθ= . (2) (cid:107)a(cid:107)(cid:107)b(cid:107) This formula is dimensionally correct; if we double the length of b, then both numerator and denominator are doubled, and the cosine is unchanged. Reversing the sign of b, on the other hand, reverses the sign of cosθ—and changes the angle by 180°. There is another law of trigonometry that leads directly to the same result. It is not so unforgettable as the formula in equation (1), but it relates the lengths of the sides of any triangle: Law of Cosines (cid:107)b−a(cid:107)2 =(cid:107)b(cid:107)2+(cid:107)a(cid:107)2−2(cid:107)b(cid:107)(cid:107)a(cid:107)cosθ. (3) When θ is a right angle, we are back to Pythagoras: (cid:107)b−a(cid:107)2 = (cid:107)b(cid:107)2+(cid:107)a(cid:107)2. For any angleθ, the expression (cid:107)b−a(cid:107)2 is (b−a)T(b−a), and equation (3) becomes bTb−2aTb+aTa=bTb+aTa−2(cid:107)b(cid:107)(cid:107)a(cid:107)cosθ. Canceling bTb and aTa on both sides of this equation, you recognize formula (2) for the cosine: aTb = (cid:107)a(cid:107)(cid:107)b(cid:107)cosθ. In fact, this proves the cosine formula in n dimensions, since we only have to worry about the plane triangle Oab. Projection onto a Line Now we want to find the projection point p. This point must be some multiple p=x(cid:98)a of thegivenvectora—everypointonthelineisamultipleofa. Theproblemistocompute b e = b−p a T a b b p = xa = a θ aTa b Op aTb Figure3.7: Theprojection pofbontoa,withcosθ= = . Ob (cid:107)a(cid:107)(cid:107)b(cid:107) the coefficient x(cid:98). All we need is the geometrical fact that the line from b to the closest point p=x(cid:98)a is perpendicular to the vector a: aTb (b−a(cid:98))⊥a, or aT(b−a(cid:98))=0, or x(cid:98)= . (4) aTa That gives the formula for the number x(cid:98)and the projection p: 3H Theprojectionofthevectorbontothelineinthedirectionof ais p=x(cid:98)a: aTb Projection onto a line p=x(cid:98)a= a. (5) aTa This allows us to redraw Figure 3.5 with a correct formula for p (Figure 3.7). This leads to the Schwarz inequality in equation (6), which is the most important inequality in mathematics. A special case is the fact that arithmetic means 1(x+y) are √ 2 larger than geometric means xy. (It is also equivalent—see Problem 1 at the end of this section—to the triangle inequality for vectors.) The Schwarz inequality seems to come almost accidentally from the statement that (cid:107)e(cid:107)2 =(cid:107)b−p(cid:107)2 in Figure 3.7 cannot be negative: (cid:176) (cid:176) (cid:181) (cid:182) (cid:176) aTb (cid:176)2 (aTb)2 aTb 2 (bTb)(aTa)−(aTb)2 (cid:176) b− a(cid:176) =bTb−2 + aTa= ≥0. (cid:176) (cid:176) aTa aTa aTa (aTa) This tells us that (bTb)(aTa)≥(aTb)2—and then we take square roots: 3I All vectors a and b satisfy the Schwarz inequality, which is |cosθ|≤1 in Rn: Schwarz inequality |aTb|≤(cid:107)a(cid:107)(cid:107)b(cid:107). (6) According to formula (2), the ratio between aTb and (cid:107)a(cid:107)(cid:107)b(cid:107) is exactly |cosθ|. Since all cosines lie in the interval −1 ≤ cosθ ≤ 1, this gives another proof of equation (6): the Schwarz inequality is the same as |cosθ| ≤ 1. In some ways that is a more easily understood proof, because cosines are so familiar. Either proof is all right in Rn, but"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.2 CosinesandProjectionsontoLines 175 notice that ours came directly from the calculation of (cid:107)b−p(cid:107)2. This stays nonnegative when we introduce new possibilities for the lengths and inner products. The name of Cauchy is also attached to this inequality |aTb|≤(cid:107)a(cid:107)(cid:107)b(cid:107), and the Russians refer to it as the Cauchy-Schwarz-Buniakowsky inequality! Mathematical historians seem to agree that Buniakowsky’s claim is genuine. One final observation about |aTb| ≤ (cid:107)a(cid:107)(cid:107)b(cid:107). Equality holds if and only if b is a multiple of a. The angle is θ= 0° or θ= 180° and the cosine is 1 or −1. In this case b is identical with its projection p, and the distance between b and the line is zero. Example 1. Project b=(1,2,3) onto the line through a=(1,1,1) to get x(cid:98)and p: aTb 6 x(cid:98)= = =2. aTa 3 The projection is p=x(cid:98)a=(2,2,2). The angle between a and b has √ (cid:107)p(cid:107) 12 aTb 6 cosθ= = √ and also cosθ= = √ √ . (cid:107)b(cid:107) 14 (cid:107)a(cid:107)(cid:107)b(cid:107) 3 14 √ √ √ The Schwarz inequality |aTb|≤(cid:107)a(cid:107)(cid:107)b(cid:107) is 6≤ 3 14. If we write 6 as 36, that is the √ √ same as 36≤ 42. The cosine is less than 1, because b is not parallel to a. Projection Matrix of Rank 1 The projection of b onto the line through a lies at p=a(aTb/aTa). That is our formula p = x(cid:98)a, but it is written with a slight twist: The vector a is put before the number x(cid:98)= aTb/aTa. There is a reason behind that apparently trivial change. Projection onto a line is carried out by a projection matrix P, and written in this new order we can see what it is. P is the matrix that multiplies b and produces p: aTb aaT P=a so the projection matrix is P= . (7) aTa aTa That is a column times a row—a square matrix—divided by the number aTa. Example 2. The matrix that projects onto the line through a=(1,1,1) is     1 1 1 1 (cid:104) (cid:105) aaT 1  3 3 3  P= = 1 1 1 1 =1 1 1 . aTa 3 3 3 3 1 1 1 1 3 3 3 This matrix has two properties that we will see as typical of projections:"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. P is a symmetric matrix."
    },
    {
        "chapter": "Orthogonality",
        "question": "2. Its square is itself: P2 =P. P2b is the projection of Pb—and Pb is already on the line! So P2b=Pb. This matrix P also gives a great example of the four fundamental subspaces: The column space consists of the line through a=(1,1,1). The nullspace consists of the plane perpendicular to a. The rank is r =1. Every column is a multiple of a, and so is Pb = x(cid:98)a. The vectors that project to p = 0 are especially important. They satisfy aTb = 0—they are perpendicular to a and their component along the line is zero. They lie in the nullspace = perpendicular plane. Actually that example is too perfect. It has the nullspace orthogonal to the column space, which is haywire. The nullspace should be orthogonal to the row space. But because P is symmetric, its row and column spaces are the same. Remark on scaling The projection matrix aaT/aTa is the same if a is doubled:       2 2 1 1 1 (cid:104) (cid:105)   1   3 3 3  a=2 gives P= 2 2 2 2 =1 1 1  as before. 12 3 3 3 2 2 1 1 1 3 3 3 The line through a is the same, and that’s all the projection matrix cares about. If a has unit length, the denominator is aTa=1 and the matrix is just P=aaT. Example 3. Project onto the “θ-direction” in the x-y plane. The line goes through a=(cosθ,sinθ) and the matrix is symmetric with P2 =P: (cid:34) (cid:35) (cid:104) (cid:105) c c s (cid:34) (cid:35) aaT s c2 cs P= = (cid:34) (cid:35) = . aTa (cid:104) (cid:105) cs s2 c c s s Here c is cosθ, s is sinθ, and c2+s2 = 1 in the denominator. This matrix P was dis- covered in Section 2.6 on linear transformations. Now we know P in any number of dimensions. We emphasize that it produces the projection p: To project b onto a, multiply by the projection matrix P: p=Pb. Transposes from Inner Products Finally we connect inner products to AT. Up to now, AT is simply the reflection of A across its main diagonal; the rows of A become the columns of AT, and vice versa. The entry in row i, column j of AT is the (j,i) entry of A: Transpose by reflection AT =(A) . ij ji There is a deeper significance to AT, Its close connection to inner products gives a new and much more “abstract” definition of the transpose:"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. (a) Given any two positive numbers x and y, choose the vector b equal to ( x, y), √ √ and choose a = ( y, x). Apply the Schwarz inequality to compare the arith- √ metic mean 1(x+y) with the geometric mean xy. 2 (b) Suppose we start with a vector from the origin to the point x, and then add a vector of length (cid:107)y(cid:107) connecting x to x+y. The third side of the triangle goes from the origin to x+y. The triangle inequality asserts that this distance cannot be greater than the sum of the first two: (cid:107)x+y(cid:107)≤(cid:107)x(cid:107)+(cid:107)y(cid:107). After squaring both sides, and expanding (x+y)T(x+y), reduce this to the Schwarz inequality."
    },
    {
        "chapter": "Orthogonality",
        "question": "2. VerifythatthelengthoftheprojectioninFigure3.7is(cid:107)p(cid:107)=(cid:107)b(cid:107)cosθ,usingformula (5)."
    },
    {
        "chapter": "Orthogonality",
        "question": "3. What multiple of a=(1,1,1) is closest to the point b=(2,4,4)? Find also the point closest to a on the line through b."
    },
    {
        "chapter": "Orthogonality",
        "question": "4. Explain why the Schwarz inequality becomes an equality in the case that a and b lie on the same line through the origin, and only in that case. What if they lie on opposite sides of the origin?"
    },
    {
        "chapter": "Orthogonality",
        "question": "5. In n dimensions, what angle does the vector (1,1,...,1) make with the coordinate axes? What is the projection matrix P onto that vector?"
    },
    {
        "chapter": "Orthogonality",
        "question": "6. The Schwarz inequality has a one-line proof if a and b are normalized ahead of time to be unit vectors: (cid:175) (cid:175) |a |2+|b |2 1 1 |aTb|=(cid:175)∑a b (cid:175) ≤∑|a ||b |≤∑ j j = + =(cid:107)a(cid:107)(cid:107)b(cid:107). j j j j 2 2 2 Which previous problem justifies the middle step?"
    },
    {
        "chapter": "Orthogonality",
        "question": "7. By choosing the correct vector b in the Schwarz inequality, prove that (a +···+a )2 ≤n(a2+···+a2). 1 n 1 n When does equality hold?"
    },
    {
        "chapter": "Orthogonality",
        "question": "8. The methane molecule CH is arranged as if the carbon atom were at the center of a 4 regular tetrahedron with four hydrogen atoms at the vertices. If vertices are placed √ at (0,0,0), (1,1,0), (1,0,1), and (0,1,1)—note that all six edges have length 2, so the tetrahedron is regular—what is the cosine of the angle between the rays going from the center (1,1,1) to the vertices? (The bond angle itself is about 109.5°, an 2 2 2 old friend of chemists.)"
    },
    {
        "chapter": "Orthogonality",
        "question": "9. Square the matrix P = aaT/aTa, which projects onto a line, and show that P2 = P. (Note the number aTa in the middle of the matrix aaTaaT!)"
    },
    {
        "chapter": "Orthogonality",
        "question": "10. Is the projection matrix P invertible? Why or why not?"
    },
    {
        "chapter": "Orthogonality",
        "question": "11. (a) FindtheprojectionmatrixP ontothelinethrougha=[1]andalsothematrixP 1 3 2 that projects onto the line perpendicular to a. (b) Compute P +P and P P and explain. 1 2 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "12. Find the matrix that projects every point in the plane onto the line x+2y=0."
    },
    {
        "chapter": "Orthogonality",
        "question": "13. Prove that the trace of P = aaT/aTa—which is the sum of its diagonal entries— always equals 1."
    },
    {
        "chapter": "Orthogonality",
        "question": "14. What matrix P projects every point in R3 onto the line of intersection of the planes x+y+t =0 and x−t =0?"
    },
    {
        "chapter": "Orthogonality",
        "question": "15. Show that the length of Ax equals the length of ATx if AAT =ATA."
    },
    {
        "chapter": "Orthogonality",
        "question": "16. Suppose P is the projection matrix onto the line through a. (a) Why is the inner product of x with Py equal to the inner product of Px with y? (b) Are the two angles the same? Find their cosines if a = (1,1,−1), x = (2,0,1), y=(2,1,2). (c) Why is the inner product of Px with Py again the same? What is the angle between those two? Problems 17–26 ask for projections onto lines. Also errors e=b−p and matri- ces P."
    },
    {
        "chapter": "Orthogonality",
        "question": "17. Project the vector b onto the line through a. Check that e is perpendicular to a:"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.2 CosinesandProjectionsontoLines 179         1 1 1 −1         (a) b=2 and a=1. (b) b=3 and a=−3. 2 1 1 −1"
    },
    {
        "chapter": "Orthogonality",
        "question": "18. Draw the projection of b onto a and also compute it from p=x(cid:98)a: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) cosθ 1 1 1 (a) b= and a= . (b) b= and a= . sinθ 0 1 −1"
    },
    {
        "chapter": "Orthogonality",
        "question": "19. In Problem 17, find the projection matrix P = aaT/aTa onto the line through each vector a. Verify in both cases that P2 =P. Multiply Pb in each case to compute the projection p."
    },
    {
        "chapter": "Orthogonality",
        "question": "20. Constructtheprojectionmatrices P andP ontothelinesthroughthe a’sinProblem 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "18. Is it true that (P +P )2 =P +P ? This would be true if P P =0. 1 2 1 2 1 2 For Problems 21–26, consult the accompanying figures."
    },
    {
        "chapter": "Orthogonality",
        "question": "21. Compute the projection matrices aaT/aTa onto the lines through a =(−1,2,2) and 1 a = (2,2,−1), Multiply those projection matrices and explain why their product 2 P P is what it is. 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "22. Project b = (1,0,0) onto the lines through a and a in Problem 21 and also onto 1 2 a =(2,−1,2). Add the three projections p +p +p . 3 1 2 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "23. Continuing Problems 21–22, find the projection matrix P onto a =(2,−1,2). Ver- 3 3 ify that P +P +P =I. The basis a , a , a is orthogonal! 1 2 3 1 2 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "24. Project the vector b=(1,1) onto the lines through a =(1,0) and a =(1,2). Draw 1 2 the projections p and p and add p +p . The projections do not add to b because 1 2 1 2 the a’s are not orthogonal."
    },
    {
        "chapter": "Orthogonality",
        "question": "25. In Problem 24, the projection of b onto the plane of a and a will equal b. Find (cid:163) (cid:164) 1 2 P=A(ATA)−1AT for A=[a a ] 1 1 . 1 2 0 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "26. Projecta =(1,0)ontoa =(1,2). Thenprojecttheresultbackontoa . Drawthese 1 2 1 projections and multiply the projection matrices P P : Is this a projection? 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 Projections and Least Squares Uptothispoint,Ax=beitherhasasolutionornot. IfbisnotinthecolumnspaceC(A), the system is inconsistent and Gaussian elimination fails. This failure is almost certain when there are several equations and only one unknown: More equations 2x = b 1 than unknowns— 3x = b 2 no solution? 4x = b . 3 This is solvable when b , b , b are in the ratio 2:3:4. The solution x will exist only if b 1 2 3 is on the same line as the column a=(2,3,4). In spite of their unsolvability, inconsistent equations arise all the time in practice. They have to be solved! One possibility is to determine x from part of the system, and ignore the rest; this is hard to justify if all m equations come from the same source. Rather than expecting no error in some equations and large errors in the others, it is much better to choose the x that minimizes an average error E in the m equations. The most convenient “average” comes from the sum of squares: Squared error E2 =(2x−b )2+(3x−b )2+(4x−b )2. 1 2 3 If there is an exact solution, the minimum error is E = 0. In the more likely case that b is not proportional to a, the graph of E2 will be a parabola. The minimum error is at the lowest point, where the derivative is zero: dE2 (cid:163) (cid:164) =2 (2x−b )2+(3x−b )3+(4x−b )4 =0. 1 2 3 dx Solving for x, the least-squares solution of this model system ax=b is denoted by x(cid:98): 2b +3b +4b aTb 1 2 3 Leastsquares solution x(cid:101)= = . 22+32+42 aTa You recognize aTb in the numerator and aTa in the denominator. The general case is the same. We “solve” ax=b by minimizing E2 =(cid:107)ax−b(cid:107)2 =(a x−b )2+···+(a x−b )2. 1 1 m m The derivative of E2 is zero at the point x(cid:98), if (a x(cid:98)−b )a +···+(a x(cid:98)−b )a =0. 1 1 1 m m m Weareminimizingthedistancefrombtothelinethrougha,andcalculusgivesthesame answer, x(cid:98)=(a b +···+a b )/(a2+···+a2), that geometry did earlier: 1 1 m m 1 m"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 ProjectionsandLeastSquares 181 aTb 3K Theleast-squaressolutiontoaproblemax=binoneunknownisx(cid:98)= . aTa Youseethatwekeepcomingbacktothegeometricalinterpretationofaleast-squares problem—to minimize a distance. By setting the derivative of E2 to zero, calculus con- firms the geometry of the previous section. The error vector e connecting b to p must be perpendicular to a: aTb Orthogonality of a and e aT(b−x(cid:98)a)=aTb− aTa=0. aTa As a side remark, notice the degenerate case a = 0. All multiples of a are zero, and the line is only a point. Therefore p = 0 is the only candidate for the projection. But the formula for x(cid:98)becomes a meaningless 0/0, and correctly reflects the fact that x(cid:98)is completely undetermined. All values of x give the same error E = (cid:107)0x−b(cid:107), so E2 is a horizontal line instead of a parabola. The “pseudoinverse” assigns the definite value x(cid:98)=0, which is a more “symmetric” choice than any other number. Least Squares Problems with Several Variables Now we are ready for the serious step, to project b onto a subspace—rather than just onto a line. This problem arises from Ax = b when A is an m by n matrix. Instead of one column and one unknown x, the matrix now has n columns. The number m of observations is still larger than the number n of unknowns, so it must be expected that Ax = b will be inconsistent. Probably, there will not exist a choice of x that perfectly fits the data b. In other words, the vector b probably will not be a combination of the columns of A; it will be outside the column space. Again the problem is to choose x(cid:98)so as to minimize the error, and again this mini- mization will be done in the least-squares sense. The error is E = (cid:107)Ax−b(cid:107), and this is exactly the distance from b to the point Ax in the column space. Searching for the least-squares solution x(cid:98), which minimizes E, is the same as locating the point p = Ax(cid:98) that is closer to b than any other point in the column space. We may use geometry or calculus to determine x(cid:98). In n dimensions, we prefer the appeal of geometry; p must be the “projection of b onto the column space.” The error vector e = b−Ax(cid:98)must be perpendicular to that space (Figure 3.8). Finding x(cid:98)and the projection p=Ax(cid:98)is so fundamental that we do it in two ways:"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. All vectors perpendicular to the column space lie in the left nullspace. Thus the error vector e=b−Ax(cid:98)must be in the nullspace of AT: AT(b−Ax(cid:98))=0 or ATAx(cid:98)=ATb. Figure3.8: Projectionontothecolumnspaceofa3by2matrix."
    },
    {
        "chapter": "Orthogonality",
        "question": "2. The error vector must be perpendicular to each column a ,...,a of A: 1 n    aT(b−Ax(cid:98))=0 aT 1 1 .  .   . . or  . . b−Ax(cid:98)=0. aT(b−Ax(cid:98))=0 aT n n This is again AT(b−Ax(cid:98)) = 0 and ATAx(cid:98)= ATb, The calculus way is to take partial derivatives of E2 = (Ax−b)T(Ax−b). That gives the same 2ATAx−2ATb = 0. The fastestwayisjusttomultiplytheunsolvableequationAx=bbyAT. Alltheseequivalent methods produce a square coefficient matrix ATA. It is symmetric (its transpose is not AAT!) and it is the fundamental matrix of this chapter. The equations ATAx(cid:98)=ATb are known in statistics as the normal equations. 3L When Ax = b is inconsistent, its least-squares solution minimizes (cid:107)Ax− b(cid:107)2: Normal equations ATAx(cid:98)=ATb. (1) ATA is invertible exactly when the columns of A are linearly independent! Then, Best estimate x(cid:98) x(cid:98)=(ATA)−1ATb. (2) The projection of b onto the column space is the nearest point Ax(cid:98): Projection p=Ax(cid:98)=A(ATA)−1ATb. (3) We choose an example in which our intuition is as good as the formulas:     1 2 4     Ax=b has no solution A=1 3, b=5, ATAx(cid:98)=ATb gives the best x. 0 0 6"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 ProjectionsandLeastSquares 183 Both columns end with a zero, so C(A) is the x-y plane within three-dimensional space The projection of b = (4,5,6) is p = (4,5,0)—the x and y components stay the same but z=6 will disappear. That is confirmed by solving the normal equations:   (cid:34) (cid:35) (cid:34) (cid:35) 1 2 1 1 0   2 5 ATA= 1 3= . 2 3 0 5 13 0 0   (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 4 13 −5 1 1 0   2 x(cid:98)=(ATA)−1ATb= 5= . −5 2 2 3 0 1 6     (cid:34) (cid:35) 1 2 4   2   Projection p=Ax(cid:98)=1 3 =5. 1 0 0 0 Inthisspecialcase,thebestwecandoistosolvethefirsttwoequationsofAx=b. Then x(cid:98) =2 and x(cid:98) =1. The error in the equation 0x +0x =6 is sure to be 6. 1 2 1 2 Remark 4. Suppose b is actually in the column space of A—it is a combination b =Ax of the columns. Then the projection of b is still b: b in column space p=A(ATA)−1ATAx=Ax=b. The closest point p is just b itself—which is obvious. Remark 5. At the other extreme, suppose b is perpendicular to every column, so ATb="
    },
    {
        "chapter": "Orthogonality",
        "question": "0. In this case b projects to the zero vector: b in left nullspace p=A(ATA)−1ATb=A(ATA)−10=0. Remark6. WhenAissquareandinvertible,thecolumnspaceisthewholespace. Every vector projects to itself, p equals b, and x(cid:98)=x: If A is invertible p=A(ATA)−1ATb=AA−1(AT)−1ATb=b. Thisistheonlycasewhenwecantakeapart(ATA)−1,andwriteitasA−1(AT)−1. When A is rectangular that is not possible. Remark 7. Suppose A has only one column, containing a. Then the matrix ATA is the number aTa and x(cid:98)is aTb/aTa. We return to the earlier formula. The Cross-Product Matrix ATA The matrix ATA is certainly symmetric. Its transpose is (ATA)T =ATATT, which is ATA again. Its i, j entry (and j, i entry) is the inner product of column i of A with column j of A. The key question is the invertibility of ATA, and fortunately ATA has the same nullspace as A. Certainly if Ax = 0 then ATAx = 0. Vectors x in the nullspace of A are also in the nullspace of ATA. To go in the other direction, start by supposing that ATAx = 0, and take the inner product with x to show that Ax=0: xTATAx=0, or (cid:107)Ax(cid:107)2 =0, or Ax=0. The two nullspaces are identical. In particular, if A has independent columns (and only x=0 is in its nullspace), then the same is true for ATA: 3M IfAhasindependentcolumns,thenATAissquare,symmetric,andinvert- ible. We show later that ATA is also positive definite (all pivots and eigenvalues are positive). This case is by far the most common and most important. Independence is not so hard in m-dimensional space if m>n. We assume it in what follows. Projection Matrices We have shown that the closest point to b is p=A(ATA)−1ATb. This formula expresses in matrix terms the construction of a perpendicular line from b to the column space of A. The matrix that gives p is a projection matrix, denoted by P: Projection matrix P=A(ATA)−1AT. (4) This matrix projects any vector b onto the column space of A.1 In other words, p = Pb is the component of b in the column space, and the error e = b−Pb is the component in the orthogonal complement. (I−P is also a projection matrix! It projects b onto the orthogonal complement, and the projection is b−Pb.) In short, we have a matrix formula for splitting any b into two perpendicular compo- nents. Pb is in the column space C(A), and the other component (I−P)b is in the left nullspace N(AT)—which is orthogonal to the column space. These projection matrices can be understood geometrically and algebraically. 3N The projection matrix P=A(ATA)−1AT has two basic properties: (i) It equals its square: P2 =P. (ii) It equals its transpose: PT =P. Conversely, any symmetric matrix with P2 =P represents a projection. 1Theremaybeariskofconfusionwithpermutationmatrices,alsodenotedbyP,buttheriskshouldbesmall, andwetrynevertoletbothappearonthesamepage."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 ProjectionsandLeastSquares 185 Proof. It is easy to see why P2 =P. If we start with any b, then Pb lies in the subspace we are projecting onto. When we project again nothing is changed. The vector Pb is already in the subspace, and P(Pb) is still Pb. In other words P2 = P. Two or three or fifty projections give the same point p as the first projection: P2 =A(ATA)−1ATA(ATA)−1AT =A(ATA)−1AT =P. To prove that P is also symmetric, take its transpose. Multiply the transposes in reverse order, and use symmetry of (ATA)−1, to come back to P: (cid:161) (cid:162) PT =(AT)T (ATA)−1 T AT =A(ATA)−1AT =P. Fortheconverse,wehavetodeducefromP2=PandPT=PthatPbistheprojection of b onto the column space of P. The error vector b−Pb is orthogonal to the space. For any vector Pc in the space, the inner product is zero: (b−Pb)TPc=bT(I−P)TPc=bT(P−P2)c=0. Thus b−Pb is orthogonal to the space, and Pb is the projection onto the column space. Example 1. Suppose A is actually invertible. If it is 4 by 4, then its four columns are independent and its column space is all of R4. What is the projection onto the whole space? It is the identity matrix. P=A(ATA)−1AT =AA−1(AT)−1AT =I. (5) The identity matrix is symmetric, I2 =I, and the error b−Ib is zero. The point of all other examples is that what happened in equation (5) is not allowed. To repeat: We cannot invert the separate parts AT and A when those matrices are rectan- gular. It is the square matrix ATA that is invertible. Least-Squares Fitting of Data Suppose we do a series of experiments, and expect the output b to be a linear function of the inputt. We look for a straight line b=C+Dt. For example:"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. At different times we measure the distance to a satellite on its way to Mars. In this case t is the time and b is the distance. Unless the motor was left on or gravity is strong, the satellite should move with nearly constant velocity v: b=b +vt. 0"
    },
    {
        "chapter": "Orthogonality",
        "question": "2. We vary the load on a structure, and measure the movement it produces. In this experimentt is the load and b is the reading from the strain gauge. Unless the load is so great that the material becomes plastic, a linear relation b=C+Dt is normal in the theory of elasticity."
    },
    {
        "chapter": "Orthogonality",
        "question": "3. Thecostofproducingt bookslikethisoneisnearlylinear,b=C+Dt,withediting and typesetting inC and then printing and binding in D. C is the set-up cost and D is the cost for each additional book. How to computeC and D? If there is no experimental error, then two measurements of b will determine the line b =C+Dt. But if there is error, we must be prepared to “average” the experiments and find an optimal line. That line is not to be confused with the line through a on which b was projected in the previous section! In fact, since there are two unknowns C and D to be determined, we now project onto a two-dimensional subspace. A perfect experiment would give a perfectC and D: C + Dt = b 1 1 C + Dt = b 2 2 . (6) . . C + Dt = b . m m This is an overdetermined system, with m equations and only two unknowns. If errors are present, it will have no solution. A has two columns, and x=(C,D):     1 t b 1 (cid:34) (cid:35) 1     1 t 2 C b 2 . .  = . , or Ax=b. (7) . . . .  D  . .  1 t b m m The best solution (C(cid:98) ,D(cid:98)) is the x(cid:98)that minimizes the squared error E2: Minimize E2 =(cid:107)b−Ax(cid:107)2 =(b −C−Dt )2+···+(b −C−Dt )2. 1 1 m m The vector p = Ax(cid:98)is as close as possible to b. Of all straight lines b =C+Dt, we are choosing the one that best fits the data (Figure 3.9). On the graph, the errors are the vertical distances b−C−Dt to the straight line (not perpendicular distances!). It is the vertical distances that are squared, summed, and minimized. Example 2. Three measurements b , b , b are marked on Figure 3.9a: 1 2 3 b=1 at t =−1, b=1 at t =1, b=3 at t =2. Note that the values t = −1,1,2 are not required to be equally spaced. The first step is to write the equations that would hold if a line could go through all three points. Then everyC+Dt would agree exactly with b:     (cid:34) (cid:35) C − D = 1 1 −1 1   C   Ax=b is C + D = 1 or 1 1  =1. D C + 2D = 3 1 2 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 ProjectionsandLeastSquares 187 Figure3.9: Straight-lineapproximationmatchestheprojection pofb. IfthoseequationsAx=bcouldbesolved,therewouldbenoerrors. Theycan’tbesolved because the points are not on a line. Therefore they are solved by least squares: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:98) 3 2 C 5 ATAx(cid:98)=ATb is = . 2 6 D(cid:98) 6 The best solution isC(cid:98) = 9, D(cid:98) = 4 and the best line is 9+4t. 7 7 7 7 Note the beautiful connections between the two figures. The problem is the same but theartshowsitdifferently. InFigure3.9b,bisnotacombinationofthecolumns(1,1,1) and (−1,1,2). In Figure 3.9, the three points are not on a line. Least squares replaces points b that are not on a line by points p that are! Unable to solve Ax = b, we solve Ax(cid:98)= p. Theline 9+4t hasheights 5, 13, 17 atthemeasurementtimes−1, 1, 2. Thosepoints 7 7 7 7 7 do lie on a line. Therefore the vector p=(5,13,17) is in the column space. This vector 7 7 7 is the projection. Figure 3.9b is in three dimensions (or m dimensions if there are m points) and Figure 3.9a is in twodimensions (or n dimensions if there are n parameters). Subtracting p from b, the errors are e = (2,−6,4). Those are the vertical errors in 7 7 7 Figure 3.9a, and they are the components of the dashed vector in Figure 3.9b. This error vector is orthogonal to the first column (1,1,1), since −2 − 6 + 4 = 0. It is orthogonal 7 7 7 to the second column (−1,1,2), because −2−6+8 =0. It is orthogonal to the column 7 7 7 space, and it is in the left nullspace. Question: If the measurements b = (2,−6,4) were those errors, what would be the 7 7 7 best line and the best x(cid:98)? Answer: The zero line—which is the horizontal axis—and x(cid:98)=0. Projection to zero. Wecanquicklysummarizetheequationsforfittingbyastraightline. Thefirstcolumn of A contains 1s, and the second column contains the times t . Therefore ATA contains i the sum of the 1s and thet and thet2: i i 3O The measurements b ,...,b are given at distinct points t ,...,t . Then 1 m 1 m the straight lineC(cid:98) +D(cid:98)t which minimizes E2 comes from least squares: (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:98) (cid:98) C m ∑t C ∑b ATA =ATb or i = i . D(cid:98) ∑t ∑t2 D(cid:98) ∑t b i i i i Remark. The mathematics of least squares is not limited to fitting the data by straight lines. In many experiments there is no reason to expect a linear relationship, and it would be crazy to look for one. Suppose we are handed some radioactive material, The output b will be the reading on a Geiger counter at various times t. We may know that we are holding a mixture of two chemicals, and we may know their half-lives (or rates of decay), but we do not know how much of each is in our hands. If these two unknown amounts are C and D, then the Geiger counter readings would behave like the sum of two exponentials (and not like a straight line): b=Ce−λt+De−µt. (8) In practice, the Geiger counter is not exact. Instead, we make readings b ,...,b at 1 m timest ,...,t , and equation (8) is approximately satisfied: 1 m Ce−λt 1 + De−µt 1 ≈ b 1 . Ax=b is . . Ce−λtm + De−µtm ≈ b . m If there are more than two readings, m>2, then in all likelihood we cannot solve for C and D. But the least-squares principle will give optimal valuesC(cid:98) and D(cid:98). The situation would be completely different if we knew the amounts C and D, and were trying to discover the decay rates λ and µ. This is a problem in nonlinear least squares, and it is harder. We would still form E2, the sum of the squares of the errors, and minimize it. But setting its derivatives to zero will not give linear equations for the optimalλ and µ. In the exercises, we stay with linear least squares. Weighted Least Squares A simple least-squares problem is the estimate x(cid:98)of a patient’s weight from two obser- vations x =b and x =b . Unless b =b , we are faced with an inconsistent system of 1 2 1 2 two equations in one unknown: (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) 1 b 1 x = . 1 b 2 Up to now, we accepted b and b as equally reliable. We looked for the value x(cid:98)that 1 2 minimized E2 =(x−b )2+(x−b )2: 1 2 dE2 b +b 1 2 =0 at x(cid:98)= . dx 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 ProjectionsandLeastSquares 189 The optimal x(cid:98)is the average. The same conclusion comes from ATAx(cid:98)= ATb. In fact ATA is a 1 by 1 matrix, and the normal equation is 2x(cid:98)=b +b . 1 2 Now suppose the two observations are not trusted to the same degree. The value x=b may be obtained from a more accurate scale—or, in a statistical problem, from a 1 larger sample—than x = b . Nevertheless, if b contains some information, we are not 2 2 willing to rely totally on b . The simplest compromise is to attach different weights w2 1 1 and w2, and choose the x(cid:98) that minimizes the weighted sum of squares: 2 W Weighted error E2 =w2(x−b )2+w2(x−b )2. 1 1 2 2 If w >w , more importance is attached to b . The minimizing process (derivative =0) 1 2 1 tries harder to make (x−b )2 small: 1 dE2 (cid:163) (cid:164) w2b +w2b =2 w2(x−b )+w2(x−b ) =0 at x(cid:98) = 1 1 2 2 . (9) dx 1 1 2 2 W w2+w2 1 2 Instead of the average of b and b (for w = w = 1), x(cid:98) is a weighted average of the 1 2 1 2 W data. This average is closer to b than to b . 1 2 The ordinary least-squares problem leading to x(cid:98) comes from changing Ax = b to W the new systemWAx=Wb. This changes the solution from x(cid:98)to x(cid:98) . The matrixWTW W turns up on both sides of the weighted normal equations: The least squares solution toWAx=Wb is x(cid:98) : W Weighted normal equations (ATWTWA)x(cid:98) =ATWTWb. W What happens to the picture of b projected to Ax(cid:98)? The projection Ax(cid:98) is still the W point in the column space that is closest to b. But the word “closest” has a new meaning when the length involvesW. The weighted length of x equals the ordinary length ofWx. PerpendicularitynolongermeansyTx=0;inthenewsystemthetestis(Wy)T(Wx)=0. The matrix WTW appears in the middle. In this new sense, the projection Ax(cid:98) and the W error b−Ax(cid:98) are again perpendicular. W Thatlastparagraphdescribesallinnerproducts: Theycomefrominvertiblematrices W. They involve only the symmetric combination C =WTW. The inner product of x andyisyTCx. ForanorthogonalmatrixW =Q,whenthiscombinationisC=QTQ=I, the inner product is not new or different. Rotating the space leaves the inner product unchanged. Every otherW changes the length and inner product. For any invertible matrixW, these rules define a new inner product and length: Weighted byW (x,y) =(Wy)T(Wx) and (cid:107)x(cid:107) =(cid:107)Wx(cid:107). (10) W W Since W is invertible, no vector is assigned length zero (except the zero vector). All possible inner products—which depend linearly on x and y and are positive when x = y(cid:54)=0—are found in this way, from some matrixC =WTW. In practice, the important question is the choice of C. The best answer comes from statisticians, and originally from Gauss. We may know that the average error is zero. That is the “expected value” of the error in b—although the error is not really expected tobezero! Wemayalsoknowtheaverageofthesquareoftheerror;thatisthevariance. If the errors in the b are independent of each other, and their variances areσ2, then the i i right weights are w = 1/σ. A more accurate measurement, which means a smaller i i variance, gets a heavier weight. In addition to unequal reliability, the observations may not be independent. If the errors are coupled—the polls for President are not independent of those for Senator, and certainly not of those for Vice-President—thenW has off-diagonal terms. The best unbiased matrixC =WTW is the inverse of the covariance matrix—whose i, j entry is the expected value of (error in b ) times (error in b ). Then the main diagonal of C−1 i j contains the variancesσ2, which are the average of (error in b )2. i i Example 3. Suppose two bridge partners both guess (after the bidding) the total num- berofspadestheyhold. Foreachguess, theerrors−1, 0, 1mighthaveequalprobability"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. Findthebestleast-squaressolutionx(cid:98)to3x=10,4x=5. WhaterrorE2isminimized? Check that the error vector (10−3x(cid:98),5−4x(cid:98)) is perpendicular to the column (3,4)."
    },
    {
        "chapter": "Orthogonality",
        "question": "2. Suppose the values b = 1 and b = 7 at times t = 1 and t = 2 are fitted by a line 1 2 1 2 b=Dt through the origin. Solve D=1 and 2D=7 by least squares, and sketch the best line."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 ProjectionsandLeastSquares 191"
    },
    {
        "chapter": "Orthogonality",
        "question": "3. Solve Ax=b by least squares, and find p=Ax(cid:98)if     1 0 1     A=0 1, b=1. 1 1 0 Verify that the error b−p is perpendicular to the columns of A."
    },
    {
        "chapter": "Orthogonality",
        "question": "4. Write out E2 =(cid:107)Ax−b(cid:107)2 and set to zero its derivatives with respect to u and v, if     (cid:34) (cid:35) 1 0 1   u   A=0 1, x= , b=3. v 1 1 4 Compare the resulting equations with ATAx(cid:98)=ATb, confirming that calculus as well as geometry gives the normal equations. Find the solution x(cid:98)and the projection p = Ax(cid:98). Why is p=b?"
    },
    {
        "chapter": "Orthogonality",
        "question": "5. The following system has no solution:     (cid:34) (cid:35) 1 −1 4   C   Ax=1 0  =5=b. D 1 1 9 Sketch and solve a straight-line fit that leads to the minimization of the quadratic (C−D−4)2+(C−5)2+(C+D−9)2? Whatistheprojectionofbontothecolumn space of A?"
    },
    {
        "chapter": "Orthogonality",
        "question": "6. Find the projection of b onto the column space of A:     1 1 1     A= 1 −1, b=2. −2 4 7 Split b into p+q, with p in the column space and q perpendicular to that space. Which of the four subspaces contains q?"
    },
    {
        "chapter": "Orthogonality",
        "question": "7. Find the projection matrix P onto the space spanned by a = (1,0,1) and a = 1 2 (1,1,−1)."
    },
    {
        "chapter": "Orthogonality",
        "question": "8. If P is the projection matrix onto a k-dimensional subspace S of the whole space Rn, what is the column space of P and what is its rank?"
    },
    {
        "chapter": "Orthogonality",
        "question": "9. (a) If P=PTP, show that P is a projection matrix. (b) What subspace does the matrix P=0 project onto?"
    },
    {
        "chapter": "Orthogonality",
        "question": "10. If the vectors a , a , and b are orthogonal, what are ATA and ATb? What is the 1 2 projection of b onto the plane of a and a ? 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "11. Suppose P is the projection matrix onto the subspace S and Q is the projection onto theorthogonalcomplementS⊥. WhatareP+QandPQ? ShowthatP−Qisitsown inverse."
    },
    {
        "chapter": "Orthogonality",
        "question": "12. If V is the subspace spanned by (1,1,0,1) and (0,0,1,0), find (a) a basis for the orthogonal complement V⊥. (b) the projection matrix P onto V. (c) the vector in V closest to the vector b=(0,1,0,−1) in V⊥."
    },
    {
        "chapter": "Orthogonality",
        "question": "13. Find the best straight-line fit (least squares) to the measurements b=4 at t =−2, b=3 at t =−1, b=1 at t =0, b=0 at t =2. Then find the projection of b=(4,3,1,0) onto the column space of   1 −2   1 −1 A= . 1 0  1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "14. The vectors a = (1,1,0) and a = (1,1,1) span a plane in R3. Find the projection 1 2 matrix P onto the plane, and find a nonzero vector b that is projected to zero."
    },
    {
        "chapter": "Orthogonality",
        "question": "15. If P is the projection matrix onto a line in the x-y plane, draw a figure to describe the effect of the “reflection matrix” H = I −2P. Explain both geometrically and algebraically why H2 =I."
    },
    {
        "chapter": "Orthogonality",
        "question": "16. Show that if u has unit length, then the rank-1 matrix P=uuT is a projection matrix: Ithasproperties(i)and(ii)in3N.Bychoosingu=a/(cid:107)a(cid:107),Pbecomestheprojection onto the line through a, and Pb is the point p = x(cid:98)a. Rank-1 projections correspond exactly to least-squares problems in one unknown."
    },
    {
        "chapter": "Orthogonality",
        "question": "17. What 2 by 2 matrix projects the x-y plane onto the −45° line x+y=0?"
    },
    {
        "chapter": "Orthogonality",
        "question": "18. We want to fit a plane y=C+Dt+Ez to the four points y=3 at t =1,z=1 y=6 at t =0,z=3 y=5 at t =2,z=1 y=0 at t =0,z=0. (a) Find 4 equations in 3 unknowns to pass a plane through the points (if there is such a plane). (b) Find 3 equations in 3 unknowns for the best least-squares solution."
    },
    {
        "chapter": "Orthogonality",
        "question": "19. If P = A(ATA)−1AT is the projection onto the column space of A, what is the pro- C jection P onto the row space? (It is not PT!) R C"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 ProjectionsandLeastSquares 193"
    },
    {
        "chapter": "Orthogonality",
        "question": "20. If P is the projection onto the column space of A, what is the projection onto the left nullspace?"
    },
    {
        "chapter": "Orthogonality",
        "question": "21. Suppose L is the line through the origin in the direction of a and L is the line 1 1 2 through b in the direction of a . To find the closest points x a and b+x a on the 2 1 1 2 2 two lines, write the two equations for the x and x that minimize (cid:107)x a −x a −b(cid:107). 1 2 1 1 2 2 Solve for x if a =(1,1,0), a =(0,1,0), b=(2,1,4). 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "22. Find the best lineC+Dt to fit b=4,2,−1,0,0 at timest =−2,−1,0,1,2."
    },
    {
        "chapter": "Orthogonality",
        "question": "23. Showthatthebestleast-squaresfittoasetofmeasurementsy ,...,y byahorizontal 1 m line (a constant function y=C) is their average y +···+y 1 m C = . m"
    },
    {
        "chapter": "Orthogonality",
        "question": "24. Find the best straight-line fit to the following measurements, and sketch your solu- tion: y=2 at t =−1, y=0 at t =0, y=−3 at t =1, y=−5 at t =2."
    },
    {
        "chapter": "Orthogonality",
        "question": "25. Suppose that instead of a straight line, we fit the data in Problem 24 by a parabola: y =C+Dt+Et2. In the inconsistent system Ax = b that comes from the four mea- surements, what are the coefficient matrix A, the unknown vector x, and the data vector b? You need not compute x(cid:98)."
    },
    {
        "chapter": "Orthogonality",
        "question": "26. A Middle-Aged man was stretched on a rack to lengths L = 5, 6, and 7 feet under applied forces of F = 1, 2, and 4 tons. Assuming Hooke’s law L = a+bF, find his normal length a by least squares. Problems 27–31 introduce basic ideas of statistics—the foundation for least squares."
    },
    {
        "chapter": "Orthogonality",
        "question": "27. (Recommended) This problem projects b = (b ,...,b ) onto the line through a = 1 m (1,...,1). We solve m equations ax=b in 1 unknown (by least squares). (a) Solve aTax(cid:98)=aTb to show that is the mean (the average) of the b’s, (b) Find e=b−ax(cid:98), the variance (cid:107)e(cid:107)2, and the standard deviation (cid:107)e(cid:107). (cid:98) (c) The horizontal line b = 3 is closest to b = (1,2,6), Check that p = (3,3,3) is perpendicular to e and find the projection matrix P."
    },
    {
        "chapter": "Orthogonality",
        "question": "28. Firstassumptionbehindleastsquares: Eachmeasurementerrorhasmeanzero. Mul- tiply the 8 error vectors b−Ax = (±1,±1,±1) by (ATA)−1AT to show that the 8 vectors x(cid:98)−x also average to zero. The estimate x(cid:98)is unbiased."
    },
    {
        "chapter": "Orthogonality",
        "question": "29. Second assumption behind least squares: The m errors e are independent with i variance σ2, so the average of (b−Ax)(b−Ax)T is σ2I. Multiply on the left by (ATA)−1AT andontherightby A(ATA)−1 toshowthattheaverageof (x(cid:98)−x)(x(cid:98)−x)T isσ2(ATA)−1. This is the all-important covariance matrix for the error in x(cid:98)."
    },
    {
        "chapter": "Orthogonality",
        "question": "30. Adoctortakesfourreadingsofyourheartrate. Thebestsolutiontox=b ,...,x=b 1 4 is the average x(cid:98)of b ,...,b . The matrix A is a column of 1s. Problem 29 gives the 1 4 expectederror(x(cid:98)−x)2 asσ2(ATA)−1= . Byaveraging,thevariancedropsfrom σ2 toσ2/4."
    },
    {
        "chapter": "Orthogonality",
        "question": "31. If you know the average x(cid:98) of 9 numbers b ,...,b , how can you quickly find the 9 1 9 average x(cid:98) with one more number b ? The idea of recursive least squares is to 10 10 avoid adding 10 numbers. What coefficient of x(cid:98) correctly gives x(cid:98) ? 9 10 x(cid:98) = 1 (cid:98) b + x(cid:98) = 1 (b +···+b ). 10 10 10 9 10 1 10 Problems 32–37 use four points b=(0,8,8,20) to bring out more ideas."
    },
    {
        "chapter": "Orthogonality",
        "question": "32. Withb=0,8,8,20att =0,1,3,4,setupandsolvethenormalequationsATAx(cid:98)=ATb. For the best straight line as in Figure 3.9a, find its four heights p and four errors e . i i What is the minimum value E2 =e2+e2+e2+e2? 1 2 3 4"
    },
    {
        "chapter": "Orthogonality",
        "question": "33. (Line C+Dt does go through p’s) With b = 0,8,8,20 at times t = 0,1,3,4, write the four equations Ax=b (unsolvable). Change the measurements to p=1,5,13,17 and find an exact solution to Ax(cid:98)= p."
    },
    {
        "chapter": "Orthogonality",
        "question": "34. Check that e = b−p = (−1,3,−5,3) is perpendicular to both columns of A. What is the shortest distance (cid:107)e(cid:107) from b to the column space of A?"
    },
    {
        "chapter": "Orthogonality",
        "question": "35. For the closest parabola b=C+Dt+Et2 to the same four points, write the unsolv- able equations Ax = b in three unknowns x = (C,D,E). Set up the three normal equationsATAx(cid:98)=ATb(solutionnotrequired). Youarenowfittingaparabolatofour points—what is happening in Figure 3.9b?"
    },
    {
        "chapter": "Orthogonality",
        "question": "36. For the closest cubic b =C+Dt+Et2+Ft3 to the same four points, write the four equations Ax = b. Solve them by elimination, This cubic now goes exactly through the points. What are p and e?"
    },
    {
        "chapter": "Orthogonality",
        "question": "37. Theaverageofthefourtimesis(cid:98)t = 1(0+1+3+4)=2. Theaverageofthefourb’s 4 is(cid:98) b= 1(0+8+8+20)=9. 4 (a) Verify that the best line goes through the center point ((cid:98)t,(cid:98) b)=(2,9). (b) Explain whyC+D(cid:98)t =(cid:98) b comes from the first equation in ATAx(cid:98)=ATb."
    },
    {
        "chapter": "Orthogonality",
        "question": "38. What happens to the weighted average x(cid:98) = (w2b +w2b )/(w2+w2) if the first W 1 1 2 2 1 2 weight w approaches zero? The measurement b is totally unreliable. 1 1"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 OrthogonalBasesandGram-Schmidt 195"
    },
    {
        "chapter": "Orthogonality",
        "question": "39. Frommindependentmeasurementsb ,...,b ofyourpulserate,weightedbyw ,...,w , 1 m 1 m what is the weighted average that replaces equation (9)? It is the best estimate when the statistical variances areσ2 ≡1/w2. i i (cid:163) (cid:164)"
    },
    {
        "chapter": "Orthogonality",
        "question": "40. IfW = 2 0 , find theW-inner product of x=(2,3) and y=(1,1), and theW-length 0 1 of x. What line of vectors isW-perpendicular to y?"
    },
    {
        "chapter": "Orthogonality",
        "question": "41. Find the weighted least-squares solution x(cid:98) to Ax=b: W       1 0 0 2 0 0       A=1 1 b=1 W =0 1 0. 1 2 1 0 0 1 Check that the projection Ax(cid:98) is still perpendicular (in theW-inner product!) to the W error b−Ax(cid:98) . W"
    },
    {
        "chapter": "Orthogonality",
        "question": "42. (a) Supposeyou guess yourprofessor’sage, making errors e=−2,−1,5 withprob- abilities 1,1,1. Check that the expected error E(e) is zero and find the variance 2 4 4 E(e2). (b) If the professor guesses too (or tries to remember), making errors −1, 0, 1 with probabilities 1,6,1, what weights w and w give the reliability of your guess 8 8 8 1 2 and the professor’s guess?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 Orthogonal Bases and Gram-Schmidt In an orthogonal basis, every vector is perpendicular to every other vector. The coor- dinate axes are mutually orthogonal. That is just about optimal, and the one possible improvement is easy: Divide each vector by its length, to make it a unit vector. That changes an orthogonal basis into an orthonormal basis of q’s: 3P The vectors q ,...,q are orthonormal if 1 n (cid:189) 0 whenever i(cid:54)= j, giving the orthogonality; qTq = i j 1 whenever i= j, giving the normalization. A matrix with orthonormal columns will be called Q. The most important example is the standard basis. For the x-y plane, the best-known axes e =(1,0) and e =(0,1) are not only perpendicular but horizontal and vertical. Q 1 2 is the 2 by 2 identity matrix. In n dimensions the standard basis e ,...,e again consists 1 n of the columns of Q=I:       1 0 0       0 1 0 Standard       e =0 , e =0 , ···, e =0 . 1   2   n   basis . . . . . . . . . 0 0 1 That is not the only orthonormal basis! We can rotate the axes without changing the right angles at which they meet. These rotation matrices will be examples of Q. If we have a subspace of Rn, the standard vectors e might not lie in that subspace. i But the subspace always has an orthonormal basis, and it can be constructed in a simple wayoutofanybasiswhatsoever. Thisconstruction,whichconvertsaskewedsetofaxes into a perpendicular set, is known as Gram-Schmidt orthogonalization. To summarize, the three topics basic to this section are:"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. The definition and properties of orthogonal matrices Q."
    },
    {
        "chapter": "Orthogonality",
        "question": "2. The solution of Qx=b, either n by n or rectangular (least squares)."
    },
    {
        "chapter": "Orthogonality",
        "question": "3. The Gram-Schmidt process and its interpretation as a new factorization A=QR. Orthogonal Matrices 3Q If Q (square or rectangular) has orthonormal columns, then QTQ=I:     — qT —   1 0 · 0 1 Orthonormal  — qT —  | | |   0 1 · 0  2 columns   . . .  q 1 q 2 ··· q n= · · · · =I. | | | — qT — 0 0 · 1 n (1) An orthogonal matrix is a square matrix with orthonormal columns.2 Then QT is Q−1. For square orthogonal matrices, the transpose is the inverse. When row i of QT multiplies column j of Q, the result is qTq = 0. On the diagonal j j where i= j, we have qTq =1. That is the normalization to unit vectors of length 1. i i Note that QTQ=I even if Q is rectangular. But then QT is only a left-inverse. Example 1. (cid:34) (cid:35) (cid:34) (cid:35) cosθ −sinθ cosθ sinθ Q= , QT =Q−1 = . sinθ cosθ −sinθ cosθ 2Orthonormalmatrixwouldhavebeenabettername,butitistoolatetochange.Also,thereisnoacceptedword forarectangularmatrixwithorthonormalcolumns. WestillwriteQ,butwewon’tcallitan“orthogonalmatrix” unlessitissquare."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 OrthogonalBasesandGram-Schmidt 197 Q rotates every vector through the angle θ, and QT rotates it back through −θ. The columns are clearly orthogonal, and they are orthonormal because sin2θ+cos2θ = 1. The matrix QT is just as much an orthogonal matrix as Q. Example 2. Any permutation matrix P is an orthogonal matrix. The columns are cer- tainly unit vectors and certainly orthogonal—because the 1 appears in a different place in each column: The transpose is the inverse.     0 1 0 0 0 1     If P=0 0 1 then P−1 =PT =1 0 0. 1 0 0 0 1 0 Ananti-diagonalP,withP =P =P =I,takesthex-y-zaxesintothez-y-xaxes— 13 22 31 a“right-handed”systemintoa“left-handed”system. Sowewerewrongifwesuggested (cid:163) (cid:164) that every orthogonal Q represents a rotation. A reflection is also allowed. P = 0 1 1 0 reflects every point (x,y) into (y,x), its mirror image across the 45° line. Geometrically, an orthogonal Q is the product of a rotation and a reflection. Theredoes remain oneproperty that is sharedby rotations andreflections, andin fact by every orthogonal matrix. It is not shared by projections, which are not orthogonal or even invertible. Projections reduce the length of a vector, whereas orthogonal matrices have a property that is the most important and most characteristic of all: 3R Multiplication by any Q preserves lengths: Lengths unchanged (cid:107)Qx(cid:107)=(cid:107)x(cid:107) for every vector x. (2) Italsopreservesinnerproductsandangles,since(Qx)T(Qy)=xTQTQy=xTy. The preservation of lengths comes directly from QTQ=I: (cid:107)Qx(cid:107)2 =(cid:107)x(cid:107)2 because (Qx)T(Qx)=xTQTQx=xTx. (3) All inner products and lengths are preserved, when the space is rotated or reflected. WecomenowtothecalculationthatusesthespecialpropertyQT=Q−1. Ifwehavea basis, then any vector is a combination of the basis vectors. This is exceptionally simple for an orthonormal basis, which will be a key idea behind Fourier series. The problem is to find the coefficients of the basis vectors: Write b as a combination b=x q +x q +···+x q . 1 1 2 2 n n To compute x there is a neat trick. Multiply both sides of the equation by qT. On the 1 1 left-hand side is qTb. On the right-hand side all terms disappear (because qTq = 0) 1 1 j except the first term. We are left with qTb=x qTq . 1 1 1 1 Since qTq = 1, we have found x = qTb. Similarly the second coefficient is x = qTb; 1 1 1 1 2 2 that term survives when we multiply by qT. The other terms die of orthogonality. Each 2 piece of b has a simple formula, and recombining the pieces gives back b: Every vector b is equal to (qTb)q +(qTb)q +···+(qTb)q . (4) 1 1 2 2 n n I can’t resist putting this orthonormal basis into a square matrix Q. The vector equa- tionx q +···+x q =bisidenticaltoQx=b. (ThecolumnsofQmultiplythecompo- 1 1 n n nentsofx.) Itssolutionisx=Q−1b. ButsinceQ−1 =QT—thisiswhereorthonormality enters—the solution is also x=QTb:      — qT — qTb 1 1  .    .  x=QTb= . . b= . .  (5) — qT — qTb n n The components of x are the inner products qTb, as in equation (4). i The matrix form also shows what happens when the columns are not orthonormal. Expressingbasacombinationx a +···+x a isthesameassolvingAx=b. Thebasis 1 1 n n vectors go into the columns of A. In that case we need A−1, which takes work. In the orthonormal case we only need QT. Remark 1. The ratio aTb/aTa appeared earlier, when we projected b onto a line. Here a is q , the denominator is 1, and the projection is (qTb)q . Thus we have a new interpre- 1 1 1 tation for formula (4): Every vector b is the sum of its one-dimensional projections onto the lines through the q’s. Sincethoseprojectionsareorthogonal,Pythagorasshouldstillbecorrect. Thesquare of the hypotenuse should still be the sum of squares of the components: (cid:107)b(cid:107)2 =(qTb)2+(qTb)2+···+(qTb)2 which is (cid:107)QTb(cid:107)2. (6) 1 2 n Remark 2. Since QT = Q−1, we also have QQT = I. When Q comes before QT, mul- tiplication takes the inner products of the rows of Q. (For QTQ it was the columns.) Since the result is again the identity matrix, we come to a surprising conclusion: The rows of a square matrix are orthonormal whenever the columns are. The rows point in completely different directions from the columns, and I don’t see geometrically why they are forced to be orthonormal—but they are.  √ √ √  1/ 3 1/ 2 1/ 6 Orthonormal columns  √ √  Q=1/ 3 0 −2/ 6. Orthonormal rows √ √ √ 1/ 3 −1/ 2 1/ 6 Rectangular Matrices with Orthogonal Columns This chapter is about Ax = b, when A is not necessarily square. For Qx = b we now admit the same possibility—there may be more rows than columns. The n orthonormal"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 OrthogonalBasesandGram-Schmidt 199 vectors q in the columns of Q have m>n components. Then Q is an m by n matrix and i we cannot expect to solve Qx=b exactly. We solve it by least squares. If there is any justice, orthonormal columns should make the problem simple. It worked for square matrices, and now it will work for rectangular matrices. The key is to notice that we still have QTQ=I. So QT is still the left-inverse of Q. For least squares that is all we need. The normal equations came from multiplying Ax = b by the transpose matrix, to give ATAx(cid:98)= ATb. Now the normal equations are QTQ = QTb. But QTQ is the identity matrix! Therefore x(cid:98)= QTb, whether Q is square and x(cid:98)is an exact solution, or Q is rectangular and we need least squares. 3S If Q has orthonormal columns, the least-squares problem becomes easy: rectangular system with no solution for most b. Qx = b rectangular system with no solution for most b. QTQx(cid:98) = QTb normal equation for the best x(cid:98)—in which QTQ=I. x(cid:98) = QTb x(cid:98) is qTb. i i p = Qx(cid:98) the projection of b is (qTb)q +···+(qTb)q . 1 1 n n p = QQTb the projection matrix is P=QQT. Thelastformulasarelike p=Ax(cid:98)andP=A(ATA)−1AT. Whenthecolumnsareorthonor- mal, the “cross-product matrix” ATA becomes QTQ = I. The hard part of least squares disappears when vectors are orthonormal. The projections onto the axes are uncoupled, and p is the sum p=(qTb)q +···+(qTb)q . 1 1 n n We emphasize that those projections do not reconstruct b. In the square case m = n, they did. In the rectangular case m > n, they don’t. They give the projection p and not the original vector b—which is all we can expect when there are more equations than unknowns, and the q’s are no longer a basis. The projection matrix is usually A(ATA)−1AT, and here it simplifies to P=Q(QTQ)−1QT or P=QQT. (7) Notice that QTQ is the n by n identity matrix, whereas QQT is an m by m projection P. It is the identity matrix on the columns of Q (P leaves them alone), But QQT is the zero matrix on the orthogonal complement (the nullspace of QT). Example 3. The following case is simple but typical. Suppose we project a point b = (x,y,z) onto the x-y plane. Its projection is p = (x,y,0), and this is the sum of the separate projections onto the x- and y-axes:         1 x 0 0         q =0 and (qTb)q =0; q =1 and (qTb)q =y. 1 1 1 2 2 2 0 0 0 0 The overall projection matrix is       1 0 0 x x       P=q qT+q qT =0 1 0, and Py=y. 1 1 2 2 0 0 0 z 0 Projection onto a plane = sum of projections onto orthonormal q and q . 1 2 Example4. Whenthemeasurementtimesaveragetozero,fittingastraightlineleadsto orthogonalcolumns. Taket =−3,t =0, andt =3. Thentheattempttofity=C+Dt 1 2 3 leads to three equations in two unknowns:     (cid:34) (cid:35) C + Dt = y 1 −3 y 1 1 1   C   C + Dt = y , or 1 0  =y . 2 2 2 D C + Dt = y 1 3 y 3 3 3 The columns (1,1,1) and (−3,0,3) are orthogonal. We can project y separately onto each column, and the best coefficientsC(cid:98) and D(cid:98) can be found separately: (cid:104) (cid:105)(cid:104) (cid:105) (cid:104) (cid:105)(cid:104) (cid:105) T T 1 1 1 y y y −3 0 3 y y y 1 2 3 1 2 3 C(cid:98) = , D(cid:98) = . 12+12+12 (−3)2+02+32 (cid:98) (cid:98) Notice that C = (y +y +y )/3 is the mean of the data. C gives the best fit by a 1 2 3 horizontal line, whereas D(cid:98)t is the best fit by a straight line through the origin. The columns are orthogonal, so the sum of these two separate pieces is the best fit by any straight line whatsoever. The columns are not unit vectors, soC(cid:98) and D(cid:98) have the length squared in the denominator. Orthogonal columns are so much better that it is worth changing to that case. if the average of the observation times is not zero—it is t¯= (t +···+t )/m—then the time 1 m origin can be shifted byt¯. Instead of y=C+Dt we work with y=c+d(t−t¯). The best line is the same! As in the example, we find (cid:104) (cid:105)(cid:104) (cid:105) T 1 ··· 1 y ··· y 1 m y +···+y 1 m c(cid:98)= = 12+12+···+12 m (cid:104) (cid:105)(cid:104) (cid:105) (8) T (t −t¯) ··· (t −t¯) y ··· y 1 m 1 m ∑(t −t¯)y (cid:98) i i d = = . (t −t¯)2+···+(t −t¯)2 ∑(t −t¯)2 1 m i The best c(cid:98)is the mean, and we also get a convenient formula for d(cid:98) . The earlier ATA had the off-diagonal entries ∑t , and shifting the time byt¯made these entries zero. This i shiftisanexampleoftheGram-Schmidtprocess, whichorthogonalizesthesituationin advance. Orthogonal matrices are crucial to numerical linear algebra, because they introduce no instability. While lengths stay the same, roundoff is under control. Orthogonalizing vectors has become an essential technique. Probably it comes second only to elimina- tion. And it leads to a factorization A=QR that is nearly as famous as A=LU."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 OrthogonalBasesandGram-Schmidt 201 The Gram-Schmidt Process Suppose you are given three independent vectors a, b, c. If they are orthonormal, life is easy. To project a vector v onto the first one, you compute (aTv)a. To project the same vector v onto the plane of the first two, you just add (aTv)a+(bTv)b. To project onto the span of a, b, c, you add three projections. All calculations require only the inner products aTv, bTv, and cTv. But to make this true, we are forced to say, “If they are orthonormal.” Now we propose to find a way to make them orthonormal. The method is simple. We are given a, b, c and we want q , q , q . There is no 1 2 3 problem with q : it can go in the direction of a. We divide by the length, so that q = 1 1 a/(cid:107)a(cid:107) is a unit vector. The real problem begins with q —which has to be orthogonal 2 to q . If the second vector b has any component in the direction of q (which is the 1 1 direction of a), that component has to be subtracted: Second vector B=b−(qTb)q and q =B/(cid:107)B(cid:107). (9) 1 1 2 B is orthogonal to q . It is the part of b that goes in a new direction, and not in the a. In 1 Figure 3.10, B is perpendicular to q . It sets the direction for q . 1 2 b q2 B a q1 b Figure3.10: Theq componentofbisremoved;aandBnormalizedtoq andq . i 1 2 At this point q and q are set. The third orthogonal direction starts with c. It will 1 2 not be in the plane of q and q , which is the plane of a and b. However, it may have a 1 2 componentinthatplane,andthathastobesubtracted. (IftheresultisC=0,thissignals that a, b, c were not independent in the first place) What is left is the component C we want, the part that is in a new direction perpendicular to the plane: Third vector C =c−(qTc)q −(qTc)q and q =C/(cid:107)C(cid:107). (10) 1 1 2 2 3 This is the one idea of the whole Gram-Schmidt process, to subtract from every new vector its components in the directions that are already settled. That idea is used over and over again.3 When there is a fourth vector, we subtract away its components in the directions of q , q , q . 1 2 3 3IfGramthoughtofitfirst,whatwasleftforSchmidt? Example 5. Gram-Schmidt Suppose the independent vectors are a, b, c:       1 1 2       a=0, b=0, c=1. 1 0 0 √ To find q , make the first vector into a unit vector: q =a/ 2. To find q , subtract from 1 1 2 the second vector its component in the first direction:    √    1 1/ 2 1   1   1  B=b−(qTb)q =0−√  0 =  0 . 1 1 2 √ 2 0 1/ 2 −1 The normalized q is B divided by its length, to produce a unit vector: 2  √  1/ 2   q = 0 . 2 √ −1/ 2 To find q , subtract from c its components along q and q : 3 1 2 C =c−(qTc)q −(qTc)q   1 1  √2  2  √    2 1/ 2 1/ 2 0 √ √         =1− 2 0 − 2 0 =1. √ √ 0 1/ 2 −1/ 2 0 Thisisalreadyaunitvector, soitisq . Iwenttodesperatelengthstocutdownthenum- 3 berofsquareroots(thepainfulpartofGram-Schmidt). Theresultisasetoforthonormal vectors q , q , q , which go into the columns of an orthogonal matrix Q: 1 2 3    √ √  1/ 2 1/ 2 0     Orthonormal basis Q=q q q = 0 0 1. 1 2 3 √ √ 1/ 2 −1/ 2 0 3T The Gram-Schmidt process starts with independent vectors a ,...,a and 1 n ends with orthonormal vectors q ,...,q . At step j it subtracts from a its 1 n j components in the directions q ,...,q that are already settled: 1 j−1 A =a −(qTa )q −···−(qT a )q . (11) j j 1 j 1 j−1 j j−1 Then q is the unit vector A /(cid:107)A (cid:107). j j j Remark on the calculations I think it is easier to compute the orthogonal a, B, C, withoutforcingtheirlengthstoequalone. Thensquarerootsenteronlyattheend,when"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 OrthogonalBasesandGram-Schmidt 203 dividing by those lengths. The example above would have the same B and C, without using square roots. Notice the 1 from aTb/aTa instead of √1 from qTb: 2 2           1 1 2 1 1   1       2  B=0− 0 and then C =1−0−2 0 . 2 0 1 0 1 −1 2 The Factorization A=QR We started with a matrix A, whose columns were a, b, c. We ended with a matrix Q, whosecolumnsareq ,q ,q . Whatistherelationbetweenthosematrices? Thematrices 1 2 3 A and Q are m by n when the n vectors are in m-dimensional space, and there has to be a third matrix that connects them. The idea is to write the a’s as combinations of the q’s. The vector b in Figure 3.10 is a combination of the orthonormal q and q , and we know what combination it is: 1 2 b=(qTb)q +(qTb)q . 1 1 2 2 Every vector in the plane is the sum of its q and q components. Similarly c is the sum 1 2 of its q , q , q components: c = (qTc)q +(qTc)q +(qTc)q . If we express that in 1 2 3 1 1 2 2 3 3 matrix form we have the new factorization A=QR:      qTa qTb qTc 1 1 1      QR factors A=a b c=q q q  qTb qTc=QR (12) 1 2 3 2 2 qTc 3 Notice the zeros in the last matrix! R is upper triangular because of the way Gram- Schmidt was done. The first vectors a and q fell on the same line. Then q , q were in 1 1 2 the same plane as a, b. The third vectors c and q were not involved until step 3. 3 The QR factorization is like A = LU, except that the first factor Q has orthonormal columns. The second factor is called R, because the nonzeros are to the right of the di- agonal (and the letterU is already taken). The off-diagonal entries of R are the numbers √ √ qTb=1/ 2 and qTc=qTc= 2, found above. The whole factorization is 1 1 2    √ √ √ √ √  1 1 2 1/ 2 1/ 2 0 2 1/ 2 2 √ √      A=0 0 1= 0 0 1 1/ 2 2=QR. √ √ 1 0 0 1/ 2 −1/ 2 0 1 You see the lengths of a, B,C on the diagonal of R. The orthonormal vectors q , q , q , 1 2 3 which are the whole object of orthogonalization, are in the first factor Q. Maybe QR is not as beautiful as LU (because of the square roots). Both factoriza- tions are vitally important to the theory of linear algebra, and absolutely central to the calculations. If LU is Hertz, then QR is Avis. The entries r =qTa appear in formula (11), when (cid:107)A (cid:107)q is substituted for A : ij i j j j j a =(qTa )q +···+(qT a )q +(cid:107)A (cid:107)q =Q times column j of R. (13) j 1 j 1 j−1 j j−1 j j 3U Every m by n matrix with independent columns can be factored into A = QR. The columns of Q are orthonormal, and R is upper triangular and invertible. When m=n and all matrices are square, Q becomes an orthogonal matrix. I must not forget the main point of orthogonalization. It simplifies the least-squares problem Ax=b. The normal equations are still correct, but ATA becomes easier: ATA=RTQTQR=RTR. (14) The fundamental equation ATAx(cid:98)=ATb simplifies to a triangular system: RTRx(cid:98)=RTQTb or Rx(cid:98)=QTb. (15) Instead of solving QRx = b, which can’t be done, we solve Rx(cid:98)= QTb which is just back-substitution because R is triangular. The real cost is the mn2 operations of Gram- Schmidt, which are needed to find Q and R in the first place. The same idea of orthogonality applies to functions, The sines and cosines are or- thogonal;thepowers1,x,x2 arenot. When f(x)iswrittenasacombinationofsinesand cosines, that is a Fourier series. Each term is a projection onto a line—the line in func- tion space containing multiples of cosnx or sinnx. It is completely parallel to the vector case, and very important. And finally we have a job for Schmidt: To orthogonalize the powers of x and produce the Legendre polynomials. Function Spaces and Fourier Series This is a brief and optional section, but it has a number of good intentions:"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. to introduce the most famous infinite-dimensional vector space (Hilbert space);"
    },
    {
        "chapter": "Orthogonality",
        "question": "2. to extend the ideas of length and inner product from vectors v to functions f(x):"
    },
    {
        "chapter": "Orthogonality",
        "question": "3. torecognizetheFourierseriesasasumofone-dimensionalprojections(theorthog- onal “columns” are the sines and cosines);"
    },
    {
        "chapter": "Orthogonality",
        "question": "4. to apply Gram-Schmidt orthogonalization to the polynomials 1,x,x2,...; and"
    },
    {
        "chapter": "Orthogonality",
        "question": "5. to find the best approximation to f(x) by a straight line. We will try to follow this outline, which opens up a range of new applications for linear algebra, in a systematic way."
    },
    {
        "chapter": "Orthogonality",
        "question": "1. Hilbert Space. After studying Rn, it is natural to think of the space R∞. It con- tains all vectors v=(v ,v ,v ,...) with an infinite sequence of components. This space 1 2 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 OrthogonalBasesandGram-Schmidt 205 is actually too big when there is no control on the size of components v . A much better j idea is to keep the familiar definition of length, using a sum of squares, and to include only those vectors that have a finite length: Length squared (cid:107)v(cid:107)2 =v2+v2+v2+··· (16) 1 2 3 Theinfiniteseriesmustconvergetoafinitesum. Thisleaves(1,1,1,...)butnot(1,1,1,...). 2 3 Vectorswithfinitelengthcanbeadded((cid:107)v+w(cid:107)≤(cid:107)v(cid:107)+(cid:107)w(cid:107))andmultipliedbyscalars, so they form a vector space. It is the celebrated Hilbert space. Hilbert space is the natural way to let the number of dimensions become infinite, andatthesametimetokeepthegeometryofordinaryEuclideanspace. Ellipsesbecome infinite-dimensionalellipsoids,andperpendicularlinesarerecognizedexactlyasbefore. The vectors v and w are orthogonal when their inner product is zero: Orthogonality vTw=v w +v w +v w +···=0. 1 1 2 2 3 3 This sum is guaranteed to converge, and for any two vectors it still obeys the Schwarz inequality |vTw|≤(cid:107)v(cid:107)(cid:107)w(cid:107). The cosine, even in Hilbert space, is never larger than 1. There is another remarkable thing about this space: It is found under a great many different disguises. Its “vectors” can turn into functions, which is the second point."
    },
    {
        "chapter": "Orthogonality",
        "question": "2. LengthsandInnerProducts. Suppose f(x)=sinxontheinterval0≤x≤2π. This f is like a vector with a whole continuum of components, the values of sinx along the whole interval. To find the length of such a vector, the usual rule of adding the squares of the components becomes impossible. This summation is replaced, in a natural and inevitable way, by integration: (cid:90) (cid:90) 2π 2π Length (cid:107)f(cid:107) of function (cid:107)f(cid:107)2 = (f(x))2dx= (sinx)2dx=π (17) 0 0 Our Hilbert space has become a function space. The vectors are functions, we have a way to measure their length, and the space contains all those functions that have a finite length—just as in equation (16). It does not contain the function F(x) = 1/x, because the integral of 1/x2 is infinite. The same idea of replacing summation by integration produces the inner product of two functions: If f(x)=sinx and g(x)=cosx, then their inner product is (cid:90) (cid:90) 2π 2π (f,g)= f(x)g(x)dx= sinxcosxdx=0. (18) 0 0 This is exactly like the vector inner product fTg. It is still related to the length by (f, f) = (cid:107)f(cid:107)2. The Schwarz inequality is still satisfied: |(f,g)| ≤ (cid:107)f(cid:107)(cid:107)g(cid:107). Of course, two functions like sinx and cosx—whose inner product is zero—will be called orthogo- √ nal. They are even orthonormal after division by their length π."
    },
    {
        "chapter": "Orthogonality",
        "question": "3. The Fourier series of a function is an expansion into sines and cosines: f(x)=a +a cosx+b sinx+a cos2x+b sin2x+···. 0 1 1 2 2 To compute a coefficient like b , multiply both sides by the corresponding function sinx 1 and integrate from 0 to 2π. (The function f(x) is given on that interval.) In other words, take the inner product of both sides with sinx: (cid:90) (cid:90) (cid:90) (cid:90) 2π 2π 2π 2π f(x)sinxdx=a sinxdx+a cosxsinxdx+b (sinx)2dx+···. 0 1 1 0 0 0 0 On the right-hand side, every integral is zero except one—the one in which sinx multi- pliesitself. Thesinesandcosinesaremutuallyorthogonalasinequation(18)Therefore b is the left-hand side divided by that one nonzero integral: 1 (cid:82) 2π f(x)sinxdx (f,sinx) b 1 = (cid:82)0 = . 2π (sinx)2dx (sinx,sinx) 0 The Fourier coefficient a would have cosx in place of sinx, and a would use cos2x. 1 2 The whole point is to see the analogy with projections. The component of the vector balongthelinespannedbyaisbTa/aTa. AFourierseriesisprojecting f(x)ontosinx. Its component p in this direction is exactly b sinx. 1 The coefficient b is the least squares solution of the inconsistent equation b sinx = 1 1 f(x). This brings b sinx as close as possible to f(x). All the terms in the series are 1 projectionsontoasineorcosine. Sincethesinesandcosinesareorthogonal, theFourier seriesgivesthecoordinatesofthe“vector” f(x)withrespecttoasetof (infinitelymany) perpendicular axes."
    },
    {
        "chapter": "Orthogonality",
        "question": "4. Gram-Schmidt for Functions. There are plenty of useful functions other than sines and cosines, and they are not always orthogonal. The simplest are the powers of x, and unfortunately there is no interval on which even 1 and x2 are perpendicular. (Their inner product is always positive, because it is the integral of x2.) Therefore the closest parabolato f(x)isnot thesumofitsprojectionsonto1,x,andx2. Therewillbeamatrix like (ATA)−1, and this coupling is given by the ill-conditioned Hilbert matrix. On the interval 0≤x≤1,       (cid:82) (cid:82) (cid:82) (1,1) (1,x) (1,x2) 1 x x2 1 1 1   (cid:82) (cid:82) (cid:82)   2 3  ATA=(x,1) (x,x) (x,x2)= x x2 x3=1 1 1 . (cid:82) (cid:82) (cid:82) 2 3 4 (x2,1) (x2,x) (x2,x2) x2 x3 x4 1 1 1 3 4 5 This matrix has a large inverse, because the axes 1, x, x2 are far from perpendicular. The situation becomes impossible if we add a few more axes. It is virtually hopeless to solve ATAx(cid:98)=ATb for the closest polynomial of degree ten. More precisely, it is hopeless to solve this by Gaussian elimination; every roundoff error would be amplified by more than 1013. On the other hand, we cannot just give"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 OrthogonalBasesandGram-Schmidt 207 up; approximation by polynomials has to be possible. The right idea is to switch to orthogonal axes (by Gram-Schmidt). We look for combinations of 1, x, and x2 that are orthogonal. Itisconvenienttoworkwithasymmetricallyplacedintervallike−1≤x≤1,because this makes all the odd powers of x orthogonal to all the even powers: (cid:90) (cid:90) 1 1 (1,x)= xdx=0, (x,x2)= x3dx=0. −1 −1 Therefore the Gram-Schmidt process can begin by accepting v = 1 and v = x as the 1 2 first two perpendicular axes. Since (x,x2)=0, it only has to correct the angle between 1 and x2. By formula (10), the third orthogonal polynomial is (cid:82) (1,x2) (x,x2) 1 x2dx 1 Orthogonalize v 3 =x2− 1− x=x2− (cid:82)−1 =x2− . (1,1) (x,x) 1 1dx 3 −1 The polynomials constructed in this way are called the Legendre polynomials and they are orthogonal to each other over the interval −1≤x≤1. (cid:181) (cid:182) (cid:181) (cid:182) (cid:183) (cid:184) 1 (cid:90) 1 1 x3 x 1 Check 1,x2− = x2− dx= − =0. 3 3 3 3 −1 −1 The closest polynomial of degree ten is now computable, without disaster, by projecting onto each of the first 10 (or 11) Legendre polynomials."
    },
    {
        "chapter": "Orthogonality",
        "question": "5. Best Straight Line. Suppose we want to approximate y = x5 by a straight line C+Dx between x = 0 and x = 1. There are at least three ways of finding that line, and if you compare them the whole chapter might become clear! (cid:163) (cid:164)"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. Solve [1 x] C =x5 by least squares. The equation ATAx(cid:98)=ATb is D (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (1,1) (1,x) C (1,x5) 1 1 C 1 = or 2 = 6 . (x,1) (x,x) D (x,x5) 1 1 D 1 2 3 17 (cid:82)"
    },
    {
        "chapter": "Orthogonality",
        "question": "2. MinimizeE2 = 1 (x5−C−Dx)2dx= 1 −2C−2D+C2+CD+1D2. Thederiva- 0 11 6 7 3 tives with respect toC and D, after dividing by 2, bring back the normal equations of method 1 (and the solution isC(cid:98) = 1− 5 , D(cid:98) = 5 ): 6 14 17 1 1 1 1 1 − +C+ D=0 and − + C+ D=0. 6 2 7 2 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. (a) Write the four equations for fitting y=C+Dt to the data y=−4 at t =−2, y=−3 at t =−1 y=−1 at t =1, y=0 at t =2. Show that the columns are orthogonal. (b) Find the optimal straight line, draw its graph, and write E2. (c) Interpret the zero error in terms of the original system of four equations in two unknowns: The right-hand side (−4,−3,−1,0) is in the space."
    },
    {
        "chapter": "Orthogonality",
        "question": "2. Project b = (0,3,0) onto each of the orthonormal vectors a = (2,2,−1) and a = 1 3 3 3 2 (−1,2,2), and then find its projection p onto the plane of a and a . 3 3 3 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "3. Find also the projection of b = (0,3,0) onto a = (2,−1,2), and add the three pro- 3 3 3 3 jections. Why is P=a aT+a aT+a aT equal to I? 1 1 2 2 3 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "4. If Q and Q are orthogonal matrices, so that QTQ = I, show that Q Q is also 1 2 1 2 orthogonal. If Q is rotation throughθ, and Q is rotation throughφ, what is Q Q ? 1 2 1 2 Canyoufindthetrigonometricidentitiesforsin(θ+φ)andcos(θ+φ)inthematrix multiplication Q Q ? 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "5. If u is a unit vector, show that Q = I−2uuT is a symmetric orthogonal matrix. (It is a reflection, also known as a Householder transformation.) Compute Q when (cid:163) (cid:164) uT = 1 1 −1 −1 . 2 2 2 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "6. Find a third column so that the matrix  √ √  1/ 3 1/ 14 √ √   Q=1/ 3 2/ 14  √ √ 1/ 3 −3/ 14 is orthogonal. It must be a unit vector that is orthogonal to the other columns; how much freedom does this leave? Verify that the rows automatically become orthonor- mal at the same time."
    },
    {
        "chapter": "Orthogonality",
        "question": "7. Show, by forming bTb directly, that Pythagoras’s law holds for any combination b=x q +···+x q of orthonormal vectors: (cid:107)b(cid:107)2 =x2+···+x2. In matrix terms, 1 1 n n 1 n b=Qx, so this again proves that lengths are preserved: (cid:107)Qx(cid:107)2 =(cid:107)x(cid:107)2."
    },
    {
        "chapter": "Orthogonality",
        "question": "8. Project the vector b = (1,2) onto two vectors that are not orthogonal, a = (1,0) 1 and a = (1,1). Show that, unlike the orthogonal case, the sum of the two one- 2 dimensional projections does not equal b."
    },
    {
        "chapter": "Orthogonality",
        "question": "9. If the vectors q , q , q are orthonormal, what combination of q and q is closest to 1 2 3 1 2 q ? 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 OrthogonalBasesandGram-Schmidt 209"
    },
    {
        "chapter": "Orthogonality",
        "question": "10. Ifq andq aretheoutputsfromGram-Schmidt,whatwerethepossibleinputvectors 1 2 a and b?"
    },
    {
        "chapter": "Orthogonality",
        "question": "11. Show that an orthogonal matrix that is upper triangular must be diagonal. (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "Orthogonality",
        "question": "12. What multiple of a = 1 should be subtracted from a2 = 4 to make the result 1 1(cid:163) (cid:164) 0 orthogonal to a ? Factor 1 4 into QR with orthonormal vectors in Q. 1 1 0"
    },
    {
        "chapter": "Orthogonality",
        "question": "13. Apply the Gram-Schmidt process to       0 0 1       a=0, b=1, c=1 1 1 1 and write the result in the form A=QR."
    },
    {
        "chapter": "Orthogonality",
        "question": "14. From the nonorthogonal a, b, c, find orthonormal vectors q , q , q : 1 2 3       1 1 0       a=1, b=0, c=1. 0 1 1"
    },
    {
        "chapter": "Orthogonality",
        "question": "15. Find an orthonormal set q , q , q for which q , q span the column space of 1 2 3 1 2   1 1   A= 2 −1. −2 4 Which fundamental subspace contains q ? What is the least-squares solution of 3 Ax=b if b=[1 2 7]T?"
    },
    {
        "chapter": "Orthogonality",
        "question": "16. Express the Gram-Schmidt orthogonalization of a , a as A=QR: 1 2     1 1     a =2, a =3. 1 2 2 1 Given n vectors a with m components, what are the shapes of A, Q, and R? i"
    },
    {
        "chapter": "Orthogonality",
        "question": "17. With the same matrix A as in Problem 16, and with b = [1 1 1]T, use A = QR to solve the least-squares problem Ax=b."
    },
    {
        "chapter": "Orthogonality",
        "question": "18. If A =QR, find a simple formula for the projection matrix P onto the column space of A."
    },
    {
        "chapter": "Orthogonality",
        "question": "19. Show that these modified Gram-Schmidt steps produce the same C as in equation (10): C∗ =c−(qTc)q and C =C∗−(qTC∗)q . 1 1 2 2 This is much more stable, to subtract the projections one at a time. √ √ √"
    },
    {
        "chapter": "Orthogonality",
        "question": "20. In Hilbert space, find the length of the vector v = (1/ 2,1/ 4,1/ 8,...) and the length of the function f(x) = ex (over the interval 0 ≤ x ≤ 1). What is the inner product over this interval of ex and e−x?"
    },
    {
        "chapter": "Orthogonality",
        "question": "21. What is the closest function acosx+bsinx to the function f(x) = sin2x on the in- terval from −πtoπ? What is the closest straight line c+dx?"
    },
    {
        "chapter": "Orthogonality",
        "question": "22. By setting the derivative to zero, find the value of b that minimizes 1 (cid:90) 2π (cid:107)b sinx−cosx(cid:107)2 = (b sinx−cosx)2dx. 1 1 0 Compare with the Fourier coefficient b . 1"
    },
    {
        "chapter": "Orthogonality",
        "question": "23. Find the Fourier coefficients a , a , b of the step function y(x), which equals 1 on 0 1 1 the interval 0≤x≤πand 0 on the remaining intervalπ<x<2π: (y,1) (y,cosx) (y,sinx) a = a = b = . 0 1 1 (1,1) (cosx,cosx) (sinx,sinx)"
    },
    {
        "chapter": "Orthogonality",
        "question": "24. FindthefourthLegendrepolynomial. Itisacubicx3+ax2+bx+cthatisorthogonal to 1, x, and x2−1 over the interval −1≤x≤1. 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "25. What is the closest straight line to the parabola y=x2 over −1≤x≤1?"
    },
    {
        "chapter": "Orthogonality",
        "question": "26. In the Gram-Schmidt formula (10), verify thatC is orthogonal to q and q . 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "27. Find an orthonormal basis for the subspace spanned by a = (1,−1,0,0), a = 1 2 (0,1,−1,0), a =(0,0,1,−1). 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "28. Apply Gram-Schmidt to (1,−1,0), (0,1,−1), and (1,0,−1), to find an orthonormal basis on the plane x +x +x =0. What is the dimension of this subspace, and how 1 2 3 many nonzero vectors come out of Gram-Schmidt?"
    },
    {
        "chapter": "Orthogonality",
        "question": "29. (Recommended) Find orthogonal vectors A, B,C by Gram-Schmidt from a, b, c: a=(1,−1,0,0) b=(0,1,−1,0) c=(0,0,1,−1). A, B,C and a, b, c are bases for the vectors perpendicular to d =(1,1,1,1)."
    },
    {
        "chapter": "Orthogonality",
        "question": "30. If A=QR then ATA=RTR= triangular times triangular. Gram-Schmidt on A corresponds to elimination on ATA. Compare     1 0 0   2 −1 0 −1 1 0    A=  with ATA=−1 2 −1.  0 −1 1  0 −1 2 0 0 −1 For ATA, the pivots are 2, 3, 4 and the multipliers are −1 and −2. 2 3 2 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.5 TheFastFourierTransform 211 (a) Using those multipliers in A, show that column 1 of A and B = column 2− 1(column 1) andC =column 3−2(column 2) are orthogonal. 2 3 (b) Check that (cid:107)column 1(cid:107)2 =2, (cid:107)B(cid:107)2 = 3, and (cid:107)C(cid:107)2 = 4, using the pivots. 2 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "31. True or false (give an example in either case): (a) Q−1 is an orthogonal matrix when Q is an orthogonal matrix. (b) If Q (3 by 2) has orthonormal columns then (cid:107)Qx(cid:107) always equals (cid:107)x(cid:107)."
    },
    {
        "chapter": "Orthogonality",
        "question": "32. (a) Find a basis for the subspace S in R4 spanned by all solutions of x +x +x −x =0. 1 2 3 4 (b) Find a basis for the orthogonal complement S⊥. (c) Find b in S and b in S⊥ so that b +b =b=(1,1,1,1). 1 2 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.5 The Fast Fourier Transform The Fourier series is linear algebra in infinite dimensions. The “vectors” are functions f(x);theyareprojectedontothesinesandcosines;thatproducestheFouriercoefficients a and b . From this infinite sequence of sines and cosines, multiplied by a and b , we k k k k canreconstruct f(x). Thatistheclassicalcase,whichFourierdreamtabout,butinactual calculations it is the discrete Fourier transform that we compute. Fourier still lives, but in finite dimensions. This is pure linear algebra, based on orthogonality. The input is a sequence of num- bers y ,...,y , instead of a function f(x). The output c ,...,c has the same length 0 n−1 0 n−1 n. The relation between y and c is linear, so it must be given by a matrix. This is the Fourier matrix F, and the whole technology of digital signal processing depends on it. The Fourier matrix has remarkable properties. Signals are digitized, whether they come from speech or images or sonar or TV (or even oil exploration). The signals are transformed by the matrix F, and later they can be transformed back—to reconstruct. What is crucially important is that F and F−1 can be quick: F−1 must be simple. The multiplications by F and F−1 must be fast. Those are both true. F−1 has been known for years, and it looks just like F. In fact, √ F is symmetric and orthogonal (apart from a factor n), and it has only one drawback: Its entries are complex numbers. That is a small price to pay, and we pay it below. The difficultiesareminimizedbythefactthatallentriesofF andF−1 tarepowersofasingle number w. That number has wn =1. The 4 by 4 discrete Fourier transform uses w = i (and notice i4 =1). The success of the whole DFT depends on F times its complex conjugate F:    1 1 1 1 1 1 1 1    1 i i2 i3 1 (−i) (−i)2 (−i)3  FF =  =4I. (1) 1 i2 i4 i61 (−i)2 (−i)4 (−i)6 1 i3 i6 i9 1 (−i)3 (−i)6 (−i)9 ImmediatelyFF =4I tellsusthatF−1=F/4. ThecolumnsofF areorthogonal(togive the zero entries in 4I). The n by n matrices will have FF =nI. Then the inverse of F is just F/n. In a moment we will look at the complex number w = e2πi/n (which equals i for n=4). It is remarkable that F is so easy to invert. If that were all (and up to 1965 it was all), the discrete transform would have an important place. Now there is more. The multipli- cations by F and F−1 can be done in an extremely fast and ingenious way. Instead of n2 separate multiplications, coming from the n2 entries in the matrix, the matrix-vector products Fc and F−1y require only 1nlogn steps. This rearrangement of the multiplica- 2 tion is called the Fast Fourier Transform. The section begins with w and its properties, moves on to F−1, and ends with the FFT—the fast transform. The great application in signal processing is filtering, and the key to its success is the convolution rule. In matrix language, all “circulant matrices” are diagonalized by F. So they reduce to two FFTs and a diagonal matrix. Complex Roots of Unity Realequationscanhavecomplexsolutions. Theequationx2+1=0ledtotheinvention of i (and also to −i!). That was declared to be a solution, and the case was closed. If someone asked about x2−i = 0, there was an answer: The square roots of a complex number are again complex numbers. You must allow combinations x+iy, with a real part x and an imaginary part y, but no further inventions are necessary. Every real or complex polynomial of degree n has a full set of n roots (possibly complex and possibly repeated). That is the fundamental theorem of algebra. We are interested in equations like x4 =1. That has four solutions—the fourth roots of unity. The two square roots of unity are 1 and −1. The fourth roots are the square roots of the square roots, 1 and −1, i and −i. The number i will satisfy i4 = 1 because it satisfies i2 = −1. For the eighth roots of unity we need the square roots of i, and that √ brings us to w=(1+i)/ 2. Squaring w produces (1+2i+i2)/2, which is i—because 1+i2 is zero. Then w8 =i4 =1. There has to be a system here. The complex numbers cosθ+isinθin the Fourier matrix are extremely special. The realpartisplottedonthex-axisandtheimaginarypartonthey-axis(Figure3.11). Then the number w lies on the unit circle; its distance from the origin is cos2θ+sin2θ= 1."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.5 TheFastFourierTransform 213 It makes an angle θ with the horizontal. The whole plane enters in Chapter 5, where complex numbers will appear as eigenvalues (even of real matrices). Here we need only special points w, all of them on the unit circle, in order to solve wn =1. 2 w = i b 2π/8 2π 2π w = e = cos +isin 3 8 8 w b b 2π w4 = − 1 w8 = 1 b 8 b Real axis b b 5 7 w w = w b w6 = −i √ Figure3.11: Theeightsolutionstoz8=1are1,w,w2,...,w7withw=(1+i)/ 2. The square of w can be found directly (it just doubles the angle): w2 =(cosθ+isinθ)2 =cos2θ−sin2θ+2isinθcosθ. The real part cos2θ−sin2θ is cos2θ, and the imaginary part 2 sinθcosθ is sin2θ. (Note that i is not included; the imaginary part is a real number.) Thus w2 = cos2θ+ isin2θ. The square of w is still on the unit circle, but at the double angle 2θ. That makes us suspect that wn lies at the angle nθ, and we are right. There is a better way to take powers of w. The combination of cosine and sine is a complex exponential, with amplitude one and phase angleθ: cosθ+isinθ=eiθ. (2) Therulesformultiplying,like(e2)(e3)=e5,continuetoholdwhentheexponentsiθare imaginary. The powers of w=eiθ stay on the unit circle: 1 Powers of w w2 =ei2θ, wn =einθ, =e−iθ. (3) w The nth power is at the angle nθ. When n = −1, the reciprocal 1/w has angle −θ. If we multiply cosθ+isinθby cos(−θ)+isin(−θ), we get the answer 1: eiθe−iθ =(cosθ+isinθ)(cosθ−isinθ)=cos2θ+sin2θ=1. Note. I remember the day when a letter came to MIT from a prisoner in New York, asking if Euler’s formula (2) was true. It is really astonishing that three of the key functions of mathematics should come together in such a graceful way. Our best answer was to look at the power series for the exponential: (iθ)2 (iθ)3 eiθ =1+iθ+ + +···. 2! 3! The real part 1−θ2/2+··· is cosθ. The imaginary partθ−θ3/6+··· is the sine, The formula is correct, and I wish we had sent a more beautiful proof. With this formula, we can solve wn = 1. It becomes einθ = 1, so that nθ must carry us around the unit circle and back to the start. The solution is to chooseθ=2π/n: The “primitive” nth root of unity is 2π 2π w =e2πi/n =cos +isin . (4) n n n √ Its nth power is e2πi, which equals 1. For n=8, this root is (1+i)/ 2: π π π π 1+i w =cos +isin =i and w =cos +isin = √ 4 8 2 2 4 4 2 The fourth root is at θ = 90°, which is 1(360°). The other fourth roots are the powers 4 i2 = −1, i3 = −i, and i4 = 1. The other eighth roots are the powers w2,w3,...,w8. The 8 8 8 roots are equally spaced around the unit circle, at intervals of 2π/n. Note again that the square of w is w , which will be essential in the Fast Fourier Transform. The roots add 8 4 up to zero. First 1+i−1−i=0, and then Sum of eighth roots 1+w +w2+···+w7 =0. (5) 8 8 8 One proof is to multiply the left side by w , which leaves it unchanged. (It yields w + 8 8 w2+···+w8 andw8 equals1.) Theeightpointseachmovethrough45°,buttheyremain 8 8 8 the same eight points. Since zero is the only number that is unchanged when multiplied by w , the sum must be zero. When n is even the roots cancel in pairs (like 1+i2 = 0 8 and i+i3 =0). But the three cube roots of 1 also add to zero. The Fourier Matrix and Its Inverse In the continuous case, the Fourier series can reproduce f(x) over a whole interval. It uses infinitely many sines and cosines (or exponentials). In the discrete case, with only n coefficients c ,...,c to choose, we only ask for equality at n points. That gives n 0 n−1 equations. We reproduce the four values y=2,4,6,8 when Fc=y: c + c + c + c = 2 0 1 2 3 c + ic + i2c + i3c = 4 0 1 2 3 Fc=y (6) c + i2c + i4c + i6c = 6 0 1 2 3 c + i3c + i6c + i9c = 8. 0 1 2 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.5 TheFastFourierTransform 215 The input sequence is y=2,4,6,8. The output sequence is c ,c ,c ,c . The four equa- 0 1 2 3 tions(6)lookforafour-termFourierseriesthatmatchestheinputsatfourequallyspaced points x on the interval from 0 to 2π:  2 at x=0 Discrete   4 at x=π/2 Fourier c +c eix+c e2ix+c e3ix = 0 1 2 3 6 at x=π  Series 8 at x=3π/2. Those are the four equations in system (6). At x = 2π the series returns y = 2 and 0 continues periodically. The Discrete Fourier Series is best written in this complex form, as a combination of exponentials eikx rather than sinkx and coskx. For every n, the matrix connecting y to c can be inverted. It represents n equations, requiringthefiniteseriesc +c eix+··· (nterms)toagreewithy(atnpoints). Thefirst 0 1 agreement is at x=0, where c +···+c =y . The remaining points bring powers of 0 n−1 0 w, and the full problem is Fc=y:       1 1 1 · 1 c y 0 0    1 w w2 · wn−1    c 1     y 1       Fc=y  1 w2 w4 · w2(n−1)  c = y . (7)   2 2       · · · · ·  ·   ·  1 wn−1 w2(n−1) · w(n−1)2 c y n−1 n−1 There stands the Fourier matrix F with entries F = wjk. It is natural to number the jk rows and columns from 0 to n−1, instead of 1 to n. The first row has j = 0, the first column has k =0, and all their entries are w0 =1. To find the c’s we have to invert F. In the 4 by 4 case, F−1 was built from 1/i=−i. That is the general rule, that F−1 comes from the complex number w−1 = w. It lies at the angle −2π/n, where w was at the angle +2π/n: 3V The inverse matrix is built from the powers of w−1 =1/w=w:   1 1 1 · 1   1 w−1 w−2 · w−(n−1) 1  F F−1 =  1 w−2 1 · ·  = . (8)   n n   · · · · ·  1 w−(n−1) w−2(n−1) · w−(n−1)2     1 1 1 1 1 1   1  Thus F =1 e2πi/3 e4πi/3 has F−1 = 1 e−2πi/3 e−4πi/3. 3 1 e4πi/3 e8πi/3 1 e−4πi/3 e−8πi/3 Row j of F times column j of F−1 is always (1+1+···+1)/n=1. The harder part is off the diagonal, to show that row j of F times column k of F−1 gives zero: 1·1+wjw−k+w2jw−2k+···+w(n−1)jw−(n−1)k =0 if j (cid:54)=k. (9) The key is to notice that those terms are the powers ofW =wjw−k: 1+W +W2+···+Wn−1 =0. (10) This number W is still a root of unity: Wn = wnjw−nk is equal to 1j1−k = 1. Since j is different from k, W is different from 1. It is one of the other roots on the unit circle. Those roots all satisfy 1+W +···+Wn−1 =0. Another proof comes from 1−Wn =(1−W)(1+W +W2+···+Wn−1). (11) SinceWn = 1, the left side is zero. ButW is not 1, so the last factor must be zero. The columns of F are orthogonal. The Fast Fourier Transform Fourieranalysisisabeautifultheory,anditisalsoverypractical. Toanalyzeawaveform into its frequencies is the best way to take a signal apart. The reverse process brings it back. For physical and mathematical reasons the exponentials are special, and we can pinpoint one central cause: If you differentiate eikx, or integrate it, or translate x to x+h, the result is still a multiple of eikx. Exponentials are exactly suited to differential equations,integralequations,anddifferenceequations. Eachfrequencycomponentgoes its own way, as an eigenvector, and then they recombine into the solution. The analysis andsynthesisofsignals—computingcfromyandyfromc—isacentralpartofscientific computing. We want to show that Fc and F−1y can be done quickly. The key is in the relation of F to F —or rather to two copies of F , which go into a matrix F∗: 4 2 2 2     1 1 1 1 1 1     1 i i2 i3  1 −1  F =  is close to F∗ = . 4 1 i2 i4 i6 2  1 1  1 i3 i6 i9 1 −1 F containsthepowersofw =i,thefourthrootof1. F∗containsthepowersofw =−1, 4 4 2 2 the square root of 1. Note especially that half the entries in F∗ are zero. The 2 by 2 2 transform, done twice, requires only half as much work as a direct 4 by 4 transform. If 64 by 64 transform could be replaced by two 32 by 32 transforms, the work would be cutinhalf(plusthecostofreassemblingtheresults). Whatmakesthistrue,andpossible in practice, is the simple connection between w and w : 64 32 (cid:179) (cid:180) 2 (w )2 =w , or e2πi/64 =e2πi/32. 64 32 The32ndrootistwiceasfararoundthecircleasthe64throot. Ifw64 =1,then(w2)32 ="
    },
    {
        "chapter": "Orthogonality",
        "question": "1. The mth root is the square of the nth root, if m is half of n: w2 =w if m= 1n. (12) n m 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.5 TheFastFourierTransform 217 The speed of the FFT, in the standard form presented here, depends on working with highly composite numbers like 210 =1024. Without the fast transform, it takes (1024)2 multiplications to produce F times c (which we want to do often). By contrast, a fast transformcandoeachmultiplicationinonly5·1024steps. Itis200timesfaster,because it replaces one factor of 1024 by 5. In general it replaces n2 multiplications by 1n(cid:96), 2 when n is 2(cid:96). By connecting F to two copies of F , and then to four copies of F , n n/2 n/4 and eventually to a very small F, the usual n2 steps are reduced to 1nlog n. 2 2 We need to see how y=F c (a vector with n components) can be recovered from two n vectors that are only half as long. The first step is to divide c itself, by separating its even-numbered components from its odd-numbered components: c(cid:48) =(c ,c ,...,c ) and c(cid:48)(cid:48) =(c ,c ,...,c ). 0 2 n−2 1 3 n−1 The coefficients just go alternately into c(cid:48) and c(cid:48)(cid:48). From those vectors, the half-size transform gives y(cid:48) = F c(cid:48) and y(cid:48)(cid:48) = F c(cid:48)(cid:48). Those are the two multiplications by the m m smaller matrix F . The central problem is to recover y from the half-size vectors y(cid:48) and m y(cid:48)(cid:48), and Cooley and Tukey noticed how it could be done: 3W The first m and the last m components of the vector y=F c are n y =y(cid:48) +wjy(cid:48)(cid:48), j =0,...,m−1 j j n j (13) y =y(cid:48) −wjy(cid:48)(cid:48), j =0,...,m−1. j+m j n j Thus the three steps are: split c into c(cid:48) and c(cid:48)(cid:48), transform them by F into y(cid:48) m and y(cid:48)(cid:48), and reconstruct y from equation (13). We verify in a moment that this gives the correct y. (You may prefer the flow graph to the algebra.) This idea can be repeated. We go from F to F to F . The final 1024 512 256 countis 1n(cid:96),whenstartingwiththepowern=2(cid:96) andgoingallthewayton=1—where 2 no multiplication is needed. This number 1n(cid:96) satisfies the rule given above: twice the 4 count for m, plus m extra multiplications, produces the count for n: (cid:181) (cid:182) 1 1 2 m((cid:96)−1) +m= n(cid:96). 2 2 Another way to count: There are (cid:96) steps from n = 2(cid:96) to n = 1. Each step needs n/2 multiplications by D in equation (13), which is really a factorization of F : n/2 n (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) I D F even-odd 512 512 512 One FFT step F = . (14) 1024 I −D F permutation 512 512 512 Thecostisonlyslightlymorethanlinear. Fourieranalysishasbeencompletelytrans- formed by the FFT. To verify equation (13), split y into even and odd: j n−1 m−1 m−1 y = ∑ wjkc is identical to ∑ w2kjc + ∑ w(2k+1)j c . j n k n 2k n 2k+1 k=0 k=0 k=0 Each sum on the right has m= 1n terms. Since w2 is w , the two sums are 2 n m m−1 m−1 y = ∑ wkjc(cid:48) +wj ∑ wkjc(cid:48)(cid:48) =y(cid:48) +wjy(cid:48)(cid:48). (15) j m k n m k j n j k=0 k=0 For the second part of equation (13), j+m in place of j produces a sign change: Inside the sums, wk(j+1) remains wkj since wkm =1k =1. m m m Outside, wj+m =−wj because wm =e2πim/n =eπi =−1. n n n The FFT idea is easily modified to allow other prime factorsof n (not only powersof 2). If n itself is a prime, a completely different algorithm is used. Example 1. The steps from n=4 to m=2 are         c c  0   0  F 2c(cid:48) c  c      1 2  → → →y. c  c  2 1 F c(cid:48)(cid:48) 2 c c 3 3 Combined, the three steps multiply c by F to give y. Since each step is linear, it must 4 come from a matrix, and the product of those matrices must be F : 4       1 1 1 1 1 1 1 1 1       1 i i2 i3   1 i 1 −1  1   =   . (16) 1 i2 i4 i6 1 −1  1 1  1  1 i3 i6 i9 1 −i 1 −1 1 You recognize the two copies of F in the center. At the right is the permutation matrix 2 that separates c into c(cid:48) and c(cid:48)(cid:48). At the left is the matrix that multiplies by wj . If we n started with F , the middle matrix would contain two copies of F . Each of those would 8 4 be split as above. Thus the FFT amounts to a giant factorization of the Fourier matrix! The single matrix F with n2 nonzeros is a product of approximately (cid:96)=log n matrices 2 (and a permutation) with a total of only n(cid:96) nonzeros. The Complete FFT and the Butterfly The first step of the FFT changes multiplication by F to two multiplications by F . n m Theeven-numberedcomponents(c ,c )aretransformedseparatelyfrom(c ,c ),Figure 0 2 1 3"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.12 gives a flow graph for n = 4. For n = 8, the key idea is to replace each F box by 4 F boxes. The new factor w = i is the square of the old factor w = w = e2πi/8. The 2 4 8 flowgraphshowstheorderthatthec’sentertheFFTandthelog nstagesthattakethem 2 through it—and it also shows the simplicity of the logic. Every stage needs 1n multiplications so the final count is 1nlogn. There is an amaz- 2 2 ing rule for the overall permutation of c’s before entering the FFT: Write the subscripts"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. What are F2 and F4 for the 4 by 4 Fourier matrix F?"
    },
    {
        "chapter": "Orthogonality",
        "question": "2. Find a permutation P of the columns of F that produces FP=F (n by n), Combine with FF =nI to find F2 and F4 for the n by n Fourier matrix."
    },
    {
        "chapter": "Orthogonality",
        "question": "3. If you form a 3 by 3 submatrix of the 6 by 6 matrix F , keeping only the entries in 6 its first, third, and fifth rows and columns, what is that submatrix?"
    },
    {
        "chapter": "Orthogonality",
        "question": "4. Mark all the sixth roots of 1 in the complex plane. What is the primitive root w ? 6 (Find its real and imaginary part.) Which power of w is equal to 1/w ? What is 6 6 1+w+w2+w3+w4+w5?"
    },
    {
        "chapter": "Orthogonality",
        "question": "5. Find all solutions to the equation eix =−1, and all solutions to eiθ =i."
    },
    {
        "chapter": "Orthogonality",
        "question": "6. What are the square and the square root of w , the primitive 128th root of 1? 128"
    },
    {
        "chapter": "Orthogonality",
        "question": "7. Solve the 4 by 4 system (6) if the right-hand sides are y =2, y =0, y =2, y =0. 0 1 2 3 In other words, solve F c=y. 4"
    },
    {
        "chapter": "Orthogonality",
        "question": "8. Solve the same system with y = (2,0,−2,0) by knowing F−1 and computing c = 4 F−1y. Verify that c +c eix+c e2ix+c e3ix takes the values 2, 0, −2, 0 at the points 4 0 1 2 3 x=0,π/2,π,3π/2."
    },
    {
        "chapter": "Orthogonality",
        "question": "9. (a) If y=(1,1,1,1), show that c=(1,0,0,0) satisfies F c=y. 4 (b) Now suppose y=(1,0,0,0), and find c."
    },
    {
        "chapter": "Orthogonality",
        "question": "10. For n = 2, write y from the first line of equation (13) and y from the second line. 0 1 For n = 4, use the first line to find y and y , and the second to find y and y , all in 0 1 2 3 terms of y(cid:48) and y(cid:48)(cid:48)."
    },
    {
        "chapter": "Orthogonality",
        "question": "11. Compute y=F c by the three steps of the Fast Fourier Transform if c=(1,0,1,0). 4"
    },
    {
        "chapter": "Orthogonality",
        "question": "12. Computey=F cbythethreestepsoftheFastFourierTransformifc=(1,0,1,0,1,0,1,0). 8 Repeat the computation with c=(0,1,0,1,0,1,0,1)."
    },
    {
        "chapter": "Orthogonality",
        "question": "13. For the 4 by 4 matrix, write out the formulas for c , c , c , c and verify that if f is 0 1 2 3 odd then c is odd. The vector f is odd if f = −f ; for n = 4 that means f = 0, n−j j 0 f =−f , f =0 as in sin0, sinπ/2, sinπ, sin3π/2. This is copied by c and it leads 3 1 2 to a fast sine transform."
    },
    {
        "chapter": "Orthogonality",
        "question": "14. Multiplythethreematricesinequation(16)andcomparewithF. inwhichsixentries do you need to know that i2 =−1?"
    },
    {
        "chapter": "Orthogonality",
        "question": "15. Invert the three factors in equation (14) to find a fast factorization of F−1."
    },
    {
        "chapter": "Orthogonality",
        "question": "16. F is symmetric. So transpose equation (14) to find a new Fast Fourier Transform!"
    },
    {
        "chapter": "Orthogonality",
        "question": "17. All entries in the factorization of F involve powers of w= sixth root of 1: 6 (cid:34) (cid:35)(cid:34) (cid:35) (cid:104) (cid:105) I D F 3 F = P . 6 I −D F 3 Write these factors with 1, w, w2 in D and 1, w2, w4 in F . Multiply! 3 Problems18–20introducetheideaofaneigenvectorandeigenvalue,whenamatrix times a vector is a multiple of that vector. This is the theme of Chapter 5."
    },
    {
        "chapter": "Orthogonality",
        "question": "18. The columns of the Fourier matrix F are the eigenvectors of the cyclic permutation P. Multiply PF to find the eigenvaluesλ toλ : 0 3       0 1 0 0 1 1 1 1 1 1 1 1 λ 0       0 0 1 01 i i2 i3  1 i i2 i3  λ  1   =  . 0 0 0 11 i2 i4 i6 1 i2 i4 i6 λ  2 1 0 0 0 1 i3 i6 i9 1 i3 i6 i9 λ 3 This is PF =FΛ or P=FΛF−1."
    },
    {
        "chapter": "Orthogonality",
        "question": "19. Two eigenvectors of this circulant matrixC are (1,1,1,1) and (1,i,i2,i3). What are the eigenvalues e and e ? 0 1          c c c c 1 1 1 1 0 1 2 3          c c c c 1 1 i i 3 0 1 2   =e   and C =e  . c c c c 1 0 1 i2 1 i2 2 3 0 1 c c c c 1 1 i3 i3 1 2 3 0"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.5 TheFastFourierTransform 221"
    },
    {
        "chapter": "Orthogonality",
        "question": "20. Find the eigenvalues of the “periodic” −1, 2, −1 matrix C. The −1s in the corners ofC make it periodic (a circulant matrix):   2 −1 0 −1   −1 2 −1 0  C =  has c =2, c =−1, c =0, c =−1. 0 1 2 3  0 −1 2 −1 −1 0 −1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "21. TomultiplyC timesx,whenC=FEF−1,wecanmultiplyF(E(F−1x))instead. The direct Cx uses n2 separate multiplications. Knowing E and F, the second way uses only nlog n+n multiplications. How many of those come from E, how many from 2 F, and how many from F−1?"
    },
    {
        "chapter": "Orthogonality",
        "question": "22. How could you quickly compute these four components of Fc starting from c +c , 0 2 c −c , c +c , c −c ? You are finding the Fast Fourier Transform! 0 2 1 3 1 3   c +c +c +c 0 1 2 3   c +ic +i2c +i3c  0 1 2 3 Fc= . c +i2c +i4c +i6c  0 1 2 3 c +i3c +i6c +i9c 0 1 2 3 Review Exercises"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.1 Find the length of a = (2,−2,1), and write two independent vectors that are per- pendicular to a."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.2 Find all vectors that are perpendicular to (1,3,1) and (2,7,2), by making those the rows of A and solving Ax=0."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.3 What is the angle between a=(2,−2,1) and b=(1,2,2)?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.4 What is the projection p of b=(1,2,2) onto a=(2,−2,1)?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.5 Find the cosine of the angle between the vectors (3,4) and (4,3),"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.6 Where is the projection of b = (1,1,1) onto the plane spanned by (1,0,0) and (1,1,0)?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.7 The system Ax=b has a solution if and only if b is orthogonal to which of the four fundamental subspaces?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.8 Which straight line gives the best fit to the following data: b = 0 at t = 0, b = 0 at t =1, b=12 att =3?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.9 Construct the projection matrix P onto the space spanned by (1,1,1) and (0,1,3)."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.10 Which constant function is closest to y = x4 (in the least-squares sense) over the interval 0≤x≤1?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.11 If Q is orthogonal, is the same true of Q3?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.12 Find all 3 by 3 orthogonal matrices whose entries are zeros and ones."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.13 What multiple of a should be subtracted from a , to make the result orthogonal to 1 2 a ? Sketch a figure. 1"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.14 Factor (cid:34) (cid:35) cosθ sinθ sinθ 0 into QR, recognizing that the first column is already a unit vector."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.15 If every entry in an orthogonal matrix is either 1 or −1, how big is the matrix? 4 4"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.16 Suppose the vectors q ,...,q are orthonormal. If b = c q +···+c q , give a 1 n 1 1 n n formula for the first coefficient c in terms of b and the q’s. 1"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.17 What words describe the equation ATAx(cid:98)= ATb, the vector p = Ax(cid:98)= Pb, and the matrix P=A(ATA)−1AT?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.18 If the orthonormal vectors q = (2,2,−1) and q = (−1,2,2) are the columns of 1 3 3 3 2 3 3 3 Q, what are the matrices QTQ and QQT? Show that QQT is a projection matrix (onto the plane of q and q ). 1 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.19 If v ,...,v is an orthonormal basis for Rn, show that v vT+···+v vT =I. 1 n 1 1 n n"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.20 True or false: If the vectors x and y are orthogonal, and P is a projection, then Px and Py are orthogonal."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.21 Trytofitalineb=C+Dt throughthepointsb=0,t =2,andb=6,t =2,andshow that the normal equations break down. Sketch all the optimal lines, minimizing the sum of squares of the two errors."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.22 What point on the plane x+y−z=0 is closest to b=(2,1,0)?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.23 Find an orthonormal basis for R3 starting with the vector (1,1,1)."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.24 CT scanners examine the patient from different directions and produce a matrix giving the densities of bone and tissue at each point. Mathematically, the problem is to recover a matrix from its projections. in the 2 by 2 case, can you recover the matrix A if you know the sum along each row and down each column?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.25 Can you recover a 3 by 3 matrix if you know its row sums and column sums, and also the sums down the main diagonal and the four other parallel diagonals?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.5 TheFastFourierTransform 223"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.26 Find an orthonormal basis for the plane x−y+z = 0, and find the matrix P that projects onto the plane. What is the nullspace of P?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.27 Let A=[3 1 1], and let V be the nullspace of A. (a) Find a basis for V and a basis for V⊥. (b) WriteanorthonormalbasisforV⊥,andfindtheprojectionmatrixP thatprojects 1 vectors in R3 onto V⊥. (c) Find the projection matrix P that projects vectors in R3 onto V. 2"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.28 Use Gram-Schmidt to construct an orthonormal pair q , q from a = (4,5,2,2) 1 2 1 and a = (1,2,0,0), Express a and a as combinations of q and q , and find the 2 1 2 1 2 triangular R in A=QR."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.29 For any A, b, x, and y, show that (a) if Ax=b and yTA=0, then yTb=0. (b) if Ax=0 and ATy=b, then xTb=0. What theorem does this prove about the fundamental subspaces?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.30 Is there a matrix whose row space contains (1,1,0) and whose nullspace contains (0,1,1)?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.31 ThedistancefromaplaneaTx=c(inm-dimensionalspace)totheoriginis|c|/(cid:107)a(cid:107). How far is the plane x +x −x −x = 8 from the origin, and what point on it is 1 2 3 4 nearest?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.32 In the parallelogram with corners at 0, v, w, and v+w, show that the sum of the squared lengths of the four sides equals the sum of the squared lengths of the two diagonals."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.33 (a) Find an orthonormal basis for the column space of A.   1 −6   3 6    A=4 8 .   5 0  7 8 (b) Write A as QR, where Q has orthonormal columns and R is upper triangular. (c) Find the least-squares solution to Ax=b, if b=(−3,7,1,0,4). (cid:163) (cid:164)"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.34 WithweightingmatrixW = 2 1 ,whatistheW-innerproductof(1,0)with(0,1)? 1 0"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.35 To solve a rectangular system Ax = b, we replace A−1 (which doesn’t exist) by (ATA)−1AT (which exists if A has independent columns). Show that this is a left- inverseofAbutnotaright-inverse. OntheleftofAitgivestheidentity;ontheright it gives the projection P."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.36 Find the straight line C+Dt that best fits the measurements b = 0,1,2,5 at times t =0,1,3,4."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.37 Find the curve y =C+D2t which gives the best least-squares fit to the measure- ments y = 6 at t = 0, y = 4 at t = 1, y = 0 at t = 2. Write the three equations that are solved if the curve goes through the three points, and find the bestC and D."
    },
    {
        "chapter": "Orthogonality",
        "question": "3.38 If the columns of A are orthogonal to each other what can you say about the form of ATA? If the columns are orthonormal, what can you say then?"
    },
    {
        "chapter": "Orthogonality",
        "question": "3.39 Under what condition on the columns of A (which may be rectangular) is ATA in- vertible? 4 Chapter Determinants"
    },
    {
        "chapter": "Orthogonality",
        "question": "4.1 Introduction Determinantsaremuchfurtherfromthecenteroflinearalgebrathantheywereahundred years ago. Mathematics keeps changing direction! After all, a single number can tell only so much about a matrix. Still, it is amazing how much this number can do. One viewpoint is this: The determinant provides an explicit “formula” for each entry of A−1 and A−1b. This formula will not change the way we compute; even the deter- minant itself is found by elimination. In fact, elimination can be regarded as the most efficient way to substitute the entries of an n by n matrix into the formula. What the formula does is to show how A−1 depends on the n2 entries of A, and how it varies when those entries vary. We can list four of the main uses of determinants:"
    },
    {
        "chapter": "Orthogonality",
        "question": "1. They test for invertibility. If the determinant of A is zero, then A is singular. If detA(cid:54)=0, then A is invertible (and A−1 involves 1/detA). The most important application, and the reason this chapter is essential to the book, is to the family of matrices A−λI. The parameter λ is subtracted all along the main diagonal, and the problem is to find the eigenvalues for which A−λI is singular. The testisdet(A−λI)=0. Thispolynomialofdegreeninλhasexactlynroots. Thematrix hasneigenvalues,Thisisafactthatfollowsfromthedeterminantformula,andnotfrom a computer."
    },
    {
        "chapter": "Determinants",
        "question": "2. The determinant of A equals the volume of a box in n-dimensional space. The edges of the box come from the rows of A (Figure 4.1). The columns of A would give an entirely different box with the same volume. (cid:82)(cid:82)(cid:82) The simplest box is a little cube dV = dxdydz, as in f(x,y,z)dV. Suppose we change to cylindrical coordinates by x =rcosθ, y=rsinθ, z=z. Just as a small inter- val dx is stretched to (dx/du)du—when u replaces x in a single integral—so the volume element becomes J drdθdz. The Jacobian determinant is the three-dimensional ana- z (a31,a32,a33) y (a21,a22,a23) (a11,a12,a13) x Figure4.1: TheboxformedfromtherowsofA: volume=|determinant|. logue of the stretching factor dx/du: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)∂x/∂r ∂x/∂θ ∂x/∂z(cid:175) (cid:175)cosθ −rsinθ 0(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) Jacobian J =(cid:175)∂y/∂r ∂y/∂θ ∂y/∂z(cid:175)=(cid:175)sinθ rcosθ 0(cid:175). (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)∂z/∂r ∂z/∂θ ∂z/∂z(cid:175) (cid:175) 0 0 1(cid:175) The value of this determinant is J = r. It is the r in the cylindrical volume element r drdθdz; this element is our little box. (It looks curved if we try to draw it, but proba- bly it gets straighter as the edges become infinitesimal.)"
    },
    {
        "chapter": "Determinants",
        "question": "3. The determinant gives a formula for each pivot. Theoretically, we could predict when a pivot entry will be zero, requiring a row exchange. From the formula determi- nant = ± (product of the pivots), it follows that regardless of the order of elimination, the product of the pivots remains the same apart from sign. Years ago, this led to the belief that it was useless to escape a very small pivot by exchanging rows, since eventually the small pivot would catch up with us. But what usually happens in practice, if an abnormally small pivot is not avoided, is that it is very soon followed by an abnormally large one. This brings the product back to normal but it leaves the numerical solution in ruins."
    },
    {
        "chapter": "Determinants",
        "question": "4. The determinant measures the dependence of A−1b on each element of b. If one parameter is changed in an experiment, or one observation is corrected, the “influence coefficient” in A−1 is a ratio of determinants. There is one more problem about the determinant. It is difficult not only to decide on its importance, and its proper place in the theory of linear algebra, but also to choose"
    },
    {
        "chapter": "Determinants",
        "question": "4.2 PropertiesoftheDeterminant 227 the best definition. Obviously, detA will not be some extremely simple function of n2 variables; otherwise A−1 would be much easier to find than it actually is. Thesimplethingsaboutthedeterminantarenottheexplicitformulas,buttheprop- ertiesitpossesses. Thissuggeststhenaturalplacetobegin. Thedeterminantcanbe(and will be) defined by its three most basic properties: detI = 1, the sign is reversed by a rowexchange, thedeterminant islinearin eachrowseparately. The problemisthen toshow,bysystematicallyusingtheseproperties,howthedeterminantcanbecomputed. This will bring us back to the product of the pivots. Section4.2explainsthesethreedefiningpropertiesofthedeterminant,andtheirmost important consequences. Section 4.3 gives two more formulas for the determinant—the “big formula” with n! terms, and a formula “by induction”. In Section 4.4 the determi- nantisappliedtofindA−1. Thenwecomputex=A−1bbyCramer’srule. Andfinally,in anoptionalremarkonpermutations, weshowthatwhatevertheorderinwhichtheprop- ertiesareused,theresultisalwaysthesame—thedefiningpropertiesareself-consistent. Here is a light-hearted question about permutations. How many exchanges does it take to change VISA into AVIS? Is this permutation odd or even?"
    },
    {
        "chapter": "Determinants",
        "question": "4.2 Properties of the Determinant This will be a pretty long list. Fortunately each rule is easy to understand, and even easier to illustrate, for a 2 by 2 example. Therefore we shall verify that the familiar definition in the 2 by 2 case, (cid:34) (cid:35) (cid:175) (cid:175) (cid:175) (cid:175) a b (cid:175)a b(cid:175) det =(cid:175) (cid:175)=ad−bc, c d (cid:175)c d(cid:175) possesses every property in the list. (Notice the two accepted notations for the deter- minant, detA and |A|.) Properties 4–10 will be deduced from the previous ones. Every property is a consequence of the first three. We emphasize that the rules apply to square matrices of any size."
    },
    {
        "chapter": "Determinants",
        "question": "1. The determinant of the identity matrix is 1. (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)1 0 0(cid:175) (cid:175)1 0(cid:175) (cid:175) (cid:175) detI =1 (cid:175) (cid:175)=1 and (cid:175)0 1 0(cid:175)=1 and... (cid:175)0 1(cid:175) (cid:175) (cid:175) (cid:175)0 0 1(cid:175)"
    },
    {
        "chapter": "Determinants",
        "question": "2. The determinant changes sign when two rows are exchanged. (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)c d(cid:175) (cid:175)a b(cid:175) Row exchange (cid:175) (cid:175)=cb−ad =−(cid:175) (cid:175). (cid:175)a b(cid:175) (cid:175)c d(cid:175) The determinant of every permutation matrix is detP=±1. By row exchanges, we can turn P into the identity matrix. Each row exchange switches the sign of the determinant, until we reach detI =1. Now come all other matrices!"
    },
    {
        "chapter": "Determinants",
        "question": "3. The determinant depends linearly on the first row. Suppose A, B, C are the same from the second row down—and row 1 of A is a linear combination of the first rows of B andC. Then the rule says: detA is the same combination of detB and detC. Linearcombinationsinvolvetwooperations—addingvectorsandmultiplyingbyscalars. Therefore this rule can be split into two parts: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a+a(cid:48) b+b(cid:48) (cid:175) (cid:175)a b(cid:175) (cid:175)a(cid:48) b(cid:48) (cid:175) Add vectors in row 1 (cid:175) (cid:175)=(cid:175) (cid:175)+(cid:175) (cid:175). (cid:175) c d (cid:175) (cid:175)c d(cid:175) (cid:175)c d(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)ta tb(cid:175) (cid:175)a b(cid:175) Multiply byt in row 1 (cid:175) (cid:175)=t(cid:175) (cid:175). (cid:175)c d(cid:175) (cid:175)c d(cid:175) Noticethatthefirstpartisnot thefalsestatementdet(B+C)=detB+detC. Youcannot add all the rows: only one row is allowed to change. Both sides give the answer ad+ a(cid:48)d−bc−b(cid:48)c. Thesecondpartisnotthefalsestatementdet(tA)=tdetA. ThematrixtAhasafactor t in every row (and the determinant is multiplied by tn). It is like the volume of a box, when all sides are stretched by 4. In n dimensions the volume and determinant go up by 4n. If only one side is stretched, the volume and determinant go up by 4; that is rule 3. By rule 2, there is nothing special about the first row. The determinant is now settled, but that fact is not at all obvious. Therefore we grad- ually use these rules to find the determinant of any matrix."
    },
    {
        "chapter": "Determinants",
        "question": "4. If two rows of A are equal, then detA=0. (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a b(cid:175) Equal rows (cid:175) (cid:175)=ab−ba=0. (cid:175)a b(cid:175) This follows from rule 2, since if the equal rows are exchanged, the determinant is sup- posedtochangesign. Butitalsohastostaythesame,becausethematrixstaysthesame. The only number which can do that is zero, so detA=0. (The reasoning fails if 1=−1, which is the case in Boolean algebra. Then rule 4 should replace rule 2 as one of the defining properties.)"
    },
    {
        "chapter": "Determinants",
        "question": "4.2 PropertiesoftheDeterminant 229"
    },
    {
        "chapter": "Determinants",
        "question": "5. Subtracting a multiple of one row from another row leaves the same determinant. (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a−(cid:96)c b−(cid:96)d(cid:175) (cid:175)a b(cid:175) Row operation (cid:175) (cid:175)=(cid:175) (cid:175). (cid:175) c d (cid:175) (cid:175)c d(cid:175) (cid:175) (cid:175) Rule3wouldsaythatthereisafurtherterm−(cid:96)(cid:175)c d(cid:175) ,butthattermiszerobyrule4. The c d usual elimination steps do not affect the determinant!"
    },
    {
        "chapter": "Determinants",
        "question": "6. If A has a row of zeros, then detA=0. (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)0 0(cid:175) Zero row (cid:175) (cid:175)=0. (cid:175)c d(cid:175) One proof is to add some other row to the zero row. The determinant is unchanged, by rule 5. Because the matrix will now have two identical rows, detA=0 by rule 4."
    },
    {
        "chapter": "Determinants",
        "question": "7. If A is triangular then detA is the product a a ···a of the diagonal entries. If 11 22 nn the triangular A has 1s along the diagonal, then detA=1. (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a b(cid:175) (cid:175)a 0(cid:175) Triangular matrix (cid:175) (cid:175)=ad (cid:175) (cid:175)=ad. (cid:175)0 d(cid:175) (cid:175)c d(cid:175) Proof. Suppose the diagonal entries are nonzero. Then elimination can remove all the off-diagonal entries, without changing the determinant (by rule 5). If A is lower triangu- lar, the steps are downward as usual. If A is upper triangular, the last column is cleared out first—using multiples of a . Either way we reach the diagonal matrix D: nn   a 11 D=  ...   has detD=a a ···a detI =a a ···a . 11 22 nn 11 22 nn a nn To find detD we patiently apply rule 3. Factoring out a and then a and finally a 11 22 nn leaves the identity matrix. At last we have a use for rule 1: detI =1. If a diagonal entry is zero then elimination will produce a zero row. By rule 5 these elimination steps do not change the determinant. By rule 6 the zero row means a zero determinant. Thismeans: Whenatriangularmatrixissingular (becauseofazeroonthe main diagonal) its determinant is zero. This is a key property. All singular matrices have a zero determinant."
    },
    {
        "chapter": "Determinants",
        "question": "8. If A is singular, then detA=0. If A is invertible, then detA(cid:54)=0. (cid:34) (cid:35) a b Singular matrix is not invertible if and only if ad−bc=0. c d If A is singular, elimination leads to a zero row in U. Then detA = detU = 0. If A is nonsingular, elimination puts the pivots d ,...,d on the main diagonal. We have a 1 n “product of pivots” formula for detA! The sign depends on whether the number of row exchanges is even or odd: Product of pivots detA=±detU =±d d ···d . (1) 1 2 n The ninth property is the product rule. I would say it is the most surprising."
    },
    {
        "chapter": "Determinants",
        "question": "9. The determinant of AB is the product of detA times detB. (cid:175) (cid:175)(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)(cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a b(cid:175)(cid:175)e f(cid:175) (cid:175)ae+bg af +bh(cid:175) Product rule |A||B|=|AB| (cid:175) (cid:175)(cid:175) (cid:175)=(cid:175) (cid:175). (cid:175)c d(cid:175)(cid:175)g h(cid:175) (cid:175)ce+dg cf +dh(cid:175) A particular case of this rule gives the determinant of A−1. It must be 1/detA: 1 detA−1 = because (detA)(detA−1)=detAA−1 =detI =1. (2) detA In the 2 by 2 case, the product rule could be patiently checked: (ad−bc)(eh− fg)=(ae+bg)(cf +dh)−(af +bh)(ce+dg). In the n by n case we suggest two possible proofs—since this is the least obvious rule. Both proofs assume that A and B are nonsingular; otherwise AB is singular, and the equation detAB=(detA)(detB) is easily verified. By rule 8, it becomes 0=0. (i) We prove that the ratio d(A) = detAB/detB has properties 1–3. Then d(A) must equal detA. For example, d(I) = detB/detB = 1; rule 1 is satisfied. If two rows of A are exchanged, so are the same two rows of AB, and the sign of d changes as required by rule 2. A linear combination in the first row of A gives the same linear combination in the first row of AB. Then rule 3 for the determinant of AB, divided by the fixed quantity detB, leads to rule 3 for the ratio d(A). Thus d(A)= detAB/detB coincides with detA, which is our product formula. (ii) This second proof is less elegant. For a diagonal matrix, detDB = (detD)(detB) follows by factoring each d from its row. Reduce a general matrix A to D by i elimination—from A toU as usual, and fromU to D by upward elimination. The determinant does not change, except for a sign reversal when rows are exchanged. ThesamestepsreduceABtoDB,withpreciselythesameeffectonthedeterminant. But for DB it is already confirmed that rule 9 is correct."
    },
    {
        "chapter": "Determinants",
        "question": "10. The transpose of A has the same determinant as A itself: detAT =detA. (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a b(cid:175) (cid:175)a c(cid:175) (cid:175) (cid:175) Transpose rule (cid:175)A(cid:175)=(cid:175) (cid:175)=(cid:175) (cid:175)=(cid:175)AT(cid:175). (cid:175)c d(cid:175) (cid:175)b d(cid:175)"
    },
    {
        "chapter": "Determinants",
        "question": "1. If a 4 by 4 matrix has detA= 1, find det(2A), det(−A), det(A2), and det(A−1). 2"
    },
    {
        "chapter": "Determinants",
        "question": "2. If a 3 by 3 matrix has detA=−1, find det(1A), det(−A), det(A2), and det(A−1). 2"
    },
    {
        "chapter": "Determinants",
        "question": "3. Row exchange: Add row 1 of A to row 2, then subtract row 2 from row 1. Then add row 1 to row 2 and multiply row 1 by −1 to reach B. Which rules show the following? (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)c d(cid:175) (cid:175)a b(cid:175) detB=(cid:175) (cid:175) equals −detA=−(cid:175) (cid:175). (cid:175)a b(cid:175) (cid:175)c d(cid:175) Those rules could replace Rule 2 in the definition of the determinant."
    },
    {
        "chapter": "Determinants",
        "question": "4. By applying row operations to produce an upper triangularU, compute     1 2 −2 0 2 −1 0 0      2 3 −4 1 −1 2 −1 0  det  and det . −1 −2 0 2  0 −1 2 −1 0 2 5 3 0 0 −1 −2 Exchange rows 3 and 4 of the second matrix and recompute the pivots and determi- nant. Note. Some readers will already know a formula for 3 by 3 determinants. It has six terms(equation(2)ofthenextsection),threegoingparalleltothemaindiagonaland three others going the opposite way with minus signs. There is a similar formula for 4 by 4 determinants, but it contains 4!=24 terms (not just eight). You cannot even be sure that a minus sign goes with the reverse diagonal, as the next exercises show."
    },
    {
        "chapter": "Determinants",
        "question": "5. Count row exchanges to find these determinants:     0 0 0 1 0 1 0 0     0 0 1 0 0 0 1 0 det =±1 and det =−1. 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0"
    },
    {
        "chapter": "Determinants",
        "question": "6. For each n, how many exchanges will put (row n, row n−1,..., row 1) into the normal order (row 1, ... , row n−1, row n)? Find detP for the n by n permutation with 1s on the reverse diagonal. Problem 5 had n=4."
    },
    {
        "chapter": "Determinants",
        "question": "7. Find the determinants of: (a) a rank one matrix   1 (cid:104) (cid:105)   A=4 2 −1 2 . 2 (b) the upper triangular matrix   4 4 8 8   0 1 2 2 U = . 0 0 2 6 0 0 0 2 (c) the lower triangular matrixUT. (d) the inverse matrixU−1."
    },
    {
        "chapter": "Determinants",
        "question": "4.2 PropertiesoftheDeterminant 233 (e) the “reverse-triangular” matrix that results from row exchanges,   0 0 0 2   0 0 2 6 M = . 0 1 2 2 4 4 8 8"
    },
    {
        "chapter": "Determinants",
        "question": "8. Show how rule 6 (det=0 if a row is zero) comes directly from rules 2 and 3."
    },
    {
        "chapter": "Determinants",
        "question": "9. Suppose you do two row operations at once, going from (cid:34) (cid:35) (cid:34) (cid:35) a b a−mc b−md to . c d c−(cid:96)a d−(cid:96)b Find the determinant of the new matrix, by rule 3 or by direct calculation."
    },
    {
        "chapter": "Determinants",
        "question": "10. If Q is an orthogonal matrix, so that QTQ = I, prove that detQ equals +1 or −1. What kind of box is formed from the rows (or columns) of Q?"
    },
    {
        "chapter": "Determinants",
        "question": "11. Prove again that detQ = 1 or −1 using only the Product rule. If |detQ| > 1 then detQn blows up. How do you know this can’t happen to Qn?"
    },
    {
        "chapter": "Determinants",
        "question": "12. Use row operations to verify that the 3 by 3 “Vandermonde determinant” is   1 a a2   det1 b b2=(b−a)(c−a)(c−b). 1 c c2"
    },
    {
        "chapter": "Determinants",
        "question": "13. (a) A skew-symmetric matrix satisfies KT =−K, as in   0 a b   K =−a 0 c. −b −c 0 In the 3 by 3 case, why is det(−K) = (−1)3detK? On the other hand detKT = detK (always). Deduce that the determinant must be zero. (b) Write down a 4 by 4 skew-symmetric matrix with detK not zero."
    },
    {
        "chapter": "Determinants",
        "question": "14. True or false, with reason if true and counterexample if false: (a) If A and B are identical except that b =2a , then detB=2detA. 11 11 (b) The determinant is the product of the pivots. (c) If A is invertible and B is singular, then A+B is invertible. (d) If A is invertible and B is singular, then AB is singular. (e) The determinant of AB−BA is zero."
    },
    {
        "chapter": "Determinants",
        "question": "15. If every row of A adds to zero, prove that detA = 0. If every row adds to 1, prove that det(A−I)=0. Show by example that this does not imply detA=1."
    },
    {
        "chapter": "Determinants",
        "question": "16. Find these 4 by 4 determinants by Gaussian elimination:     11 12 13 14 1 t t2 t3     21 22 23 24 t 1 t t2  det  and det . 31 32 33 34 t2 t 1 t  41 42 43 44 t3 t2 t 1"
    },
    {
        "chapter": "Determinants",
        "question": "17. Find the determinants of (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 4 2 1 3 −2 4−λ 2 A= , A−1 = , A−λI = . 1 3 10 −1 4 1 3−λ For which values ofλ is A−λI a singular matrix?"
    },
    {
        "chapter": "Determinants",
        "question": "18. Evaluate detA by reducing the matrix to triangular form (rules 5 and 7).       1 1 3 1 1 3 1 1 3       A=0 4 6, B=0 4 6, C =0 4 6. 1 5 8 0 0 1 1 5 9 What are the determinants of B,C, AB, ATA, andCT?"
    },
    {
        "chapter": "Determinants",
        "question": "19. Suppose that CD = −DC, and find the flaw in the following argument: Taking de- terminants gives (detC)(detD) = −(detD)(detC), so either detC = 0 or detD = 0. ThusCD=−DC is only possible ifC or D is singular."
    },
    {
        "chapter": "Determinants",
        "question": "20. Do these matrices have determinant 0, 1, 2, or 3?       0 0 1 0 1 1 1 1 1       A=1 0 0 B=1 0 1 C =1 1 1. 0 1 0 1 1 0 1 1 1"
    },
    {
        "chapter": "Determinants",
        "question": "21. The inverse of a 2 by 2 matrix seems to have determinant =1: (cid:34) (cid:35) 1 d −b ad−bc detA−1 =det = =1. ad−bc −c a ad−bc What is wrong with this calculation? What is the correct detA−1? Problems 22–28 use the rules to compute specific determinants."
    },
    {
        "chapter": "Determinants",
        "question": "22. Reduce A toU and find detA= product of the pivots:     1 1 1 1 2 3     A=1 2 2 and A=2 2 3. 1 2 3 3 3 3"
    },
    {
        "chapter": "Determinants",
        "question": "4.2 PropertiesoftheDeterminant 235"
    },
    {
        "chapter": "Determinants",
        "question": "23. By applying row operations to produce an upper triangularU, compute     1 2 3 0 2 1 1 1      2 6 6 1 1 2 1 1 det  and det . −1 0 0 3 1 1 2 1 0 2 0 7 1 1 1 2"
    },
    {
        "chapter": "Determinants",
        "question": "24. Use row operations to simplify and compute these determinants:     101 201 301 1 t t2     det102 202 302 and dett 1 t . 103 203 303 t2 t 1"
    },
    {
        "chapter": "Determinants",
        "question": "25. Elimination reduces A toU. Then A=LU:      3 3 4 1 0 0 3 3 4      A= 6 8 7 = 2 1 00 2 −1=LU. −3 5 −9 −1 4 1 0 0 −1 Find the determinants of L,U, A,U−1L−1, andU−1L−1A."
    },
    {
        "chapter": "Determinants",
        "question": "26. If a is i times j, show that detA=0. (Exception when A=[1].) ij"
    },
    {
        "chapter": "Determinants",
        "question": "27. If a is i+ j, show that detA=0. (Exception when n=1 or 2.) ij"
    },
    {
        "chapter": "Determinants",
        "question": "28. Compute the determinants of these matrices by row operations:       0 a 0 0 0 a 0   a a a   0 0 b 0   A=0 0 b, B= , and C =a b b. 0 0 0 c c 0 0 a b c d 0 0 0"
    },
    {
        "chapter": "Determinants",
        "question": "29. What is wrong with this proof that projection matrices have detP=1? 1 P=A(ATA)−1AT so |P|=|A| |AT|=1. |AT||A|"
    },
    {
        "chapter": "Determinants",
        "question": "30. (Calculus question) Show that the partial derivatives of ln(detA) give A−1: (cid:34) (cid:35) ∂f/∂a ∂f/∂c f(a,b,c,d)=ln(ad−bc) leads to =A−1. ∂f/∂b ∂f/∂d"
    },
    {
        "chapter": "Determinants",
        "question": "31. (MATLAB) The Hilbert matrix hilb(n) has i, j entry equal to 1/(i+ j−1). Print ti determinants of hilb(1),hilb(2),...,hilb(10). Hilbert matrices are hard to work with! What are the pivots?"
    },
    {
        "chapter": "Determinants",
        "question": "32. (MATLAB)Whatisatypicaldeterminant(experimentally)ofrand(n)andrandn(n) for n=50,100,200,400? (And what does “Inf” mean in MATLAB?)"
    },
    {
        "chapter": "Determinants",
        "question": "33. Using MATLAB, find the largest determinant of a 4 by 4 matrix of 0s and 1s."
    },
    {
        "chapter": "Determinants",
        "question": "34. If you know that detA=6, what is the determinant of B? (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)row 1(cid:175) (cid:175)row 1+row 2(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) detA=(cid:175)row 2(cid:175)=6 detB=(cid:175)row 2+row 3(cid:175)= (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)row 3(cid:175) (cid:175)row 3+row 1(cid:175)"
    },
    {
        "chapter": "Determinants",
        "question": "35. Suppose the 4 by 4 matrix M has four equal rows all containing a, b, c, d. We know that det(M)=0. The problem is to find det(I+M) by any method: (cid:175) (cid:175) (cid:175) (cid:175) 1+a b c d (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) a 1+b c d (cid:175) det(I+M)=(cid:175) (cid:175). (cid:175) a b 1+c d (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) a b c 1+d Partial credit if you find this determinant when a = b = c = d = 1. Sudden death if you say that det(I+M)=detI+detM."
    },
    {
        "chapter": "Determinants",
        "question": "4.3 Formulas for the Determinant The first formula has already appeared. Row operations produce the pivots in D: 4A If A is invertible, then PA=LDU and detP=±1. The product rule gives detA=±detLdetDdetU =±(product of the pivots). (1) The sign ±1 depends on whether the number of row exchanges is even or odd. The triangular factors have detL=detU =1 and detD=d ···d . 1 n In the 2 by 2 case, the standard LDU factorization is (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) a b 1 0 a 0 1 b/a = . c d c/a 1 0 (ad−bc)/a 0 1 The product of the pivots is ad−bc. That is the determinant of the diagonal matrix D. If the first step is a row exchange, the pivots are c and (−detA)/c. Example 1. The −1,2,−1 second difference matrix has pivots 2/1,3/2,... in D:     2 −1 2     −1 2 −1   3/2       −1 2 · =LDU =L 4/3 U.      · · −1  ·  −1 2 (n+1)/n"
    },
    {
        "chapter": "Determinants",
        "question": "4.3 FormulasfortheDeterminant 237 Its determinant is the product of its pivots. The numbers 2,...,n all cancel: (cid:181) (cid:182)(cid:181) (cid:182) (cid:181) (cid:182) 3 4 n+1 detA=2 ··· =n+1. 2 3 n MATLAB computes the determinant from the pivots. But concentrating all information into the pivots makes it impossible to figure out how a change in one entry would affect the determinant. We want to find an explicit expression for the determinant in terms of the n2 entries. For n = 2, we will be proving that ad−bc is correct. For n = 3, the determinant formula is again pretty well known (it has six terms): (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a 11 a 12 a 13(cid:175) (cid:175) (cid:175) +a a a +a a a +a a a 11 22 33 12 23 31 13 21 32 (cid:175)a a a (cid:175)= (2) 21 22 23 (cid:175) (cid:175) −a a a −a a a −a a a . (cid:175)a a a (cid:175) 11 23 32 12 21 33 13 22 31 31 32 33 Ourgoalisto derivetheseformulasdirectly fromthedefiningproperties 1–3of detA. If we can handle n=2 and n=3 in an organized way, you will see the pattern. To start, each row can be broken down into vectors in the coordinate directions: (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) a b = a 0 + 0 b and c d = c 0 + 0 d . Then we apply the property of linearity, first in row 1 and then in row 2: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a b(cid:175) (cid:175)a 0(cid:175) (cid:175)0 b(cid:175) (cid:175) (cid:175)=(cid:175) (cid:175)+(cid:175) (cid:175) Separate into (cid:175)c d(cid:175) (cid:175)c d(cid:175) (cid:175)c d(cid:175) nn =22 easy (3) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) determinants (cid:175)a 0(cid:175) (cid:175)a 0(cid:175) (cid:175)0 b(cid:175) (cid:175)0 b(cid:175) =(cid:175) (cid:175)+(cid:175) (cid:175)+(cid:175) (cid:175)+(cid:175) (cid:175). (cid:175)c 0(cid:175) (cid:175)0 d(cid:175) (cid:175)c 0(cid:175) (cid:175)0 d(cid:175) Every row splits into n coordinate directions, so this expansion has nn terms. Most of those terms (all but n! = n factorial) will be automatically zero. When two rows are in the same coordinate direction, one will be a multiple of the other, and (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a 0(cid:175) (cid:175)0 b(cid:175) (cid:175) (cid:175)=0, (cid:175) (cid:175)=0. (cid:175)c 0(cid:175) (cid:175)0 d(cid:175) We pay attention only when the rows point in different directions. The nonzero terms have to come in different columns. Suppose the first row has a nonzero term in column α, the second row is nonzero in column β, and finally the nth row in column v. The column numbers α,β,...,v are all different. They are a reordering, or permutation, of the numbers 1,2,...,n. The 3 by 3 case produces 3!=6 determinants: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a 11 a 12 a 13(cid:175) (cid:175)a 11 (cid:175) (cid:175) a 12 (cid:175) (cid:175) a 13(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a a a (cid:175)=(cid:175) a (cid:175)+(cid:175) a (cid:175)+(cid:175)a (cid:175) 21 22 23 22 23 21 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a a a (cid:175) (cid:175) a (cid:175) (cid:175)a (cid:175) (cid:175) a (cid:175) 31 32 33 33 31 32 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (4) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a 11 (cid:175) (cid:175) a 12 (cid:175) (cid:175) a 13(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) +(cid:175) a (cid:175)+(cid:175)a (cid:175)+(cid:175) a (cid:175). 23 21 22 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) a (cid:175) (cid:175) a (cid:175) (cid:175)a (cid:175) 32 33 31 All but these n! determinants are zero, because a column is repeated. (There are n choices for the first column α, n−1 remaining choices for β, and finally only one choice for the last column v. All but one column will be used by that time, when we “snake” down the rows of the matrix). In other words, there are n! ways to permute the numbers 1,2,...,n. The column numbers give the permutations: Column numbers (α,β,v)=(1,2,3), (2,3,1), (3,1,2), (1,3,2), (2,1,3), (3,2,1). Those are the 3!=6 permutations of (1,2,3); the first one is the identity. The determinant of A is now reduced to six separate and much simpler determinants. Factoring out the a , there is a term for every one of the six permutations: ij (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)1 (cid:175) (cid:175) 1 (cid:175) (cid:175) 1(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) detA=a a a (cid:175) 1 (cid:175)+a a a (cid:175) 1(cid:175)+a a a (cid:175)1 (cid:175) 11 22 33 12 23 31 13 21 32 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 1(cid:175) (cid:175)1 (cid:175) (cid:175) 1 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (5) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)1 (cid:175) (cid:175) 1 (cid:175) (cid:175) 1(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) +a a a (cid:175) 1(cid:175)+a a a (cid:175)1 (cid:175)+a a a (cid:175) 1 (cid:175). 11 23 32 12 21 33 13 22 31 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 1 (cid:175) (cid:175) 1(cid:175) (cid:175)1 (cid:175) Everytermisaproductofn=3entriesa ,witheachrowandcolumnrepresentedonce. ij If the columns come in the order (α,...,v), that term is the product a ···a times the 1α nv determinant of a permutation matrix P. The determinant of the whole matrix is the sum of these n! terms, and that sum is the explicit formula we are after: Big Formula detA= ∑ (a a ···a )detP. (6) 1α 2β nv allP’s Forannbynmatrix,thissumistakenoveralln!permutations(α,...,v)ofthenumbers (1,...,n). The permutation gives the column numbers as we go down the matrix. The is appear in P at the same places where the a’s appeared in A. It remains to find the determinant of P. Row exchanges transform it to the identity matrix, and each exchange reverses the sign of the determinant: detP=+1 or −1 for an even or odd number of row exchanges."
    },
    {
        "chapter": "Determinants",
        "question": "4.3 FormulasfortheDeterminant 239 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)1 (cid:175) (cid:175) 1(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (1,3,2) is odd so (cid:175) 1(cid:175)=−1 (3,1,2) is even so (cid:175)1 (cid:175)=1 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 1 (cid:175) (cid:175) 1 (cid:175) (1,3,2) requires one exchange and (3,1,2) requires two exchanges to recover (1,2,3). These are two of the six ± signs. For n=2, we only have (1,2) and (2,1): (cid:34) (cid:35) (cid:34) (cid:35) 1 0 0 1 detA=a a det +a a det =a a −a a (or ad−bc). 11 22 12 21 11 22 12 21 0 1 1 0 No one can claim that the big formula (6) is particularly simple. Nevertheless, it is possible to see why it has properties 1–3. For A = I, every product of the a will be ij zero, except for the column sequence (1,2,...,n). This term gives detI =1. Property 2 will be checked in the next section, because here we are most interested in property 3: The determinant should depend linearly on the first row a ,a ,...,a . 11 12 1n Look at all the terms a a ···a involving a . The first column is α= 1. This 1α 2β nv 11 leaves some permutation (β,...,v) of the remaining columns (2,...,n). We collect all these terms together as a C , where the coefficient of a is a smaller determinant— 11 11 11 with row 1 and column 1 removed: Cofactor of a C =∑(a ···a )detP=det(submatrix of A). (7) 11 11 2β nv Similarly, the entry a is multiplied by some smaller determinantC . Grouping all the 12 12 terms that start with the same a , formula (6) becomes 1j Cofactors along row 1 detA=a C +a C +···+a C . (8) 11 11 12 12 1n 1n This shows that detA depends linearly on the entries a ,...,a of the first row. 11 1n Example 2. For a 3 by 3 matrix, this way of collecting terms gives detA=a (a a −a a )+a (a a −a a )+a (a a −a a ). (9) 11 22 33 23 32 12 23 31 21 33 13 21 32 22 31 The cofactorsC ,C ,C are the 2 by 2 determinants in parentheses. 11 12 13 Expansion of detA in Cofactors Wewantonemoreformulaforthedeterminant. Ifthismeantstartingagainfromscratch, itwouldbetoomuch,Buttheformulaisalreadydiscovered—itis(8),andtheonlypoint is to identify the cofactorsC that multiply a . 1j 1j We know that C depends on rows 2,...,n. Row 1 is already accounted for by 1j a . Furthermore, a also accounts for the jth column, so its cofactorC must depend 1j 1j 1j entirely on the other columns. No row or column can be used twice in the same term. What we are really doing is splitting the determinant into the following sum: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a 11 a 12 a 13(cid:175) (cid:175)a 11 (cid:175) (cid:175) a 12 (cid:175) (cid:175) a 13(cid:175) Cofactor (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a a a (cid:175)=(cid:175) a a (cid:175)+(cid:175)a a (cid:175)+(cid:175)a a (cid:175). 21 22 23 22 23 21 23 21 22 splitting (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a a a (cid:175) (cid:175) a a (cid:175) (cid:175)a a (cid:175) (cid:175)a a (cid:175) 31 32 33 32 33 31 33 31 32 Foradeterminantofordern,thissplittinggivesnsmallerdeterminants(minors)oforder n−1; you see the three 2 by 2 submatrices. The submatrix M is formed by throwing 1j away row 1 and column j. Its determinant is multiplied by a —and by a plus or minus 1j sign. These signs alternate as in detM , −detM , detM : 11 12 13 Cofactors of row 1 C =(−1)1+jdetM . 1j 1j The second cofactorC is a a −a a , which is detM times −1. This same tech- 12 23 31 21 33 12 nique works on every n by n matrix. The splitting above confirms thatC is the deter- 11 minant of the lower right corner M . 11 There is a similar expansion on any other row, say row i. It could be proved by exchanging row i with row 1. Remember to delete row i and column j of A for M : ij 4B The determinant of A is a combination of any row i times its cofactors: detA by cofactors detA=a C +a C +···+a C . (10) i1 i1 i2 i2 in in The cofactorC is the determinant of M with the correct sign: 1j ij delete row i and column j C =(−1)i+jdetM . (11) ij ij These formulas express detA as a combination of determinants of order n−1. We could have defined the determinant by induction on n. A 1 by 1 matrix has detA =a , 11 and then equation (10) defines the determinants of 2 by 2 matrices, 3 by 3 matrices, and n by n matrices. We preferred to define the determinant by its properties, which are much simpler to explain. The explicit formula (6) and the cofactor formula (10) followed directly from these properties. There is one more consequence of detA = detAT. We can expand in cofactors of a column of A, which is a row of AT. Down column j of A, detA=a C +a C +···+a C . (12) 1j 1j 2j 2j nj nj Example 3. The 4 by 4 second difference matrix A4 has only two nonzeros in row 1:   2 −1 0 0   −1 2 −1 0  Use cofactors A4= .  0 −1 2 −1 0 0 −1 2"
    },
    {
        "chapter": "Determinants",
        "question": "1. For these matrices, find the only nonzero term in the big formula (6):     0 1 0 0 0 0 1 2     1 0 1 0 0 3 4 5 A=  and B= . 0 1 0 1 6 7 8 9 0 0 1 0 0 0 0 1 There is only one way of choosing four nonzero entries from different rows and different columns. By deciding even or odd, compute detA and detB."
    },
    {
        "chapter": "Determinants",
        "question": "2. Expand those determinants in cofactors of the first row. Find the cofactors (they include the signs (−1)i+j) and the determinants of A and B."
    },
    {
        "chapter": "Determinants",
        "question": "3. True or false? (a) The determinant of S−1AS equals the determinant of A. (b) If detA=0 then at least one of the cofactors must be zero. (c) A matrix whose entries are 0s and 1s has determinant 1, 0, or −1."
    },
    {
        "chapter": "Determinants",
        "question": "4. (a) Find the LU factorization, the pivots, and the determinant of the 4 by 4 matrix whose entries are a = smaller of i and j. (Write out the matrix.) ij (b) Find the determinant if a = smaller of n and n , where n =2, n =6, n =8, ij i j 1 2 3 n =10. Can you give a general rule for any n ≤n ≤n ≤n ? 4 1 2 3 4"
    },
    {
        "chapter": "Determinants",
        "question": "5. Let F be the determinant of the 1, 1, −1 tridiagonal matrix (n by n): n   1 −1   1 1 −1    F =det 1 1 −1 . n    · · · 1 1 By expanding in cofactors along row 1, show that F =F +F . This yields the n n−1 n−2 Fibonacci sequence 1,2,3,5,8,13,... for the determinants."
    },
    {
        "chapter": "Determinants",
        "question": "6. Suppose A is the n by n tridiagonal matrix with is on the three diagonals: n   (cid:34) (cid:35) 1 1 0 (cid:104) (cid:105) 1 1   A = 1 , A = , A =1 1 1, ... 1 2 3 1 1 0 1 1 Let D be the determinant of A ; we want to find it. n n (a) Expand in cofactors along the first row to show that D =D −D . n n−1 n−2 (b) Starting from D = 1 and D = 0, find D ,D ,...,D . By noticing how these 1 2 3 4 8 numbers cycle around (with what period?) find D . 1000"
    },
    {
        "chapter": "Determinants",
        "question": "7. (a) Evaluate this determinant by cofactors of row 1: (cid:175) (cid:175) (cid:175) (cid:175) 4 4 4 4 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)1 2 0 1(cid:175) (cid:175) (cid:175). (cid:175)2 0 1 2(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 1 1 0 2 (b) Check by subtracting column 1 from the other columns and recomputing."
    },
    {
        "chapter": "Determinants",
        "question": "8. Compute the determinants of A , A , A . Can you predict A ? 2 3 4 n     0 1 1 1 (cid:34) (cid:35) 0 1 1   0 1   1 0 1 1 A = A =1 0 1 A = . 2 3 4 1 0 1 1 0 1 1 1 0 1 1 1 0 Use row operations to produce zeros, or use cofactors of row 1."
    },
    {
        "chapter": "Determinants",
        "question": "4.3 FormulasfortheDeterminant 243"
    },
    {
        "chapter": "Determinants",
        "question": "9. How many multiplications to find an n by n determinant from (a) the big formula (6)? (b) the cofactor formula (10), building from the count for n−1? (c) the product of pivots formula (including the elimination steps)?"
    },
    {
        "chapter": "Determinants",
        "question": "10. Ina5by5matrix,doesa+signor−signgowitha a a a a downthereverse 15 24 33 42 51 diagonal? Inotherwords,isP=(5,4,3,2,1)evenorodd? Thecheckerboardpattern of ± signs for cofactors does not give detP."
    },
    {
        "chapter": "Determinants",
        "question": "11. If A is m by n and B is n by m, explain why (cid:34) (cid:35) (cid:195) (cid:34) (cid:35) (cid:33) 0 A I 0 det =detAB. Hint: Postmultiply by . −B I B I Do an example with m < n and an example with m > n. Why does your second example automatically have detAB=0?"
    },
    {
        "chapter": "Determinants",
        "question": "12. SupposethematrixAisfixed,exceptthata variesfrom−∞to+∞. Giveexamples 11 in which detA is always zero or never zero. Then show from the cofactor expansion (8) that otherwise detA=0 for exactly one value of a . 11 Problems 13–23 use the big formula with n! terms: |A|=∑±a a ···a . 1α 2β nv"
    },
    {
        "chapter": "Determinants",
        "question": "13. Compute the determinants of A, B,C from six terms. Independent rows?       1 2 3 1 2 3 1 1 1       A=3 1 2 B=4 4 4 C =1 1 0. 3 2 1 5 6 7 1 0 0"
    },
    {
        "chapter": "Determinants",
        "question": "14. Compute the determinants of A, B,C. Are their columns independent?     (cid:34) (cid:35) 1 1 0 1 2 3     A 0 A=1 0 1 B=4 5 6 C = . 0 B 0 1 1 7 8 9"
    },
    {
        "chapter": "Determinants",
        "question": "15. Show that detA=0, regardless of the five nonzeros marked by x’s:   x x x   A=0 0 x. (What is the rank of A?) 0 0 x"
    },
    {
        "chapter": "Determinants",
        "question": "16. This problem shows in two ways that detA=0 (the x’s are any numbers):   x x x x x   x x x x x 5 by 5 matrix   A=0 0 0 x x. 3 by 3 zero matrix   0 0 0 x x Always singular 0 0 0 x x (a) How do you know that the rows are linearly dependent? (b) Explain why all 120 terms are zero in the big formula for detA."
    },
    {
        "chapter": "Determinants",
        "question": "17. Find two ways to choose nonzeros from four different rows and columns:     1 0 0 1 1 0 0 2     0 1 1 1 0 3 4 5 A=  B= . (B has the same zeros as A.) 1 1 0 1 5 4 0 3 1 0 0 1 2 0 0 1 Is detA equal to 1+1 or 1−1 or −1−1? What is detB?"
    },
    {
        "chapter": "Determinants",
        "question": "18. Place the smallest number of zeros in a 4 by 4 matrix that will guarantee detA = 0. Place as many zeros as possible while still allowing detA(cid:54)=0."
    },
    {
        "chapter": "Determinants",
        "question": "19. (a) If a =a =a =0, how many of the six terms in detA will be zero? 11 22 33 (b) Ifa =a =a =a =0, howmanyofthe24products a a a a aresure 11 22 33 44 1j 2k 3(cid:96) 4m to be zero?"
    },
    {
        "chapter": "Determinants",
        "question": "20. How many 5 by 5 permutation matrices have detP = +1? Those are even permuta- tions. Find one that needs four exchanges to reach the identity matrix."
    },
    {
        "chapter": "Determinants",
        "question": "21. If detA (cid:54)= 0, at least one of the n! terms in the big formula (6) is not zero. Deduce that some ordering of the rows of A leaves no zeros on the diagonal. (Don’t use P from elimination; that PA can have zeros on the diagonal.)"
    },
    {
        "chapter": "Determinants",
        "question": "22. Prove that 4 is the largest determinant for a 3 by 3 matrix of 1s and −1s."
    },
    {
        "chapter": "Determinants",
        "question": "23. How many permutations of (1,2,3,4) are even and what are they? Extra credit: What are all the possible 4 by 4 determinants of I+P ? even Problems 24–33 use cofactorsC =(−1)i+jdetM . Delete row i, column j. ij ij"
    },
    {
        "chapter": "Determinants",
        "question": "24. Find cofactors and then transpose. MultiplyCT andCT by A and B! A B   (cid:34) (cid:35) 1 2 3 2 1   A= B=4 5 6. 3 6 7 0 0"
    },
    {
        "chapter": "Determinants",
        "question": "25. Find the cofactor matrixC and compare ACT with A−1:     2 −1 0 3 2 1   1  A=−1 2 −1 A−1 = 2 4 2. 4 0 −1 2 1 2 3"
    },
    {
        "chapter": "Determinants",
        "question": "4.3 FormulasfortheDeterminant 245"
    },
    {
        "chapter": "Determinants",
        "question": "26. The matrix B is the −1, 2, −1 matrix A except that b = 1 instead of a = 2. n n 11 11 Using cofactors of the last row of B , show that |B |=2|B |−|B |=1: 4 4 3 2     1 −1   1 −1 −1 2 −1    B =  B =−1 2 −1. 4 3  −1 2 −1 −1 2 −1 2 The recursion |B |=2|B |−|B | is the same as for the A’s. The difference is in n n−1 n−2 the starting values 1, 1, 1 for n=1,2,3. What are the pivots?"
    },
    {
        "chapter": "Determinants",
        "question": "27. B is still the same as A except for b = 1. So use linearity in the first row, where n n 11 [1 −1 0] equals [2 −1 0] minus [1 0 0]: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 1 −1 0 2 −1 0 1 0 0 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)−1 (cid:175) (cid:175)−1 (cid:175) (cid:175)−1 (cid:175) |B |=(cid:175) (cid:175)=(cid:175) (cid:175)−(cid:175) (cid:175). n (cid:175) A (cid:175) (cid:175) A (cid:175) (cid:175) A (cid:175) (cid:175) n−1 (cid:175) (cid:175) n−1 (cid:175) (cid:175) n−1 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 0 0 0 Linearity in row 1 gives |B |=|A |−|A |= . n n n−1"
    },
    {
        "chapter": "Determinants",
        "question": "28. The n by n determinantC has 1s above and below the main diagonal: n (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 0 1 0 0 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)0 1 0(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)0 1(cid:175) (cid:175) (cid:175) (cid:175)1 0 1 0(cid:175) C =(cid:175)0(cid:175) C =(cid:175) (cid:175) C =(cid:175)1 0 1(cid:175) C =(cid:175) (cid:175). 1 2 3 4 (cid:175)1 0(cid:175) (cid:175) (cid:175) (cid:175)0 1 0 1(cid:175) (cid:175)0 1 0(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 0 0 1 0 (a) What are the determinants ofC ,C ,C ,C ? 1 2 3 4 (b) By cofactors find the relation betweenC andC andC . FindC . n n−1 n−2 10"
    },
    {
        "chapter": "Determinants",
        "question": "29. Problem 28 has 1s just above and below the main diagonal. Going down the matrix, which order of columns (if any) gives all 1s? Explain why that permutation is even for n=4,8,12,... and odd for n=2,6,10,... C =0 (odd n) C =1 (n=4,8,...) C =−1 (n=2,6,...). n n n"
    },
    {
        "chapter": "Determinants",
        "question": "30. Explain why this Vandermonde determinant contains x3 but not x4 or x5:   1 a a2 a3   1 b b2 b3  V =det . 4 1 c c2 c3 1 x x2 x3 The determinant is zero at x = , , and . The cofactor of x3 is V = 3 (b−a)(c−a)(c−b). ThenV =(x−a)(x−b)(x−c)V . 4 3"
    },
    {
        "chapter": "Determinants",
        "question": "31. Compute the determinants S , S , S of these 1, 3, 1 tridiagonal matrices: 1 2 3 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)3 1 0(cid:175) (cid:175) (cid:175) (cid:175)3 1(cid:175) (cid:175) (cid:175) S (cid:175)3(cid:175) S =(cid:175) (cid:175) S =(cid:175)1 3 1(cid:175). 1 2 3 (cid:175)1 3(cid:175) (cid:175) (cid:175) (cid:175)0 1 3(cid:175) Make a Fibonacci guess for S and verify that you are right. 4"
    },
    {
        "chapter": "Determinants",
        "question": "32. Cofactors of those 1, 3, 1 matrices give S = 3S −S . Challenge: Show that n n−1 n−2 S is the Fibonacci number F by proving F = 3F −F . Keep using n 2n+2 2n+2 2n 2n−2 Fibonacci’s rule F =F +F . k k−1 k−2"
    },
    {
        "chapter": "Determinants",
        "question": "33. Change 3 to 2 in the upper left corner of the matrices in Problem 32. Why does that subtract S from the determinant S ? Show that the determinants become the n−1 n Fibonacci numbers 2, 5, 13 (always F ). 2n+1 Problems 34–36 are about block matrices and block determinants."
    },
    {
        "chapter": "Determinants",
        "question": "34. With 2 by 2 blocks, you cannot always use block determinants! (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)A B(cid:175) (cid:175)A B(cid:175) (cid:175) (cid:175)=|A||D| but (cid:175) (cid:175)(cid:54)=|A||D|−|C||B|. (cid:175)0 D(cid:175) (cid:175)C D(cid:175) (a) Why is the first statement true? Somehow B doesn’t enter. (b) Show by example that equality fails (as shown) whenC enters. (c) Show by example that the answer det(AD−CB) is also wrong."
    },
    {
        "chapter": "Determinants",
        "question": "35. With block multiplication, A=LU has A =L U in the upper left corner: k k k (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) A ∗ L 0 U ∗ k k k A= = . ∗ ∗ ∗ ∗ 0 ∗ (a) Suppose the first three pivots of A are 2, 3, −1. What are the determinants of L , 1 L , L (with diagonal 1s),U ,U ,U , and A , A , A ? 2 3 1 2 3 1 2 3 (b) If A , A , A have determinants 5, 6, 7, find the three pivots. 1 2 3"
    },
    {
        "chapter": "Determinants",
        "question": "36. Block elimination subtracts CA−1 times the first row [A B] from the second row [C D]. This leaves the Schur complement D−CA−1B in the corner: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) I 0 A B A B = . −CA−1 I C D 0 D−CA−1B Take determinants of these matrices to prove correct rules for square blocks: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)A B(cid:175) (cid:175) (cid:175) (cid:175) (cid:175)=|A|(cid:175) D−CA−1B(cid:175) =|AD−CB|. (cid:175)C D(cid:175) ifA−1exists ifAC=CA"
    },
    {
        "chapter": "Determinants",
        "question": "4.4 ApplicationsofDeterminants 247"
    },
    {
        "chapter": "Determinants",
        "question": "37. A 3 by 3 determinant has three products “down to the right” and three “down to the left” with minus signs. Compute the six terms in the figure to find D. Then explain without determinants why this matrix is or is not invertible:"
    },
    {
        "chapter": "Determinants",
        "question": "38. For A in Problem 6, five of the 4! = 24 terms in the big formula (6) are nonzero. 4 Find those five terms to show that D =−1. 4"
    },
    {
        "chapter": "Determinants",
        "question": "39. For the 4 by 4 tridiagonal matrix (entries −1, 2, −1), find the five terms in the big formula that give detA=16−4−4−4+1."
    },
    {
        "chapter": "Determinants",
        "question": "40. Find the determinant of this cyclic P by cofactors of row 1. How many exchanges reorder 4, 1, 2, 3 into 1, 2, 3, 4? Is |P2|=+1 or −1?     0 0 0 1 0 0 1 0 (cid:34) (cid:35)     1 0 0 0 0 0 0 1 0 I P=  P2 = = . 0 1 0 0 1 0 0 0 I 0 0 0 1 0 0 1 0 0"
    },
    {
        "chapter": "Determinants",
        "question": "41. A=2∗eye(n)−diag(ones(n−1, 1),1)−diag(ones(n−1, 1),−1) is the −1, 2, −1 matrix. Change A(1,1) to 1 so detA = 1. Predict the entries of A−1 based on n = 3 and test the prediction for n=4."
    },
    {
        "chapter": "Determinants",
        "question": "42. (MATLAB) The −1, 2, −1 matrices have determinant n+1. Compute (n+1)A−1 for n = 3 and 4, and verify your guess for n = 5. (Inverses of tridiagonal matrices have the rank-1 form uvT above the diagonal.)"
    },
    {
        "chapter": "Determinants",
        "question": "43. AllPascalmatriceshavedeterminant1. IfIsubtract1fromthen,nentry,whydoes the determinant become zero? (Use rule 3 or a cofactor.)     1 1 1 1 1 1 1 1     1 2 3 4  1 2 3 4  det =1(known) det =0(explain). 1 3 6 10 1 3 6 10 1 4 10 20 1 4 10 19"
    },
    {
        "chapter": "Determinants",
        "question": "4.4 Applications of Determinants This section follows through on four major applications: inverse of A, solving Ax = b, volumes of boxes, and pivots. They are among the key computations in linear algebra (done by elimination). Determinants give formulas for the answers."
    },
    {
        "chapter": "Determinants",
        "question": "1. Computation of A−1. The 2 by 2 case shows how cofactors go into A−1: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) −1 a b 1 d −b 1 C C 11 21 = = . c d ad−bc −c a detA C C 12 22 We are dividing by the determinant, and A is invertible exactly when detA is nonzero. ThenumberC =d isthecofactorofa. ThenumberC =−cisthecofactorofb(note 11 12 the minus sign). That numberC goes in row 2, column 1! 12 The row a, b times the columnC ,C produces ad−bc. This is the cofactor expan- 11 12 sion of detA. That is the clue we need: A−1 divides the cofactors by detA. Cofactor matrix CT C A−1 = means (A−1) = ji . (1) ij C is transposed detA detA Our goal is to verify this formula for A−1. We have to see why ACT =(detA)I:      a ··· a C ··· C detA ··· 0 11 1n 11 1n  . .  . .   . .   . . . .  . . . . = . . . . . (2) a ··· a C ··· C 0 ··· detA n1 nn n1 nn WithcofactorsC ,...,C inthefirstcolumnandnotthefirstrow,theymultiplya ,...,a 11 1n 11 1n and give the diagonal entry detA. Every row of A multiplies its cofactors (the cofactor expansion) to give the same answer detA on the diagonal. The critical question is: Why do we get zeros off the diagonal? If we combine the entries a from row 1 with the cofactorsC for row 2, why is the result zero? 1j 2j row 1 of A, row 2 ofC a C +a C +···+a C =0. (3) 11 21 12 22 1n 2n The answer is: We are computing the determinant of a new matrix B, with a new row 2. The first row of A is copied into the second row of B. Then B has two equal rows, and detB = 0. Equation (3) is the expansion of detB along its row 2, where B has exactly thesamecofactorsasA(becausethesecondrowisthrownawaytofindthosecofactors). The remarkable matrix multiplication (2) is correct. ThatmultiplicationACT=(detA)I immediatelygivesA−1. Rememberthatthecofac- tor from deleting row i and column j of A goes into row j and column i ofCT. Dividing by the number detA (if it is not zero!) gives A−1 =CT/detA. Example 1. The inverse of a sum matrix is a difference matrix:     1 1 1 1 −1 0   CT   A=0 1 1 has A−1 = =0 1 −1. detA 0 0 1 0 0 1 The minus signs enter because cofactors always include (−1)i+j."
    },
    {
        "chapter": "Determinants",
        "question": "4.4 ApplicationsofDeterminants 249"
    },
    {
        "chapter": "Determinants",
        "question": "2. The Solution of Ax = b. The multiplication x = A−1b is just CTb divided bydetA. There is a famous way in which to write the answer (x ,...,x ): 1 n 4C Cramer’s rule: The jth component of x=A−1b is the ratio   a a b a 11 12 1 1n x j = detB j , where B j =  . . . . . . . . . . . .   has b in column j. (4) detA a a b a n1 n2 n nn Proof. Expand detB in cofactors of its jth column (which is b). Since the cofactors j ignore that column, detB is exactly the jth component in the productCTb: j detB =b C +b C +···+b C . j 1 1j 2 2j n nj DividingthisbydetAgivesx . Eachcomponentofxisaratiooftwodeterminants. That j fact might have been recognized from Gaussian elimination, but it never was. Example 2. The solution of x + 3x = 0 1 2 2x + 4x = 6 1 2 has 0 and 6 in the first column for x and in the second column for x : 1 2 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)0 3(cid:175) (cid:175)1 0(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)6 4(cid:175) (cid:175)2 6(cid:175) −18 6 x = (cid:175) (cid:175) = =9, x = (cid:175) (cid:175) = =−3. 1 (cid:175) (cid:175) −2 2 (cid:175) (cid:175) −2 (cid:175)1 3(cid:175) (cid:175)1 3(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)2 4(cid:175) (cid:175)2 4(cid:175) ThedenominatorsarealwaysdetA. For1000equationsCramer’sRulewouldneed1001 determinants. To my dismay I found in a book called Mathematics for the Millions that Cramer’s Rule was actually recommended (and elimination was thrown aside): To deal with a set involving the four variables u, v, w, z, we first have to eliminate one of them in each of three pairs to derive three equations in three variables and then proceed as for the three-fold left-hand set to derive values for two of them. The reader who does so as an exercise will begin to realize how formidably laborious the method of elimination becomes, when we have to deal with more than three variables. This consideration invites us to explore the possibility of a speedier method..."
    },
    {
        "chapter": "Determinants",
        "question": "3. The Volume of a Box. The connection between the determinant and the volume is clearest when all angles are right angles—the edges are perpendicular, and the box is rectangular. Then the volume is the product of the edge lengths: volume=(cid:96) (cid:96) ···(cid:96) . 1 2 n We want to obtain the same (cid:96) (cid:96) ···(cid:96) from detA, when the edges of that box are the 1 2 n rows of A. With right angles, these rows are orthogonal and AAT is diagonal:       row 1 r r (cid:96)2 0 o o 1 Right-angled box AAT =  . . .  w ··· w=  ...  . Orthogonal rows row n 1 n 0 (cid:96)2 n The (cid:96) are the lengths of the rows (the edges). and the zeros off the diagonal come i because the rows are orthogonal. Using the product and transposing rules, Rightangle case (cid:96)2(cid:96)2···(cid:96)2 =det(AAT)=(detA)(detAT)=(detA)2. 1 2 n The square root of this equation says that the determinant equals the volume. The sign of detA will indicate whether the edges form a “right-handed” set of coordinates, as in the usual x-y-z system, or a left-handed system like y-x-z. If the angles are not 90°, the volume is not the product of the lengths. In the plane (Figure 4.2), the “volume” of a parallelogram equals the base (cid:96) times the height h, The vector b−p of length h is the second row b=(a ,a ), minus its projection p onto the 21 22 first row. The key point is this: By rule 5, detA is unchanged when a multiple of row 1 is subtracted from row 2. We can change the parallelogram to a rectangle, where it is already proved that volume = determinant. In n dimensions, it takes longer to make each box rectangular, but the idea is the same. The volume and determinant are unchanged if we subtract from each row its pro- jection onto the space spanned by the preceding rows—leaving a perpendicular “height vector” like pb. This Gram-Schmidt process produces orthogonal rows, with volume = determinant. So the same equality must have held for the original rows. b=(a21,a22) a a height `h=det(cid:20) b−p(cid:21)=det(cid:20) b(cid:21) h= b p | − | a=(a11,a12) p length `= a | | 0 Figure4.2: Volume(area)oftheparallelogram=(cid:96)timesh=|detA|. This completes the link between volumes and determinants, but it is worth coming back one more time to the simplest case. We know that (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 0 det =1, det =1. 0 1 c 1"
    },
    {
        "chapter": "Determinants",
        "question": "4.4 ApplicationsofDeterminants 251 Thesedeterminantsgivethevolumes—orareas,sinceweareintwodimensions—drawn in Figure 4.3. The parallelogram has unit base and unit height; its area is also 1. row 2 = (0,1) row 2 = (c,1) 1 1 row 1 = (1,0) row 1 = (1,0) 1 1 Figure4.3: Theareasofaunitsquareandaunitparallelogramareboth1."
    },
    {
        "chapter": "Determinants",
        "question": "4. A Formula for the Pivots. We can finally discover when elimination is possible without row exchanges. The key observation is that the first k pivots are completely determined by the submatrix A in the upper left corner of A. The remaining rows and k columns of A have no effect on this corner of the problem:     Elimination on A a b e a b e     includes A=c d f→0 (ad−bc)/a (af −ec)/a. elimination on A g h i g h i 2 Certainly the first pivot depended only on the first row and column, The second pivot (ad−bc)/adependsonlyonthe2by2cornersubmatrixA . TherestofAdoesnotenter 2 until the third pivot. Actually it is not just the pivots, but the entire upper-left corners of L, D, andU, that are determined by the upper-left corner of A:     1 a 1 b/a ∗     A=LDU =c/a 1  (ad−bc)/a  1 ∗. ∗ ∗ 1 ∗ 1 What we see in the first two rows and columns is exactly the factorization of the corner submatrix A . This is a general rule if there are no row exchanges: 2 4D IfAisfactoredintoLDU, theupperleftcornerssatisfyA =L D U . For k k k k every k, the submatrix A is going through a Gaussian elimination of its own. k The proof is to see that this corner can be settled first, before even looking at other eliminations. Or use the laws for block multiplication: (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) L 0 D 0 U F L D U L D F k k k k k k k k LDU = = . B C 0 E 0 G BD U BD F+CEG k k k Comparing the last matrix with A, the corner L D U coincides with A . Then: k k k k detA =detL detD detU =detD =d d ···d . k k k k k 1 2 k The product of the first k pivots is the determinant of A . This is the same rule that k we know already for the whole matrix. Since the determinant of A will be given by k−1 d d ···d , we can isolate each pivot d as a ratio of determinants: 1 2 k−1 k detA d d ···d k 1 2 k Formula for pivots = =d . (5) k detA d d ···d k−1 1 2 k−1 In our example above, the second pivot was exactly this ratio (ad −bc)/a. It is the determinant of A divided by the determinant of A . (By convention detA = 1, so that 2 1 0 the first pivot is a/1=a.) Multiplying together all the individual pivots, we recover detA detA detA detA 1 2 n n d d ···d = ··· = =detA. 1 2 n detA detA detA detA 0 1 n−1 0 From equation (5) we can finally read off the answer to our original question: The pivot entries are all nonzero whenever the numbers detA are all nonzero: k 4E Elimination can be completed without row exchanges (so P=I and A= LU), if and only if the leading submatrices A ,A ,...,A are all nonsingular. 1 2 n That does it for determinants, except for an optional remark on property 2—the sign reversal on row exchanges. The determinant of a permutation matrix P was the only questionablepointinthebigformula. Independentoftheparticularrowexchangeslink- ing P to I, is the number of exchanges always even or always odd? If so, its determinant is well defined by rule 2 as either +1 or −1. Starting from (3,2,1), a single exchange of 3 and 1 would achieve the natural order (1,2,3). So would an exchange of 3 and 2, then 3 and 1, and then 2 and 1. In both sequences, the number of exchanges is odd. The assertion is that an even number of exchanges can never produce the natural order beginning with (3,2,1). Here is a proof. Look at each pair of numbers in the permutation, and let N count the pairs in which the larger number comes first. Certainly N = 0 for the natural order (1,2,3). The order (3,2,1) has N =3 since all pairs (3,2), (3,1), and (2,1) are wrong. We will show that every exchange alters N by an odd number. Then to arrive at N = 0 (the natural order) takes a number of exchanges having the same evenness or oddness as N. When neighbors are exchanged, N changes by +1 or −1. Any exchange can be achieved by an odd number of exchanges of neighbors. This will complete the proof; an odd number of odd numbers is odd. To exchange the first and fourth entries below, which happen to be 2 and 3, we use five exchanges (an odd number) of neighbors: (2,1,4,3)→(1,2,4,3)→(1,4,2,3)→(1,4,3,2)→(1,3,4,2)→(3,1,4,2). We need (cid:96)−k exchanges of neighbors to move the entry in place k to place (cid:96). Then (cid:96)−k−1 exchanges move the one originally in place (cid:96) (and now found in place (cid:96)−1) back down to place k. Since ((cid:96)−k)+((cid:96)−k−1) is odd, the proof is complete. The determinant not only has all the properties found earlier, it even exists."
    },
    {
        "chapter": "Determinants",
        "question": "1. Find the determinant and all nine cofactorsC of this triangular matrix: ij   1 2 3   A=0 4 0. 0 0 5 FormCT and verify that ACT =(detA)I. What is A−1?"
    },
    {
        "chapter": "Determinants",
        "question": "2. Use the cofactor matrixC to invert these symmetric matrices:     2 −1 0 1 1 1     A=−1 2 −1 and B=1 2 2. 0 −1 2 1 2 3"
    },
    {
        "chapter": "Determinants",
        "question": "3. Find x, y, and z by Cramer’s Rule in equation (4): x + 4y − z = 1 ax + by = 1 and x + y + z = 0 cx + dy = 0 2x + 3z = 0."
    },
    {
        "chapter": "Determinants",
        "question": "4. (a) Find the determinant when a vector x replaces column j of the identity (consider x =0 as a separate case): j   1 x 1    1 ·    if M = x  then detM = . j    · 1  x 1 n (b) If Ax=b, show that AM is the matrix B in equation (4), with b in column j. j (c) Derive Cramer’s rule by taking determinants in AM =B . j"
    },
    {
        "chapter": "Determinants",
        "question": "5. (a) Draw the triangle with vertices A = (2,2), B = (−1,3), and C = (0,0). By regarding it as half of a parallelogram, explain why its area equals (cid:34) (cid:35) 1 2 2 area(ABC)= det . 2 −1 3 (b) Move the third vertex toC =(1,−4) and justify the formula     x y 1 2 2 1 1 1 1   1   area(ABC)= detx y 1= det−1 3 1. 2 2 2 2 x y 1 1 −4 1 3 3 Hint: Subtracting the last row from each of the others leaves     (cid:34) (cid:35) 2 2 1 1 6 0     1 6 det−1 3 1=det−2 7 0=det . −2 7 1 −4 1 1 −4 1 Sketch A(cid:48) =(1,6), B(cid:48) =(−2,7),C(cid:48) =(0,0) and their relation to A, B,C."
    },
    {
        "chapter": "Determinants",
        "question": "6. Explain in terms of volumes why det3A=3ndetA for an n by n matrix A."
    },
    {
        "chapter": "Determinants",
        "question": "7. Predict in advance, and confirm by elimination, the pivot entries of     2 1 2 2 1 2     A=4 5 0 and B=4 5 3. 2 7 0 2 7 0"
    },
    {
        "chapter": "Determinants",
        "question": "8. Find all the odd permutations of the numbers {1,2,3,4}. They come from an odd number of exchanges and lead to detP=−1."
    },
    {
        "chapter": "Determinants",
        "question": "9. Suppose the permutation P takes (1,2,3,4,5) to (5,4,1,2,3). (a) What does P2 do to (1,2,3,4,5)? (b) What does P−1 do to (1,2,3,4,5)?"
    },
    {
        "chapter": "Determinants",
        "question": "10. If P is an odd permutation, explain why P2 is even but P−1 is odd."
    },
    {
        "chapter": "Determinants",
        "question": "11. Prove that if you keep multiplying A by the same permutation matrix P, the first row eventually comes back to its original place."
    },
    {
        "chapter": "Determinants",
        "question": "12. If A is a 5 by 5 matrix with all |a | ≤ 1, then detA ≤ . Volumes or the big ij formula or pivots should give some upper bound on the determinant. Problems 13–17 are about Cramer’s Rule for x=A−1b."
    },
    {
        "chapter": "Determinants",
        "question": "13. Solve these linear equations by Cramer’s Rule x =detB /detA: j j 2x + x = 1 1 2 2x + 5x = 1 1 2 (a) (b) x + 2x + x = 70 1 2 3 x + 4x = 2. 1 2 x + 2x = 0. 2 3"
    },
    {
        "chapter": "Determinants",
        "question": "14. Use Cramer’s Rule to solve for y (only). Call the 3 by 3 determinant D: ax + by + cz = 1 ax + by = 1 (a) (b) dx + ey − fz = 0 cx + dy = 0. gx + hy + iz = 0."
    },
    {
        "chapter": "Determinants",
        "question": "4.4 ApplicationsofDeterminants 255"
    },
    {
        "chapter": "Determinants",
        "question": "15. Cramer’s Rule breaks down when detA = 0. Example (a) has no solution, whereas (b) has infinitely many. What are the ratios x =detB /detA? j j 2x +3x =1 2x +3x =1 1 2 1 2 (a) (parallel lines) (b) (same line) 4x +6x =1. 4x +6x =2. 1 2 1 2"
    },
    {
        "chapter": "Determinants",
        "question": "16. Quick proof of Cramer’s rule. The determinant is a linear function of column 1. It is zeroiftwocolumnsareequal. Whenb=Ax=x a +x a +x a goesintocolumn 1 1 2 2 3 3 1 to produce B , the determinant is 1 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)b a a (cid:175)=(cid:175)x a +x a +x a a a (cid:175)=x (cid:175)a a a (cid:175)=x detA. 2 3 1 1 2 2 3 3 2 3 1 1 2 3 1 (a) What formula for x comes from left side = right side? 1 (b) What steps lead to the middle equation?"
    },
    {
        "chapter": "Determinants",
        "question": "17. If the right side b is the last column of A, solve the 3 by 3 system Ax = b. Explain how each determinant in Cramer’s Rule leads to your solution x. Problems 18–26 are about A−1 =CT/detA. Remember to transposeC."
    },
    {
        "chapter": "Determinants",
        "question": "18. Find A−1 from the cofactor formulaCT/detA. Use symmetry in part (b):     1 2 0 2 −1 0     (a) A=0 3 0. (b) A=−1 2 −1. 0 4 1 0 −1 2"
    },
    {
        "chapter": "Determinants",
        "question": "19. If all the cofactors are zero, how do you know that A has no inverse? If none of the cofactors are zero, is A sure to be invertible?"
    },
    {
        "chapter": "Determinants",
        "question": "20. Find the cofactors of A and multiply ACT to find detA:     1 1 4 6 −3 0     A=1 2 2, C =· · ·, and ACT = . 1 2 5 · · · If you change that corner entry from 4 to 100, why is detA unchanged?"
    },
    {
        "chapter": "Determinants",
        "question": "21. Suppose detA=1 and you know all the cofactors. How can you find A?"
    },
    {
        "chapter": "Determinants",
        "question": "22. From the formula ACT =(detA)I show that detC =(detA)n−1."
    },
    {
        "chapter": "Determinants",
        "question": "23. (For professors only) If you know all 16 cofactors of a 4 by 4 invertible matrix A, how would you find A?"
    },
    {
        "chapter": "Determinants",
        "question": "24. If all entries of A are integers, and detA = 1 or −1, prove that all entries of A−1 are integers. Give a 2 by 2 example."
    },
    {
        "chapter": "Determinants",
        "question": "25. L is lower triangular and S is symmetric. Assume they are invertible:     a 0 0 a b d     L=b c 0 S=b c e. d e f d e f (a) Which three cofactors of L are zero? Then L−1 is lower triangular. (b) Which three pairs of cofactors of S are equal? Then S−1 is symmetric."
    },
    {
        "chapter": "Determinants",
        "question": "26. For n = 5 the matrix C contains cofactors and each 4 by 4 cofactor contains terms and each term needs multiplications. Compare with 53 = 125 for the Gauss-Jordan computation of A−1. Problems 27–36 are about area and volume by determinants."
    },
    {
        "chapter": "Determinants",
        "question": "27. (a) Find the area of the parallelogram with edges v=(3,2) and w=(1,4). (b) Find the area of the triangle with sides v, w, and v+w. Draw it. (c) Find the area of the triangle with sides v, w, and w−v. Draw it."
    },
    {
        "chapter": "Determinants",
        "question": "28. A box has edges from (0,0,0) to (3,1,1), (1,3,1), and (1,1,3). Find its volume and also find the area of each parallelogram face."
    },
    {
        "chapter": "Determinants",
        "question": "29. (a) The corners of a triangle are (2,1), (3,4), and (0,5). What is the area? (b) A new corner at (−1,0) makes it lopsided (four sides). Find the area."
    },
    {
        "chapter": "Determinants",
        "question": "30. The parallelogram with sides (2,1) and (2,3) has the same area as the parallelogram with sides (2,2) and (1,3). Find those areas from 2 by 2 determinants and say why they must be equal. (I can’t see why from a picture. Please write to me if you do.)"
    },
    {
        "chapter": "Determinants",
        "question": "31. The Hadamard matrix H has orthogonal rows. The box is a hypercube! (cid:175) (cid:175) (cid:175) (cid:175) 1 1 1 1 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)1 1 −1 −1(cid:175) What is detH =(cid:175) (cid:175)=volume of a hypercube in R4? (cid:175)1 −1 −1 1 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) 1 −1 1 −1"
    },
    {
        "chapter": "Determinants",
        "question": "32. If the columns of a 4 by 4 matrix have lengths L , L , L , L , what is the largest 1 2 3 4 possible value for the determinant (based on volume)? If all entries are 1 or −1, what are those lengths and the maximum determinant?"
    },
    {
        "chapter": "Determinants",
        "question": "33. Show by a picture how a rectangle with area x y minus a rectangle with area x y 1 2 2 1 produces the area x y −x y of a parallelogram. 1 2 2 1"
    },
    {
        "chapter": "Determinants",
        "question": "34. When the edge vectors a, b, c are perpendicular, the volume of the box is (cid:107)a(cid:107) times (cid:107)b(cid:107) times (cid:107)c(cid:107). The matrix ATA is . Find detATA and detA."
    },
    {
        "chapter": "Determinants",
        "question": "4.4 ApplicationsofDeterminants 257"
    },
    {
        "chapter": "Determinants",
        "question": "35. An n-dimensional cube has how many corners? How many edges? How many (n− 1)-dimensional faces? The n-cube whose edges are the rows of 2I has volume . A hypercube computer has parallel processors at the corners with connections along the edges."
    },
    {
        "chapter": "Determinants",
        "question": "36. The triangle with corners (0,0), (1,0), (0,1) has area 1. The pyramid with four 2 corners (0,0,0), (1,0,0), (0,1,0), (0,0,1) has volume . The pyramid in R4 with five corners at (0,0,0,0) and the rows of I has what volume? Problems 37–40 are about areas dA and volumes dV in calculus."
    },
    {
        "chapter": "Determinants",
        "question": "37. Polar coordinates satisfy x=rcosθand y=rsinθ. Polar area J dr dθincludes J: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)∂x/∂r ∂x/∂θ(cid:175) (cid:175)cosθ −rsinθ(cid:175) J =(cid:175) (cid:175)=(cid:175) (cid:175). (cid:175)∂y/∂r ∂y/∂θ(cid:175) (cid:175)sinθ rcosθ (cid:175) The two columns are orthogonal. Their lengths are . Thus J = ."
    },
    {
        "chapter": "Determinants",
        "question": "38. Spherical coordinates ρ, φ, θ give x = ρsinφcosθ, y = ρsinφsinθ, z = ρcosφ. Find the Jacobian matrix of 9 partial derivatives: ∂x/∂ρ,∂x/∂φ,∂x/∂θ are in row"
    },
    {
        "chapter": "Determinants",
        "question": "1. Simplify its determinant to J =ρ2sinφ. Then dV =ρ2sinφdρdφdθ."
    },
    {
        "chapter": "Determinants",
        "question": "39. The matrix that connects r,θto x, y is in Problem 37. Invert that matrix: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)∂r/∂x ∂r/∂y(cid:175) (cid:175)cosθ ?(cid:175) J−1 =(cid:175) (cid:175)=(cid:175) (cid:175)=? (cid:175)∂θ/∂x ∂θ/∂y(cid:175) (cid:175) ? ?(cid:175) It is surprising that∂r/∂x=∂x/∂r. The product JJ−1 =I gives the chain rule ∂x ∂x∂r ∂x ∂θ = + =1. ∂x ∂r∂x ∂θ∂x"
    },
    {
        "chapter": "Determinants",
        "question": "40. The triangle with corners (0,0), (6,0), and (1,4) has area . When you rotate it byθ=60° the area is . The rotation matrix has (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)cosθ −sinθ(cid:175) (cid:175)1 ?(cid:175) determinant=(cid:175) (cid:175)=(cid:175)2 (cid:175)=? (cid:175)sinθ cosθ (cid:175) (cid:175)? ?(cid:175)"
    },
    {
        "chapter": "Determinants",
        "question": "41. Let P = (1,0,−1), Q = (1,1,1), and R = (2,2,1). Choose S so that PQRS is a parallelogram, and compute its area. Choose T,U,V so that OPQRSTUV is a tilted box, and compute its volume."
    },
    {
        "chapter": "Determinants",
        "question": "42. Suppose (x,y,z), (1,1,0), and (1,2,1) lie on a plane through the origin. What deter- minant is zero? What equation does this give for the plane?"
    },
    {
        "chapter": "Determinants",
        "question": "43. Suppose (x,y,z) is a linear combination of (2,3,1) and (1,2,3). What determinant is zero? What equation does this give for the plane of all combinations?"
    },
    {
        "chapter": "Determinants",
        "question": "44. If Ax=(1,0,...,0) show how Cramer’s Rule gives x= first column of A−1."
    },
    {
        "chapter": "Determinants",
        "question": "45. (VISAtoAVIS)Thistakesanoddnumberofexchanges(IVSA,AVSI,AVIS).Count the pairs of letters in VISA and AVIS that are reversed from alphabetical order. The difference should be odd. Review Exercises"
    },
    {
        "chapter": "Determinants",
        "question": "4.1 Find the determinants of     1 1 1 1 2 −1 0 −1     1 1 1 2 −1 2 −1 0    and  . 1 1 3 1  0 −1 2 −1 1 4 1 1 −1 0 −1 2"
    },
    {
        "chapter": "Determinants",
        "question": "4.2 If B=M−1AM, why is detB=detA? Show also that detA−1B=1."
    },
    {
        "chapter": "Determinants",
        "question": "4.3 Starting with A, multiply its first row by 3 to produce B, and subtract the first row of B from the second to produceC. How is detC related to detA?"
    },
    {
        "chapter": "Determinants",
        "question": "4.4 Solve 3u+2v=7, 4u+3v=11 by Cramer’s rule."
    },
    {
        "chapter": "Determinants",
        "question": "4.5 IftheentriesofAandA−1 areallintegers,howdoyouknowthatbothdeterminants are 1 or −1? Hint: What is detA times detA−1?"
    },
    {
        "chapter": "Determinants",
        "question": "4.6 Find all the cofactors, and the inverse or the nullspace, of (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 3 5 cosθ −sinθ a b , , and . 6 9 sinθ cosθ a b"
    },
    {
        "chapter": "Determinants",
        "question": "4.7 Whatisthevolumeoftheparallelepipedwithfourofitsverticesat(0,0,0),(−1,2,2), (2,−1,2), and (2,2,−1)? Where are the other four vertices?"
    },
    {
        "chapter": "Determinants",
        "question": "4.8 How many terms are in the expansion of a 5 by 5 determinant, and how many are sure to be zero if a =0? 21"
    },
    {
        "chapter": "Determinants",
        "question": "4.9 If P is an even permutation matrix and P is odd, deduce from P +P = P (PT+ 1 2 1 2 1 1 PT)P that det(P +P )=0. 2 2 1 2"
    },
    {
        "chapter": "Determinants",
        "question": "4.10 If detA > 0, show that A can be connected to I by a continuous chain of matrices A(t) all with positive determinants. (The straight path A(t) = A+t(I−A) does go from A(0)= A to A(1)= I, but in between A(t) might be singular. The problem is not so easy, and solutions are welcomed by the author.)"
    },
    {
        "chapter": "Determinants",
        "question": "4.4 ApplicationsofDeterminants 259"
    },
    {
        "chapter": "Determinants",
        "question": "4.11 Explain why the point (x,y) is on the line through (2,8) and (4,7) if   x y 1   det2 8 1=0, or x+2y−18=0. 4 7 1"
    },
    {
        "chapter": "Determinants",
        "question": "4.12 In analogy with the previous exercise, what is the equation for (x,y,z) to be on the plane through (2,0,0), (0,2,0), and (0,0,4)? It involves a 4 by 4 determinant."
    },
    {
        "chapter": "Determinants",
        "question": "4.13 If the points (x,y,z), (2,1,0), and (1,1,1) lie on a plane through the origin, what determinant is zero? Are the vectors (1,0,−1), (2,1,0), (1,1,1) independent?"
    },
    {
        "chapter": "Determinants",
        "question": "4.14 If every row of A has either a single +1, or a single −1, or one of each (and is otherwise zero), show that detA=1 or −1 or 0. (cid:163) (cid:164)"
    },
    {
        "chapter": "Determinants",
        "question": "4.15 IfC = a b and D=[u v], thenCD=−DC yields 4 equations Ax=0: c d w z      2a c b 0 u 0       b a+d 0 b v 0 CD+DC =0 is   = .  c 0 a+d c w 0 0 c b 2d z 0 (a) Show that detA=0 if a+d =0. Solve for u, v, w, z, the entries of D. (b) Show that detA=0 if ad =bc (soC is singular). In all other cases,CD=−DC is only possible with D= zero matrix."
    },
    {
        "chapter": "Determinants",
        "question": "4.16 Thecircularshiftpermutes(1,2,...,n)into(2,3,...,1). Whatisthecorresponding permutation matrix P, and (depending on n) what is its determinant?"
    },
    {
        "chapter": "Determinants",
        "question": "4.17 Find the determinant of A = eye(5) + ones(5) and if possible eye(n) + ones(n). 5 Chapter Eigenvalues and Eigenvectors"
    },
    {
        "chapter": "Determinants",
        "question": "5.1 Introduction This chapter begins the “second half” of linear algebra. The first half was about Ax = b. The new problem Ax =λx will still be solved by simplifying a matrix—making it diagonal if possible. The basic step is no longer to subtract a multiple of one row from another: Elimination changes the eigenvalues, which we don’t want. DeterminantsgiveatransitionfromAx=btoAx=λx. Inbothcasesthedeterminant leads to a “formal solution”: to Cramer’s rule for x = A−1b, and to the polynomial det(A−λI), whose roots will be the eigenvalues. (All matrices are now square; the eigenvalues of a rectangular matrix make no more sense than its determinant.) The determinantcanactuallybeusedifn=2or3. Forlargen,computingλismoredifficult than solving Ax=b. Thefirststepistounderstandhoweigenvaluescanbeuseful,Oneoftheirapplications is to ordinary differential equations. We shall not assume that the reader is an expert on differential equations! If you can differentiate xn, sinx, and ex, you know enough. As a specific example, consider the coupled pair of equations dv =4v−5w, v=8 at t =0, dt (1) dw =2v−3w, w=5 at t =0. dt This is an initial-value problem. The unknown is specified at time t = 0 by the given initial values 8 and 5. The problem is to find v(t) and w(t) for later timest >0. It is easy to write the system in matrix form. Let the unknown vector be u(t), with initial value u(0). The coefficient matrix is A: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) v(t) 8 4 −5 Vector unknown u(t)= , u(0)= , A= . w(t) 5 2 −3 The two coupled equations become the vector equation we want: du Matrix form =Au with u=u(0) at t =0. (2) dt"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.1 Introduction 261 This is the basic statement of the problem. Note that it is a first-order equation—no higher derivatives appear—and it is linear in the unknowns, It also has constant coeffi- cients; the matrix A is independent of time. How do we find u(t)? If there were only one unknown instead of two, that question would be easy to answer. We would have a scalar instead of a vector equation: du Single equation =au with u=u(0) at t =0. (3) dt The solution to this equation is the one thing you need to know: Pure exponential u(t)=eatu(0). (4) At the initial time t = 0, u equals u(0) because e0 = 1. The derivative of eat has the required factor a, so that du/dt = au. Thus the initial condition and the equation are both satisfied. Notice the behavior of u for large times. The equation is unstable if a > 0, neutrally stable if a = 0, or stable if a < 0; the factor eat approaches infinity, remains bounded, or goes to zero. If a were a complex number, a =α+iβ, then the same tests would be appliedtotherealpartα. Thecomplexpartproducesoscillationseiβt =cosβt+isinβt. Decay or growth is governed by the factor eαt. So much for a single equation. We shall take a direct approach to systems, and look for solutions with the same exponential dependence ont just found in the scalar case: v(t)=eλty (5) w(t)=eλtz or in vector notation u(t)=eλtx. (6) This is the whole key to differential equations du/dt = Au: Look for pure exponential solutions. Substituting v=eλty and w=eλtz into the equation, we find λeλty=4eλty−5eλtz λeλtz=2eλty−3eλtz. The factor eλt is common to every term, and can be removed. This cancellation is the reason for assuming the same exponentλ for both unknowns; it leaves 4y−5z=λy Eigenvalue problem (7) 2y−3z=λz. That is the eigenvalue equation. In matrix form it is Ax=λx. You can see it again if we use u = eλtx—a number eλt that grows or decays times a fixed vector x. Substituting into du/dt =Au givesλeλtx=Aeλtx. The cancellation of eλt produces Eigenvalue equation Ax=λx. (8) Now we have the fundamental equation of this chapter. It involves two unknowns λ and x. It is an algebra problem, and differential equations can be forgotten! The number λ (lambda) is an eigenvalue of the matrix A, and the vector x is the associated eigenvector. Our goal is to findthe eigenvaluesand eigenvectors,λ’sand x’s, and to use them. The Solution of Ax=λx NoticethatAx=λxisanonlinearequation;λmultipliesx. Ifwecoulddiscoverλ,then the equation for x would be linear. In fact we could write λIx in place of λx, and bring this term over to the left side: (A−λI)x=0. (9) The identity matrix keeps matrices and vectors straight; the equation (A−λ)x = 0 is shorter, but mixed up. This is the key to the problem: The vector x is in the nullspace of A−λI. The numberλ is chosen so that A−λI has a nullspace. Of course every matrix has a nullspace. It was ridiculous to suggest otherwise, but you see the point. We want a nonzero eigenvector x, The vector x = 0 always satisfies Ax =λx, but it is useless in solving differential equations. The goal is to build u(t) out of exponentials eλtx, and we are interested only in those particular values λ for which there is a nonzero eigenvector x. To be of any use, the nullspace of A−λI must contain vectors other than zero. In short, A−λI must be singular. For this, the determinant gives a conclusive test. 5A The numberλ is an eigenvalue of A if and only if A−λI is singular: det(A−λI)=0. (10) This is the characteristic equation. Eachλ is associated with eigenvectors x: (A−λI)x=0 or Ax=λx. (11) In our example, we shift A byλI to make it singular: (cid:34) (cid:35) 4−λ −5 SubtractλI A−λI = . 2 −3−λ Note thatλ is subtracted only from the main diagonal (because it multiplies I). Determinant |A−λI|=(4−λ)(−3−λ)+10 or λ2−λ−2. This is the characteristic polynomial. Its roots, where the determinant is zero, are the eigenvalues. They come from the general formula for the roots of a quadratic, or from"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.1 Introduction 263 factoring into λ2−λ−2 = (λ+1)(λ−2). That is zero if λ = −1 or λ = 2, as the general formula confirms: √ √ −b± b2−4ac 1± 9 Eigenvalues λ= = =−1 and 2. 2a 2 There are two eigenvalues, because a quadratic has two roots. Every 2 by 2 matrix A−λI hasλ2 (and no higher power ofλ) in its determinant. Thevaluesλ=−1andλ=2leadtoasolutionofAx=λxor(A−λI)x=0. Amatrix with zero determinant is singular, so there must be nonzero vectors x in its nullspace. In fact the nullspace contains a whole line of eigenvectors; it is a subspace! (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 5 −5 y 0 λ =−1: (A−λ I)x= = . 1 1 2 −2 z 0 The solution (the first eigenvector) is any nonzero multiple of x : 1 (cid:34) (cid:35) 1 Eigenvector forλ x = . 1 1 1 The computation forλ is done separately: 2 (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 2 −5 y 0 λ =2: (A−λ I)x= = . 2 2 2 −5 z 0 The second eigenvector is any nonzero multiple of x : 2 (cid:34) (cid:35) 5 Eigenvector forλ x = . 2 2 2 You might notice that the columns of A−λ I give x , and the columns of A−λ I are 1 2 2 multiples of x . This is special (and useful) for 2 by 2 matrices. 1 In the 3 by 3 case, I often set a component of x equal to 1 and solve (A−λI)x=0 for the other components. Of course if x is an eigenvector then so is 7x and so is −x. All vectors in the nullspace of A−λI (which we call the eigenspace) will satisfy Ax =λx. In our example the eigenspaces are the lines through x =(1,1) and x =(5,2). 1 2 Before going back to the application (the differential equation), we emphasize the steps in solving Ax=λx:"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. Compute the determinant of A−λI. With λ subtracted along the diagonal, this determinant is a polynomial of degree n. It starts with (−λ)n."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. Find the roots of this polynomial. The n roots are the eigenvalues of A."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. For each eigenvalue solve the equation (A−λI)x = 0. Since the determinant is zero, there are solutions other than x=0. Those are the eigenvectors. In the differential equation, this produces the special solutions u = eλtx. They are the pure exponential solutions to du/dt =Au. Notice e−t and e2t: (cid:34) (cid:35) (cid:34) (cid:35) 1 5 u(t)=eλ 1tx =e−t and u(t)=eλ 2tx =e2t . 1 2 1 2 These two special solutions give the complete solution. They can be multiplied by any numbers c and c , and they can be added together. When u and u satisfy the linear 1 2 1 2 equation du/dt =Au, so does their sum u +u : 1 2 Complete solution u(t)=c eλ 1tx +c eλ 2tx (12) 1 1 2 2 This is superposition, and it applies to differential equations (homogeneous and linear) just as it applied to matrix equations Ax = 0. The nullspace is always a subspace, and combinations of solutions are still solutions. Nowwehavetwofreeparametersc andc ,anditisreasonabletohopethattheycan 1 2 be chosen to satisfy the initial condition u=u(0) att =0: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 1 5 c 8 1 Initial condition c x +c x =u(0) or = . (13) 1 1 2 2 1 2 c 5 2 The constants are c =3 and c =1, and the solution to the original equation is 1 2 (cid:34) (cid:35) (cid:34) (cid:35) 1 5 u(t)=3e−t +e2t . (14) 1 2 Writing the two components separately, we have v(0)=8 and w(0)=5: Solution v(t)=3e−t+5e2t, w(t)=3e−t+2e2t. The key was in the eigenvalues λ and eigenvectors x. Eigenvalues are important in themselves, and not just part of a trick for finding u. Probably the homeliest example is that of soldiers going over a bridge.1 Traditionally, they stop marching and just walk across. If they happen to march at a frequency equal to one of the eigenvalues of the bridge, it would begin to oscillate. (Just as a child’s swing does; you soon notice the natural frequency of a swing, and by matching it you make the swing go higher.) An engineer tries to keep the natural frequencies of his bridge or rocket away from those of the wind or the sloshing of fuel. And at the other extreme, a stockbroker spends his life trying to get in line with the natural frequencies of the market. The eigenvalues are the most important feature of practically any dynamical system. Summary and Examples To summarize, this introduction has shown how λ and x appear naturally and auto- matically when solving du/dt = Au. Such an equation has pure exponential solutions 1OnewhichIneverreallybelieved—butabridgedidcrashthiswayin1831."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.1 Introduction 265 u=eλtx;theeigenvaluegivestherateofgrowthordecay,andtheeigenvectorxdevelops at this rate. The other solutions will be mixtures of these pure solutions, and the mixture is adjusted to fit the initial conditions. The key equation was Ax = λx. Most vectors x will not satisfy such an equation. They change direction when multiplied by A, so that Ax is not a multiple of x. This means that only certain special numbers are eigenvalues, and only certain special vectors x are eigenvectors. We can watch the behavior of each eigenvector, and then combine these “normal modes” to find the solution. To say the same thing in another way, the underlying matrix can be diagonalized. The diagonalization in Section 5.2 will be applied to difference equations, Fibonacci numbers, and Markov processes, and also to differential equations. In every example, we start by computing the eigenvalues and eigenvectors; there is no shortcut to avoid that. Symmetric matrices are especially easy. “Defective matrices” lack a full set of eigenvectors, sotheyarenot diagonalizable. Certainly theyhavetobe discussed, butwe will not allow them to take over the book. We start with examples of particularly good matrices. Example 1. Everything is clear when A is a diagonal matrix: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 3 0 1 0 A= has λ =3 with x = , λ =2 with x = . 1 1 2 2 0 2 0 1 On each eigenvector A acts like a multiple of the identity: Ax = 3x and Ax = 2x . 1 1 2 2 Other vectors like x = (1,5) are mixtures x +5x of the two eigenvectors, and when A 1 2 multiplies x and x it produces the eigenvaluesλ =3 andλ =2: 1 2 1 2 (cid:34) (cid:35) 3 A times x +5x is 3x +10x = . 1 2 1 2 10 This is Ax for a typical vector x—not an eigenvector. But the action of A is determined by its eigenvectors and eigenvalues. Example 2. The eigenvalues of a projection matrix are 1 or 0! (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 1 P= 2 2 has λ =1 with x = , λ =0 with x = . 1 1 1 1 1 2 2 −1 2 2 We have λ= 1 when x projects to itself, and λ= 0 when x projects to the zero vector. ThecolumnspaceofPisfilledwitheigenvectors,andsoisthenullspace. Ifthosespaces have dimension r and n−r, then λ= 1 is repeated r times and λ= 0 is repeated n−r times (always nλ’s):   1 0 0 0   Four eigenvalues 0 0 0 0 P=  has λ=1,1,0,0. allowing repeats 0 0 0 0 0 0 0 1 There is nothing exceptional about λ= 0. Like every other number, zero might be an eigenvalue and it might not. If it is, then its eigenvectors satisfy Ax = 0x. Thus x is in the nullspace of A. A zero eigenvalue signals that A is singular (not invertible); its determinant is zero. Invertible matrices have allλ(cid:54)=0. Example 3. The eigenvalues are on the main diagonal when A is triangular: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)1−λ 4 5 (cid:175) (cid:175) (cid:175) det(A−λI)=(cid:175) 0 3−λ 6 (cid:175)=(1−λ)(3−λ)(1−λ). (cid:175) 4 (cid:175) 4 2 (cid:175) 0 0 1−λ(cid:175) 2 The determinant is just the product of the diagonal entries. It is zero ifλ=1,λ= 3, or 4 λ= 1; the eigenvalues were already sitting along the main diagonal. 2 This example, in which the eigenvalues can be found by inspection, points to one main theme of the chapter: To transform A into a diagonal or triangular matrix without changing its eigenvalues. We emphasize once more that the Gaussian factorization A= LU is not suited to this purpose. The eigenvalues of U may be visible on the diagonal, but they are not the eigenvalues of A. For most matrices, there is no doubt that the eigenvalue problem is computationally more difficult than Ax = b. With linear systems, a finite number of elimination steps producedtheexactanswerinafinitetime. (Orequivalently, Cramer’srulegaveanexact formula for the solution.) No such formula can give the eigenvalues, or Galois would turn in his grave. For a 5 by 5 matrix, det(A−λI) involvesλ5. Galois and Abel proved that there can be no algebraic formula for the roots of a fifth-degree polynomial. All they will allow is a few simple checks on the eigenvalues, after they have been computed, and we mention two good ones: sum and product. 5B The sum of the n eigenvalues equals the sum of the n diagonal entries: Trace of A=λ +···+λ =a +···+a . (15) 1 n 11 nn Furthermore, the product of the n eigenvalues equals the determinant of A. The projection matrix P had diagonal entries 1, 1 and eigenvalues 1, 0. Then 1 + 1 2 2 2 2 agrees with 1+0 as it should. So does the determinant, which is 0·1 = 0. A singular matrix, with zero determinant, has one or more of its eigenvalues equal to zero. There should be no confusion between the diagonal entries and the eigenvalues. For a triangular matrix they are the same—but that is exceptional. Normally the pivots, diagonal entries, and eigenvalues are completely different, And for a 2 by 2 matrix, the trace and determinant tell us everything: (cid:34) (cid:35) a b has trace a+d, and determinant ad−bc c d"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.1 Introduction 267 (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)a−λ b (cid:175) det(A−λI)=det(cid:175) (cid:175)=λ2−(trace)λ+determinant (cid:175) c d−λ(cid:175) (cid:163) (cid:164) trace± (trace)2−4det 1/2 The eigenvalues areλ= . 2 Those twoλ’s add up to the trace; Exercise 9 gives ∑λ = trace for all matrices. i Eigshow There is a MATLAB demo (just type eigshow), displaying the eigenvalue problem for a 2by2matrix. Itstartswiththeunitvectorx=(1,0). Themousemakesthisvectormove around the unit circle. At the same time the screen shows Ax, in color and also moving. PossiblyAx isaheadof x. Possibly Ax isbehind x. SometimesAx is parallelto x. At that parallel moment, Ax=λx (twice in the second figure). y = (0,1) x2 A= \"0.8 0.3 # Ax1=x1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "0.2 0.7 Ax2=0.5x2 Ay =(0.3,0.7) ellipse ofAx’s Ax= (0.8,0.2) circleofx’s x= (1,0) The eigenvalue λ is the length of Ax, when the unit eigenvector x is parallel. The built-in choices for A illustrate three possibilities: 0, 1, or 2 real eigenvectors."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. There are no real eigenvectors. Ax stays behind or ahead of x. This means the eigenvalues and eigenvectors are complex, as they are for the rotation Q."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. There is only one line of eigenvectors (unusual). The moving directions Ax and x meet but don’t cross. This happens for the last 2 by 2 matrix below."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. Find the eigenvalues and eigenvectors of the matrix A= 1 −1 . Verify that the trace 2 4 equals the sum of the eigenvalues, and the determinant equals their product. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. With the same matrix A, solve the differential equation du/dt = Au, u(0) = 0 . 6 What are the two pure exponential solutions?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. If we shift to A−7I, what are the eigenvalues and eigenvectors and how are they related to those of A? (cid:34) (cid:35) −6 −1 B=A−7I = . 2 −3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "4. Solve du/dt =Pu, when P is a projection: (cid:34) (cid:35) (cid:34) (cid:35) du 1 1 5 = 2 2 u with u(0)= . dt 1 1 3 2 2 Part of u(0) increases exponentially while the nullspace part stays fixed."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5. Find the eigenvalues and eigenvectors of     3 4 2 0 0 2     A=0 1 2 and B=0 2 0. 0 0 0 2 0 0 Check thatλ +λ +λ equals the trace andλ λ λ equals the determinant. 1 2 3 1 2 3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "6. Giveanexampletoshowthattheeigenvaluescanbechangedwhenamultipleofone row is subtracted from another. Why is a zero eigenvalue not changed by the steps of elimination?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "7. Suppose thatλ is an eigenvalue of A, and x is its eigenvector: Ax=λx. (a) Show that this same x is an eigenvector of B = A−7I, and find the eigenvalue. This should confirm Exercise 3. (b) Assumingλ(cid:54)=0, show that x is also an eigenvector of A−1—and find the eigen- value."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "8. Show that the determinant equals the product of the eigenvalues by imagining that the characteristic polynomial is factored into det(A−λI)=(λ −λ)(λ −λ)···(λ −λ), (16) 1 2 n and making a clever choice ofλ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.1 Introduction 269"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "9. Show that the trace equals the sum of the eigenvalues, in two steps. First, find the coefficient of (−λ)n−1 on the right side of equation (16). Next, find all the terms in   a −λ a ··· a 11 12 1n    a 21 a 22−λ ··· a 2n  det(A−λI)=det . . .   . . . . . .  a a ··· a −λ n1 n2 nn that involve (−λ)n−1. They all come from the main diagonal! Find that coefficient of (−λ)n−1 and compare."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "10. (a) Construct 2 by 2 matrices such that the eigenvalues of AB are not the products of the eigenvalues of A and B, and the eigenvalues of A+B are not the sums of the individual eigenvalues. (b) Verify, however, that the sum of the eigenvalues of A+B equals the sum of all the individual eigenvalues of A and B, and similarly for products. Why is this true?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "11. The eigenvalues of A equal the eigenvalues of AT. This is because det(A−λI) equals det(AT−λI). That is true because . Show by an example that the eigen- vectors of A and AT are not the same."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "12. Find the eigenvalues and eigenvectors of (cid:34) (cid:35) (cid:34) (cid:35) 3 4 a b A= and A= . 4 −3 b a"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "13. If B has eigenvalues 1, 2, 3,C has eigenvalues 4, 5, 6, and D has eigenvalues 7, 8, 9, (cid:163) (cid:164) what are the eigenvalues of the 6 by 6 matrix A= B C ? 0 D"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "14. Find the rank and all four eigenvalues for both the matrix of ones and the checker board matrix:     1 1 1 1 0 1 0 1     1 1 1 1 1 0 1 0 A=  and C = . 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 Which eigenvectors correspond to nonzero eigenvalues?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "15. WhataretherankandeigenvalueswhenAandC inthepreviousexercisearenbyn? Remember that the eigenvalueλ=0 is repeated n−r times."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "16. If A is the 4 by 4 matrix of ones, find the eigenvalues and the determinant of A−I."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "17. Choose the third row of the “companion matrix”   0 1 0   A=0 0 1 · · · so that its characteristic polynomial |A−λI| is −λ3+4λ2+5λ+6."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "18. Suppose A has eigenvalues 0, 3, 5 with independent eigenvectors u, v, w. (a) Give a basis for the nullspace and a basis for the column space. (b) Find a particular solution to Ax=v+w. Find all solutions. (c) Show that Ax = u has no solution. (If it had a solution, then would be in the column space.)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "19. The powers Ak of this matrix A approaches a limit as k →∞: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) .8 .3 .70 .45 .6 .6 A= , A2 = , and A∞ = . .2 .7 .30 .55 .4 .4 The matrix A2 is halfway between A and A∞. Explain why A2 = 1(A+A∞) from the 2 eigenvalues and eigenvectors of these three matrices."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "20. Find the eigenvalues and the eigenvectors of these two matrices: (cid:34) (cid:35) (cid:34) (cid:35) 1 4 2 4 A= and A+I = . 2 3 2 4 A+I has the eigenvectors as A. Its eigenvalues are by 1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "21. Compute the eigenvalues and eigenvectors of A and A−1: (cid:34) (cid:35) (cid:34) (cid:35) 0 2 −3/4 1/2 A= and A−1 = . 2 3 1/2 0 A−1 has the eigenvectors as A. When A has eigenvalues λ and λ , its inverse 1 2 has eigenvalues ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "22. Compute the eigenvalues and eigenvectors of A and A2: (cid:34) (cid:35) (cid:34) (cid:35) −1 3 7 −3 A= and A2 = . 2 0 −2 6 A2 has the same as A. When A has eigenvalues λ and λ , A2 has eigenvalues 1 2 ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "23. (a) If you know x is an eigenvector, the way to findλ is to ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.1 Introduction 271 (b) If you knowλ is an eigenvalue, the way to find x is to ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "24. What do you do to Ax=λx, in order to prove (a), (b), and (c)? (a) λ2 is an eigenvalue of A2, as in Problem 22. (b) λ−1 is an eigenvalue of A−1, as in Problem 21. (c) λ+1 is an eigenvalue of A+I, as in Problem 20. (cid:161) (cid:162)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "25. Fromtheunitvectoru= 1,1,3,5 ,constructtherank-1projectionmatrixP=uuT. 6 6 6 6 (a) Show that Pu=u. Then u is an eigenvector withλ=1. (b) If v is perpendicular to u show that Pv= zero vector. Thenλ=0. (c) Find three independent eigenvectors of P all with eigenvalueλ=0."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "26. Solve det(Q−λI)=0 by the quadratic formula, to reachλ=cosθ±isinθ: (cid:34) (cid:35) cosθ −sinθ Q= rotates the xy-plane by the angleθ. sinθ cosθ Find the eigenvectors of Q by solving (Q−λI)x=0. Use i2 =−1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "27. Every permutation matrix leaves x=(1,1,...,1) unchanged. Thenλ=1. Find two moreλ’s for these permutations:     0 1 0 0 0 1     P=0 0 1 and P=0 1 0. 1 0 0 1 0 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "28. IfAhasλ =4andλ =5, thendet(A−λI)=(λ−4)(λ−5)=λ2−9λ+20. Find 1 2 three matrices that have trace a+d =9, determinant 20, andλ=4,5."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "29. A 3 by 3 matrix B is known to have eigenvalues 0, 1, 2, This information is enough to find three of these: (a) the rank of B, (b) the determinant of BTB, (c) the eigenvalues of BTB, and (d) the eigenvalues of (B+I)−1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "30. Choose the second row of A=[0 1] so that A has eigenvalues 4 and 7. ∗ ∗"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "31. Choose a, b, c, so that det(A−λI)=9λ−λ3. Then the eigenvalues are −3, 0, 3:   0 1 0   A=0 0 1. a b c"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "32. Construct any 3 by 3 Markov matrix M: positive entries down each column add to"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. If e = (1,1,1), verify that MTe = e. By Problem 11, λ= 1 is also an eigenvalue of M. Challenge: A 3 by 3 singular Markov matrix with trace 1 has eigenvalues 2 λ= ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "33. Find three 2 by 2 matrices that haveλ =λ =0. The trace is zero and the determi- 1 2 nant is zero. The matrix A might not be 0 but check that A2 =0."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "34. This matrix is singular with rank 1. Find threeλ’s and three eigenvectors:     1 2 1 2 (cid:104) (cid:105)     A=2 2 1 2 =4 2 4. 1 2 1 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "35. Suppose A and B have the same eigenvalues λ ,...,λ with the same independent 1 n eigenvectors x ,...,x . Then A = B. Reason: Any vector x is a combination c x + 1 n 1 1 ···+c x . What is Ax? What is Bx? n n"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "36. (Review) Find the eigenvalues of A, B, andC:       1 2 3 0 0 1 2 2 2       A=0 4 5, B=0 2 0, and C =2 2 2. 0 0 6 3 0 0 2 2 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "37. When a+b=c+d, show that (1,1) is an eigenvector and find both eigenvalues: (cid:34) (cid:35) a b A= . c d"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "38. WhenPexchangesrows1and2and columns1and2, theeigenvaluesdon’tchange. Find eigenvectors of A and PAP forλ=11:     1 2 1 6 3 3     A=3 6 3 and PAP=2 1 1. 4 8 4 8 4 4"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "39. Challenge problem: Is there a real 2 by 2 matrix (other than I) with A3 = I? Its eigenvalues must satisfy λ3 = I. They can be e2πi/3 and e−2πi/3. What trace and determinant would this give? Construct A."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "40. There are six 3 by 3 permutation matrices P. What numbers can be the determinants of P? What numbers can be pivots? What numbers can be the trace of P? What four numbers can be eigenvalues of P?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.2 DiagonalizationofaMatrix 273"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.2 Diagonalization of a Matrix We start right off with the one essential computation. It is perfectly simple and will be used in every section of this chapter. The eigenvectors diagonalize a matrix: 5C Suppose the n by n matrix A has n linearly independent eigenvectors. If these eigenvectors are the columns of a matrix S, then S−1AS is a diagonal matrix Λ. The eigenvalues of A are on the diagonal of Λ:   λ 1   Diagonalization S−1AS=Λ=  λ 2  . (1)  ...  λ n We call S the “eigenvector matrix” and Λ the “eigenvalue matrix”—using a capital lambda because of the small lambdas for the eigenvalues on its diagonal. Proof. Put the eigenvectors x in the columns of S, and compute AS by columns: i     | | | | | |     AS=Ax x ··· x =λ x λ x ··· λ x . 1 2 n 1 1 2 2 n n | | | | | | Then the trick is to split this last matrix into a quite different product SΛ:       λ 1       λ 2  λ 1x 1 λ 2x 2 ··· λ nx n=x 1 x 2 ··· x n  ...  . λ n It is crucial to keep these matrices in the right order. If Λ came before S (instead of after), then λ would multiply the entries in the first row. We want λ to appear in the 1 1 first column. As it is, SΛ is correct. Therefore, AS=SΛ, or S−1AS=Λ, or A=SΛS−1. (2) S is invertible, because its columns (the eigenvectors) were assumed to be independent. We add four remarks before giving any examples or applications. Remark 1. If the matrix A has no repeated eigenvalues—the numbers λ ,...,λ are 1 n distinct—then its n eigenvectors are automatically independent (see 5D below). There- fore any matrix with distinct eigenvalues can be diagonalized. Remark2. ThediagonalizingmatrixSisnotunique. Aneigenvectorxcanbemultiplied by a constant, and remains an eigenvector. We can multiply the columns of S by any nonzeroconstants,andproduceanewdiagonalizingS. Repeatedeigenvaluesleaveeven more freedom in S. For the trivial example A = I, any invertible S will do: S−1IS is is always diagonal (Λ is just I). All vectors are eigenvectors of the identity. Remark 3. Other matrices S will not produce a diagonal Λ. Suppose the first column of S is y. Then the first column of SΛ is λ y. If this is to agree with the first column of 1 AS, which by matrix multiplication is Ay, then y must be an eigenvector: Ay=λ y. The 1 order of the eigenvectors in S and the eigenvalues in Λ is automatically the same. Remark 4. Not all matrices possess n linearly independent eigenvectors, so not all ma- trices are diagonalizable. The standard example of a “defective matrix” is (cid:34) (cid:35) 0 1 A= . 0 0 Its eigenvalues areλ =λ =0, since it is triangular with zeros on the diagonal: 1 2 (cid:34) (cid:35) −λ 1 det(A−λI)=det =λ2. 0 −λ All eigenvectors of this A are multiples of the vector (1,0): (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 0 1 0 c x= , or x= . 0 0 0 0 λ = 0 is a double eigenvalue—its algebraic multiplicity is 2. But the geometric multi- plicity is 1—there is only one independent eigenvector. We can’t construct S. Here is a more direct proof that this A is not diagonalizable. Since λ =λ = 0, Λ 1 2 would have to be the zero matrix, But if Λ = S−1AS = 0, then we premultiply by S and postmultiply by S−1, to deduce falsely that A=0. There is no invertible S. That failure of diagonalization was not a result ofλ=0. It came fromλ =λ : 1 2 (cid:34) (cid:35) (cid:34) (cid:35) 3 1 2 −1 Repeated eigenvalues A= and A= . 0 3 1 0 Their eigenvalues are 3, 3 and 1, 1. They are not singular! The problem is the shortage of eigenvectors—which are needed for S. That needs to be emphasized: Diagonalizability of A depends on enough eigenvectors. Invertibility of A depends on nonzero eigenvalues. There is no connection between diagonalizability (n independent eigenvector) and in- vertibility (no zero eigenvalues). The only indication given by the eigenvalues is this: Diagonalization can fail only if there are repeated eigenvalues. Even then, it does not always fail. A = I has repeated eigenvalues 1,1,...,1 but it is already diagonal! There is no shortage of eigenvectors in that case. The test is to check, for an eigenvalue that is repeated p times, whether there are p independent eigenvectors—in other words, whether A−λI has rank n−p. To complete that circle of ideas, we have to show that distinct eigenvalues present no problem."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.2 DiagonalizationofaMatrix 275 5D If eigenvectors x ,...,x correspond to different eigenvalues λ ,...,λ , 1 k 1 k then those eigenvectors are linearly independent. Suppose first that k = 2, and that some combination of x and x produces zero: 1 2 c x +c x = 0. Multiplying by A, we find c λ x +c λ x = 0. Subtracting λ times 1 1 2 2 1 1 1 2 2 2 2 the previous equation, the vector x disappears: 2 c (λ −λ )x =0. 1 1 2 1 Since λ (cid:54)= λ and x (cid:54)= 0, we are forced into c = 0. Similarly c = 0, and the two 1 2 1 1 2 vectors are independent; only the trivial combination gives zero. Thissameargumentextendstoanynumberofeigenvectors: Ifsomecombinationpro- duceszero,multiplybyA,subtractλ timestheoriginalcombination,andx disappears— k k leavingacombinationofx ,...,x , whichproduceszero. Byrepeatingthesamesteps 1 k−1 (this is really mathematical induction) we end up with a multiple of x that produces 1 zero. This forces c =0, and ultimately every c =0. Therefore eigenvectors that come 1 i from distinct eigenvalues are automatically independent. A matrix with n distinct eigenvalues can be diagonalized. This is the typical case. Examples of Diagonalization The main point of this section is S−1AS = A. The eigenvector matrix S converts A into its eigenvalue matrix Λ (diagonal). We see this for projections and rotations. (cid:183) (cid:184) (cid:163) (cid:164) 1 1 Example 1. The projection A = 2 2 has eigenvalue matrix Λ = 1 0 . The eigen- 1 1 0 0 2 2 vectors go into the columns of S: (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 0 S= and AS=SΛ= . 1 −1 1 0 That last equation can be verified at a glance. Therefore S−1AS=Λ. Example 2. The eigenvalues themselves are not so clear for a rotation: (cid:34) (cid:35) 0 −1 90° rotation K = has det(K−λI)=λ2+1. 1 0 How can a vector be rotated and still have its direction unchanged? Apparently it can’t—except for the zero vector, which is useless. But there must be eigenvalues, and we must be able to solve du/dt =Ku. The characteristic polynomialλ2+1 should still have two roots—but those roots are not real. You see the way out. The eigenvalues of K are imaginary numbers, λ = i and λ = 1 2 −i. The eigenvectors are also not real. Somehow, in turning through 90°, they are multiplied by i or −i: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) −i −1 y 0 1 (K−λ I)x = = and x = 1 1 1 1 −i z 0 −i (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) i −1 y 0 1 (K−λ I)x = = and x = . 2 2 2 1 i z 0 i The eigenvalues are distinct, even if imaginary, and the eigenvectors are independent. They go into the columns of S: (cid:34) (cid:35) (cid:34) (cid:35) 1 1 i 0 S= and S−1KS= . −i i 0 −i We are faced with an inescapable fact, that complex numbers are needed even for real matrices. If there are too few real eigenvalues, there are always n complex eigen- values. (Complex includes real, when the imaginary part is zero.) If there are too few eigenvectors in the real world R3, or in Rn, we look in C3 or Cn. The space Cn contains all column vectors with complex components, and it has new definitions of length and inner product and orthogonality. But it is not more difficult than Rn, and in Section 5.5 we make an easy conversion to the complex case. Powers and Products: Ak and AB Thereisonemoresituationinwhichthecalculationsareeasy. TheeigenvalueofA2 are exactly λ2,...,λ2, and every eigenvector of A is also an eigenvector of A2. We start 1 n from Ax=λx, and multiply again by A: A2x=Aλx=λAx=λ2x. (3) Thus λ2 is an eigenvalue of A2, with the same eigenvector x. If the first multiplication by A leaves the direction of x unchanged, then so does the second. The same result comes from diagonalization, by squaring S−1AS=Λ: Eigenvalues of A2 (S−1AS)(S−1AS)=Λ2 or S−1A2S=Λ2. The matrix A2 is diagonalized by the same S, so the eigenvectors are unchanged. The eigenvalues are squared. This continues to hold for any power of A: 5E The eigenvalues of Ak areλk,...,λk, and each eigenvector of A is still an 1 n eigenvector of Ak. When S diagonalizes A, it also diagonalizes Ak: Λk =(S−1AS)(S−1AS)···(S−1AS)=S−1AkS. (4) Each S−1 cancels an S, except for the first S−1 and the last S."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. Factor the following matrices into SΛS−1: (cid:34) (cid:35) (cid:34) (cid:35) 1 1 2 1 A= and A= . 1 1 0 0 (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. Find the matrix A whose eigenvalues are 1 and 4, and whose eigenvectors are 3 (cid:163) (cid:164) 1 and 2 , respectively. (Hint: A=SΛS−1.) 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. Find all the eigenvalues and eigenvectors of   1 1 1   A=1 1 1 1 1 1 and write two different diagonalizing matrices S."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "4. If a 3 by 3 upper triangular matrix has diagonal entries 1, 2, 7, how do you know it can be diagonalized? What is Λ?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.2 DiagonalizationofaMatrix 279"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5. Which of these matrices cannot be diagonalized? (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 2 −2 2 0 2 0 A = A = A = . 1 2 3 2 −2 2 −2 2 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "6. (a) If A2 =I, what are the possible eigenvalues of A? (b) If this A is 2 by 2, and not I or −I, find its trace and determinant. (c) If the first row is (3,−1), what is the second row? (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "7. If A= 4 3 , find A100 by diagonalizing A. 1 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "8. Suppose A=uvT is a column times a row (a rank-1 matrix). (a) By multiplying A times u, show that u is an eigenvector. What isλ? (b) What are the other eigenvalues of A (and why)? (c) Compute trace(A) from the sum on the diagonal and the sum ofλ’s."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "9. Show by direct calculation that AB and BA have the same trace when (cid:34) (cid:35) (cid:34) (cid:35) a b q r A= and B= . c d s t Deduce that AB−BA=I is impossible (except in infinite dimensions)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "10. Suppose A has eigenvalues 1, 2, 4. What is the trace of A2? What is the determinant of (A−1)T?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "11. IftheeigenvaluesofAare1, 1, 2, whichofthefollowingarecertaintobetrue? Give a reason if true or a counterexample if false: (a) A is invertible. (b) A is diagonalizable. (c) A is not diagonalizable."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "12. Suppose the only eigenvectors of A are multiples of x=(1,0,0). True or false: (a) A is not invertible. (b) A has a repeated eigenvalue. (c) A is not diagonalizable. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "13. DiagonalizethematrixA= 5 4 andfindoneofitssquareroots—amatrixsuchthat 4 5 R2 =A. How many square roots will there be?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "14. Suppose the eigenvector matrix S has ST =S−1. Show that A=SΛS−1 is symmetric and has orthogonal eigenvectors. Problems 15–24 are about the eigenvalue and eigenvector matrices."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "15. Factor these two matrices into A=SΛS−1: (cid:34) (cid:35) (cid:34) (cid:35) 1 2 1 1 A= and A= . 0 3 2 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "16. If A=SΛS−1 then A3 =( )( )( ) and A−1 =( )( )( ). (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "17. If A has λ = 2 with eigenvector x = 1 and λ = 5 with x = 1 , use SΛS−1 to 1 1 0 2 2 1 find A. No other matrix has the sameλ’s and x’s."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "18. Suppose A = SΛS−1. What is the eigenvalue matrix for A+2I? What is the eigen- vector matrix? Check that A+2I =( )( )( )−1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "19. True or false: If the n columns of S (eigenvectors of A) are independent, then (a) A is invertible. (b) A is diagonalizable. (c) S is invertible. (d) S is diagonalizable."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "20. If the eigenvectors of A are the columns of I, then A is a matrix. If the eigen- vector matrix S is triangular, then S−1 is triangular and A is triangular."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "21. Describe all matrices S that diagonalize this matrix A: (cid:34) (cid:35) 4 0 A= . 1 2 Then describe all matrices that diagonalize A−1. (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "22. Write the most general matrix that has eigenvectors 1 and 1 . 1 −1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "23. Find the eigenvalues of A and B and A+B: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 1 2 1 A= , B= , A+B= . 1 1 0 1 1 2 Eigenvalues of A+B (are equal to)(are not equal to) eigenvalues of A plus eigenval- ues of B."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "24. Find the eigenvalues of A, B, AB, and BA: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 1 1 1 2 1 A= , B= , AB= , and BA= . 1 1 0 1 1 2 1 1 EigenvaluesofAB(areequalto)(arenotequalto)eigenvaluesofAtimeseigenvalues of B. Eigenvalues of AB (are)(are not) equal to eigenvalues of BA."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.2 DiagonalizationofaMatrix 281 Problems 25–28 are about the diagonalizability of A."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "25. True or false: If the eigenvalues of A are 2, 2, 5, then the matrix is certainly (a) invertible. (b) diagonalizable. (c) not diagonalizable."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "26. If the eigenvalues of A are 1 and 0, write everything you know about the matrices A and A2."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "27. Complete these matrices so that detA=25. Then trace=10, andλ=5 is repeated! Find an eigcnvector with Ax=5x. These matrices will nothe diagonalizabie because there is no second line of eigenvectors. (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 8 9 4 10 5 A= , A= , and A= . 2 1 −5 (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "28. The matrix A = 3 1 is not diagonalizable because the rank of A−3I is . 0 3 Change one entry to make A diagonalizable. Which entries could you change? Problems 29–33 are about powers of matrices."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "29. Ak =SΛkS−1 approachesthezeromatrixask→∞ifandonlyifeveryλhasabsolute value less than . Does Ak →0 or Bk →0? (cid:34) (cid:35) (cid:34) (cid:35) .6 .4 .6 .9 A= and B= . .4 .6 .1 .6"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "30. (Recommended) Find Λ and S to diagonalize A in Problem 29. What is the limit of Λk as k → ∞? What is the limit of SΛkS−1? In the columns of this limiting matrix you see the ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "31. Find Λ and S to diagonalize B in Problem 29. What is B10u for these u ? 0 0 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 3 3 6 u = , u = , and u = . 0 0 0 1 −1 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "32. Diagonalize A and compute SΛkS−1 to prove this formula for Ak: (cid:34) (cid:35) (cid:34) (cid:35) 2 1 1 3k+1 3k−1 A= has Ak = . 1 2 2 3k−1 3k+1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "33. Diagonalize B and compute SΛkS−1 to prove this formula for Bk: (cid:34) (cid:35) (cid:34) (cid:35) 3 1 3k 3k−2k B= has Bk = . 0 2 0 2k Problems 34–44 are new applications of A=SΛS−1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "34. SupposethatA=SΛS−1. TakedeterminantstoprovethatdetA=λ λ ···λ =prod- 1 2 n uct ofλ’s. This quick proof only works when A is ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "35. The trace of S times ΛS−1 equals the trace of ΛS−1 times S. So the trace of a diago- nalizable A equals the trace of Λ, which is . (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "36. If A = SΛS−1, diagonalize the block matrix B = A 0 . Find its eigenvalue and 0 2A eigenvector matrices."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "37. Consider all 4 by 4 matrices A that are diagonalized by the same fixed eigenvector matrix S. Show that the A’s form a subspace (cA and A +A have this same S). 1 2 What is this subspace when S=I? What is its dimension?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "38. Suppose A2 =A. On the left side A multiplies each column of A. Which of our four subspaces contains eigenvectors withλ=1? Which subspace contains eigenvectors withλ=0? Fromthedimensionsofthosesubspaces, Ahasafullsetofindependent eigenvectors and can be diagonalized."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "39. Suppose Ax = λx. If λ = 0, then x is in the nullspace. If λ (cid:54)= 0, then x is in the column space. Those spaces have dimensions (n−r)+r =n. So why doesn’t every square matrix have n linearly independent eigenvectors?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "40. Substitute A = SΛS−1 into the product (A−λ I)(A−λ I)···(A−λ I) and explain 1 2 n why this produces the zero matrix. We are substituting the matrix A for the number λ in the polynomial p(λ) = det(A−λI). The Cayley-Hamilton Theorem says that this product is always p(A)= zero matrix, even if A is not diagonalizable. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "41. Test the Cayley-Hamilton Theorem on Fibonacci’s matrix A = 1 1 . The theorem 1 0 predicts that A2−A−I =0, since det(A−λI) isλ2−λ−1. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "42. If A= a b , then det(A−λI) is (λ−a)(λ−d). Check the Cayley-Hamilton state- c d ment that (A−aI)(A−dI)= zero matrix. (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "43. If A = 1 0 and AB = BA, show that B = a b is also diagonal. B has the same 0 2 c d eigen as A, but different eigen . These diagonal matrices B form a two- dimensional subspace of matrix space. AB−BA = 0 gives four equations for the unknowns a, b, c, d—find the rank of the 4 by 4 matrix."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "44. If A is 5 by 5. then AB−BA= zero matrix gives 25 equations for the 25 entries in B. Show that the 25 by 25 matrix is singular by noticing a simple nonzero solution B."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "45. Find the eigenvalues and eigenvectors for both of these Markov matrices A and A∞. Explain why A100 is close to A∞: (cid:34) (cid:35) (cid:34) (cid:35) .6 .2 1/3 1/3 A= and A∞ = . .4 .8 2/3 2/3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 DifferenceEquationsandPowersAk 283"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 Difference Equations and Powers Ak Difference equations u = Au move forward in a finite number of finite steps. A k+1 k differential equation takes an infinite number of infinitesimal steps, but the two theories stayabsolutelyinparallel. Itisthesameanalogybetweenthediscreteandthecontinuous that appears over and over in mathematics. A good illustration is compound interest, when the time step gets shorter. Suppose you invest $1000 at 6% interest. Compounded once a year, the principal P ismultipliedby1.06. ThisisadifferenceequationP =AP =1.06P withatimestep k+1 k k of one year. After 5 years, the original P =1000 has been multiplied 5 times: 0 Yearly P =(1.06)5P which is (1.06)51000=$1338. 5 0 Nowsupposethetimestepisreducedtoamonth. Thenewdifferenceequationis p = k+1 (1+.06/12)p . After 5 years, or 60 months, you have $11 more: k (cid:181) (cid:182) 60 .06 Monthly p = 1+ p which is (1.005)601000=$1349. 60 0 12 The next step is to compound every day, on 5(365) days. This only helps a little: (cid:181) (cid:182) 5·365 .06 Daily compounding 1+ 1000=$1349.83. 365 Finally, to keep their employees really moving, banks offer continuous compounding. The interest is added on at every instant, and the difference equation breaks down. You can hope that the treasurer does not know calculus (which is all about limits as ∆t →0). The bank could compound the interest N times a year, so ∆t =1/N: (cid:181) (cid:182) 5N .06 Continuously 1+ 1000→e.301000=$1349.87. N Or the bank can switch to a differential equation—the limit of the difference equation p =(1+.06∆t)p . Moving p to the left side and dividing by ∆t, k+1 k k Discrete to p −p dp k+1 k =.06p approaches =.06p. (1) k continuous ∆t dt The solution is p(t) = e.06tp . After t = 5 years, this again amounts to $1349.87. The 0 principal stays finite, even when it is compounded every instant—and the improvement over compounding every day is only four cents. Fibonacci Numbers The main object of this section is to solve u = Au . That leads us to Ak and powers k+1 k of matrices. Our second example is the famous Fibonacci sequence: Fibonacci numbers 0,1,1,2,3,5,8,13,.... You see the pattern: Every Fibonacci number is the sum of the two previous F’s: Fibonacci equation F =F +F . (2) k+2 k+1 k That is the difference equation. It turns up in a most fantastic variety of applications, and deserves a book of its own. Leaves grow in a spiral pattern, and on the apple or oak you find five growths for every two turns around the stem. The pear tree has eight for every three turns, and the willow is 13:5. The champion seems to be a sunflower whose seeds chose an almost unbelievable ratio of F /F =144/233.2 12 13 How could we find the 1000th Fibonacci number, without starting at F = 0 and 0 F =1,andworkingallthewayouttoF ? Thegoalistosolvethedifferenceequation 1 1000 F =F +F . Thiscanbereducedtoaone-stepequationu =Au . Everystep k+2 k+1 k k+1 k multiplies u =(F ,F ) by a matrix A: k k+1 k (cid:34) (cid:35)(cid:34) (cid:35) F =F +F 1 1 F k+2 k+1 k k+1 becomes u = =Au . (3) k+1 k F =F 1 0 F k+1 k+1 k The one-step system u = Au is easy to solve, It starts from u . After one step it k+1 k 0 produces u = Au . Then u is Au , which is A2u . Every step brings a multiplication 1 0 2 1 0 by A, and after k steps there are k multiplications: The solution to a difference equation u =Au is u =Aku . k+1 k k 0 The real problem is to find some quick way to compute the powers Ak, and thereby find the 1000th Fibonacci number. The key lies in the eigenvalues and eigenvectors: 5G If A can be diagonalized, A=SΛS−1, then Ak comes from Λk: u =Aku =(SΛS−1)(SΛS−1)···(SΛS−1)u =SΛkS−1u . (4) k 0 0 0 The columns of S are the eigenvectors of A. Writing S−1u = c, the solution 0 becomes     λk c 1 1 u k =SΛkc= x 1 ··· x n   ...    . . .  =c 1λ 1kx 1+···+c nλ nkx n. λk c n n (5) After k steps, u is a combination of the n “pure solutions”λkx. k These formulas give two different approaches to the same solution u = SΛkS−1u . k 0 The first formula recognized that Ak is identical with SΛkS−1, and we could stop there. 2Forthesebotanicalapplications, seeD’ArcyThompson’sbookOnGrowthandForm(CambridgeUniversity Press, 1942) or Peter Stevens’s beautiful Patterns in Nature (Little, Brown, 1974). Hundreds of other properties of the F have been published in the Fibonacci Quarterly. Apparently Fibonacci brought Arabic numerals into n Europe,about1200A.D."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 DifferenceEquationsandPowersAk 285 But the second approach brings out the analogy with a differential equation: The pure exponential solutions eλ itx are now the pure powers λkx . The eigenvectors x are i i i i amplifiedbytheeigenvaluesλ. Bycombiningthesespecialsolutionstomatchu —that i 0 is where c came from—we recover the correct solution u =SΛkS−1u . k 0 In any specific example like Fibonacci’s, the first step is to find the eigenvalues: (cid:34) (cid:35) 1−λ 1 A−λI = has det(A−λI)=λ2−λ−1 1 −λ √ √ 1+ 5 1− 5 Two eigenvalues λ = and λ = . 1 2 2 2 ThesecondrowofA−λI is(1,−λ). Toget(A−λI)x=0,theeigenvectorisx=(λ,1), The first Fibonacci numbers F =0 and F =1 go into u , and S−1u =c: 0 1 0 0 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) −1 λ λ 1 1/(λ −λ ) 1 1 S−1u = 1 2 gives c= 1 2 = √ . 0 1 1 0 −1/(λ −λ ) 5 −1 1 2 Those are the constants in u = c λkx +c λkx . Both eigenvectors x and x have k 1 1 1 2 2 2 1 2 second component 1. That leaves F =c λk+c λk in the second component of u : k 1 1 2 2 k   (cid:195) √ (cid:33) (cid:195) √ (cid:33) k k Fibonacci 1 1+ 5 1− 5 F = √  − . k numbers 5 2 2 This is the answer we wanted. The fractions and square roots look surprising because Fibonacci’s rule F =F +F must produce whole numbers, Somehow that formula k+2 k+1 k √ √ for F must give an integer. In fact, since the second term [(1− 5)/2]k/ 5 is always k less than 1, it must just move the first term to the nearest integer: 2 (cid:195) √ (cid:33) 1000 1 1+ 5 F =nearest integer to √ . 1000 5 2 This is an enormous number, and F will be even bigger. The fractions are becoming 1001 √ insignificant, and the ratio F /F must be very close to (1+ 5)/2≈1.618. Since 1001 1000 λk is insignificant compared toλk, the ratio F /F approachesλ . 2 1 k+1 k 1 (cid:163) (cid:164) That is a typical difference equation, leading to the powers of A = 1 1 . it involved √ 1 0 5 because the eigenvalues did. If we choose a matrix withλ =1 andλ =6. we can 1 2 focus on the simplicity of the computation—after A has been diagonalized: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) −4 −5 1 −1 A= has λ=1 and 6, with x = and x = 1 2 10 11 −1 2 (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 1 −1 1k 0 2 1 2−6k 1−6k Ak =SΛkS−1 is = . −1 2 0 6k 1 1 −2+2·6k −1+2·6k The powers 6k and 1k appear in that last matrix Ak, mixed in by the eigenvectors. For the difference equation u = Au , we emphasize the main point. Every eigen- k+1 k vector x produces a “pure solution” with powers ofλ: One solution is u =x, u =λx, u =λ2x,... 0 1 2 When the initial u is an eigenvector x, this is the solution: u =λkx. In general u 0 k 0 is not an eigenvector. But if u is a combination of eigenvectors, the solution u is the 0 k same combination of these special solutions. 5H If u = c x +···+c x , then after k steps u = c λkx +···+c λkx . 0 1 1 n n k 1 1 1 n n n Choose the c’s to match the starting vector u : 0    c 1   .  u 0 =x 1 ··· x n . . =Sc and c=S−1u 0. (6) c n Markov Matrices There was an exercise in Chapter 1, about moving in and out of California, that is worth another look. These were the rules: Each year 1 of the people outside California move in, and 2 of the people 10 10 inside California move out. We start with y people outside and z inside. 0 0 At the end of the first year the numbers outside and inside are y and z : 1 1 (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) Difference y =.9y +.2z y .9 .2 y 1 0 0 1 0 or = . equation z =.1y +.8z z .1 .8 z 1 0 0 1 0 This problem and its matrix have the two essential properties of a Markov process:"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. The total number of people stays fixed: Each column of the Markov matrix adds up to 1. Nobody is gained or lost."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. The numbers outside and inside can never become negative: The matrix has no negative entries. The powers Ak are all nonnegative.3 We solve this Markov difference equation using u = SΛkS−1u . Then we show that k 0 the population approaches a “steady state.” First A has to be diagonalized: (cid:34) (cid:35) .9−λ .2 A−λI = has det(A−λI)=λ2−1.7λ+.7 .1 .8−λ 3Furthermore,historyiscompletelydisregarded; eachnewu dependsonlyonthecurrentu . Perhapseven k+1 k ourlivesareexamplesofMarkovprocesses,butIhopenot."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 DifferenceEquationsandPowersAk 287 (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) 2 1 1 1 1 λ andλ =.7: A=SΛS−1 = 3 3 . 1 2 1 −1 .7 1 −2 3 3 To find Ak, and the distribution after k years, change SΛS−1 to SΛkS−1: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) y y 2 1 1k 1 1 y k =Ak 0 = 3 3 0 z z 1 −1 .7k 1 −2 z k 0 3 3 0 (cid:34) (cid:35) (cid:34) (cid:35) 2 1 =(y +z ) 3 +(y −2z )(.7)k 3 . 0 0 1 0 0 −1 3 3 Those two terms are c λkx +c λkx . The factor λk = 1 is hidden in the first term. In 1 1 1 2 2 2 1 the long run, the other factor (.7)k becomes extremely small. The solution approaches a limiting state u =(y ,z ): ∞ ∞ ∞ (cid:34) (cid:35) (cid:34) (cid:35) y 2 Steady state ∞ =(y +z ) 3 . z 0 0 1 ∞ 3 The total population is still y +z , but in the limit 2 of this population is outside Cali- 0 0 3 fornia and 1 is inside. This is true no matter what the initial distribution may have been! 3 If the year starts with 2 outside and 1 inside, then it ends the same way: 3 3 (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) .9 .2 2 2 3 = 3 . or Au =u . .1 .8 1 1 ∞ ∞ 3 3 The steady state is the eigenvector of A corresponding to λ = 1. Multiplication by A, from one time step to the next, leaves u unchanged. ∞ The theory of Markov processes is illustrated by that California example: 5I A Markov matrix A has all a ≥0, with each column adding to 1. ij (a) λ =1 is an eigenvalue of A. 1 (b) Its eigenvector x is nonnegative—and it is a steady state, since Ax =x . 1 1 1 (c) The other eigenvalues satisfy (cid:107)λ(cid:107)≤1. i (d) If A or any power of A has all positive entries, these other |λ| are below 1. i The solution Aku approaches a multiple of x —which is the steady state 0 1 u . ∞ To find the right multiple of x , use the fact that the total population stays the same. If 1 California started with all 90 million people out, it ended with 60 million out and 30 million in. It ends the same way if all 90 million were originally inside. We note that many authors transpose the matrix so its rows add to 1. Remark. Our description of a Markov process was deterministic: populations moved in fixed proportions. But if we look at a single individual, the fractions that move become probabilities. With probability 1 , an individual outside California moves in. If inside, 10 the probability of moving out is 2 . The movement becomes a random process, and A is 10 called a transition matrix. The components of u = Aku specify the probability that the individual is outside k 0 or inside the state. These probabilities are never negative and add to 1—everybody has to be somewhere. That brings us back to the two fundamental properties of a Markov matrix: Each column adds to 1, and no entry is negative. Why is λ = 1 always an eigenvalue? Each column of A−I adds up to 1−1 = 0. Therefore the rows of A−I add up to the zero row, they are linearly dependent, and det(A−I)=0. Except for very special cases, u will approach the corresponding eigenvector4. In k the formula u =c λkx +···+c λkx , no eigenvalue can be larger than 1. (Otherwise k 1 1 1 n n n the probabilities u would blow up.) If all other eigenvalues are strictly smaller than k λ =1, then the first term in the formula will be dominant. The otherλk go to zero, and 1 i u →c x =u =steady state. k 1 1 ∞ This is an example of one of the central themes of this chapter: Given information about A, find information about its eigenvalues. Here we foundλ =1. max Stability of u =Au k+1 k There is an obvious difference between Fibonacci numbers and Markov processes. The numbers F become larger and larger, while by definition any “probability” is between 0 k and 1. The Fibonacci equation is unstable. So is the compound interest equation P = k+1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1.06P ; the principal keeps growing forever. If the Markov probabilities decreased to k zero, that equation would be stable; but they do not, since at every stage they must add to 1. Therefore a Markov process is neutrally stable. We want to study the behavior of u = Au as k → ∞. Assuming that A can be k+1 k diagonalized, u will be a combination of pure solutions: k Solution at time k u =SΛkS−1u =c λkx +···+c λkx . k 0 1 1 1 n n n The growth of u is governed by theλk. Stability depends on the eigenvalues: k i 5J The difference equation u =Au is k+1 k stable if all eigenvalues satisfy |λ|<1; i neutrally stable if some |λ|=1 and all the other |λ|<1; and i i unstable if at least one eigenvalue has |λ|>1. i In the stable case, the powers Ak approach zero and so does u =Aku . k 0 4Ifeverybodyoutsidemovesinandeverybodyinsidemovesout, thenthepopulationsarereversedeveryyear (cid:163) (cid:164) and there is no steady state. The transition matrix is A= 01 and −1 is an eigenvalue as well as +1—which 10 cannothappenifalla >0. ij"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 DifferenceEquationsandPowersAk 289 Example 1. This matrix A is certainly stable: (cid:34) (cid:35) 0 4 1 A= has eigenvalues 0 and . 0 1 2 2 The λ’s are on the main diagonal because A is triangular. Starting from any u , and 0 following the rule u =Au , the solution must eventually approach zero: k+1 k (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 0 4 2 1 1 u = , u = , u = , u = , u = 2 ,··· 0 1 1 1 2 1 3 1 4 1 2 4 8 16 Thelargereigenvalueλ= 1 governsthedecay;afterthefirststepeveryu is 1u . The 2 k 2 k−1 real effect of the first step is to split u into the two eigenvectors of A: 0 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:181) (cid:182) k 8 −8 1 8 −8 u = + and then u = +(0)k . 0 k 1 0 2 1 0 Positive Matrices and Applications in Economics By developing the Markov ideas we can find a small gold mine (entirely optional) of matrix applications in economics. Example 2 (Leontief’s input-output matrix). This is one of the first great successes of mathematical economics. To illustrate it, we construct a consumption matrix—in which a , gives the amount of product j that is ij needed to create one unit of product i:   .4 0 .1 (steel)   A=0 .1 .8. (food) .5 .7 .1 (labor) The first question is: Can we produce y units of steel, y units of food, and y units of 1 2 3 labor? We must start with larger amounts p , p , p , because some part is consumed 1 2 3 by the production itself. The amount consumed is Ap, and it leaves a net production of p−Ap. Problem To find a vector p such that p−Ap=y, or p=(I−A)−1y. On the surface, we are only asking if I−A is invertible. But there is a nonnegative twist totheproblem. Demandandproduction,yand p,arenonnegative. Since pis(1−A)−1y, the real question is about the matrix that multiplies y: When is (I−A)−1 a nonnegative matrix? Roughly speaking, A cannot be too large. If production consumes too much, nothing is left as output. The key is in the largest eigenvalueλ of A, which must be below 1: 1 Ifλ >1, (I−A)−1 fails to be nonnegative. 1 Ifλ =1, (I−A)−1 fails to exist. 1 Ifλ <1, (I−A)−1 is a converging sum of nonnegative matrices: 1 Geometric series (I−A)−1 =I+A+A2+A3+···. (7) The 3 by 3 example hasλ =.9, and output exceeds input. Production can go on. 1 Those are easy to prove, once we know the main fact about a nonnegative matrix like A: Not only is the largest eigenvalue λ positive, but so is the eigenvector x . Then 1 1 (I−A)−1 has the same eigenvector, with eigenvalue 1/(1−λ ). 1 If λ exceeds 1, that last number is negative. The matrix (I −A)−1 will take the 1 positive vector x to a negative vector x /(1−λ ). In that case (I−A)−1 is definitely 1 1 1 not nonnegative. If λ = 1, then I−A is singular. The productive case is λ < 1, when 1 1 the powers of A go to zero (stability) and the infinite series I+A+A2+··· converges. MultiplyingthisseriesbyI−Aleavestheidentitymatrix—allhigherpowerscancel—so (I−A)−1 is a sum of nonnegative matrices, We give two examples: (cid:34) (cid:35) 0 2 A= hasλ =2 and the economy is lost 1 2 0 (cid:34) (cid:35) .5 2 1 A= hasλ = and we can produce anything. 1 0 .5 2 (cid:163) (cid:164) (cid:163) (cid:164) The matrices (I−A)−1 in those two cases are −1 1 2 and 2 8 . 3 2 1 0 2 Leontief’s inspiration was to find a model that uses genuine data from the real econ- omy. The table for 1958 contained 83 industries in the United States, with a “trans- actions table” of consumption and production for each one. The theory also reaches beyond (I−A)−1, to decide natural prices and questions of optimization. Normally la- bor is in limited supply and ought to be minimized. And, of course, the economy is not always linear. Example 3 (The prices in a closed input-output model ). Themodeliscalled“closed”wheneverythingproducedisalsoconsumed. Nothinggoes outside the system. In that case A goes back to a Markov matrix. The columns add up to 1. We might be talking about the value of steel and food and labor, instead of the number of units, The vector p represents prices instead of production levels. Suppose p is a vector of prices. Then Ap multiplies prices by amounts to give the 0 0 value of each product. That is a new set of prices which the system uses for the next set of values A2p . The question is whether the prices approach equilibrium. Are there 0 prices such that p=Ap, and does the system take us there? Yourecognize pasthe(nonnegative)eigenvectoroftheMarkovmatrixA,withλ=1. It is the steady state p , and it is approached from any starting point p . By repeating a ∞ 0 transaction over and over, the price tends to equilibrium."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. Prove that every third Fibonacci number in 0,1,1.2,3,... is even."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. Bernadelli studied a beetle “which lives three years only. and propagates in as third year.” They survive the first year with probability 1, and the second with probability 2 1, and then produce six females on the way out: 3   0 0 6   Beetle matrix A=1 0 0. 2 0 1 0 3 Show that A3 =I, and follow the distribution of 3000 beetles for six years. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. For the Fibonacci matrix A= 1 1 , compute A2, A3, and A4. Then use the text and 1 0 a calculator to find F . 20"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "4. Supposeeach“Gibonacci”number G istheaverageofthetwopreviousnumbers k+2 G and G . Then G = 1(G +G ): k+1 k k+2 2 k+1 k (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) G = 1G +1G G G k+2 2 k+1 2 k is k+2 = A k+1 . G =G G G k+1 k+1 k+1 k (a) Find the eigenvalues and eigenvectors of A. (b) Find the limit as n→∞ of the matrices An =SΛnS−1. (c) If G =0 and G =1, show that the Gibonacci numbers approach 2. 0 1 3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5. Diagonalize the Fibonacci matrix by completing S−1: (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) 1 1 λ λ λ 0 1 2 1 = . 1 0 1 1 0 λ 2 (cid:163) (cid:164) Do the multiplication SΛkS−1 1 to find its second component. This is the kth Fi- 0 bonacci number F =(λkłλk)/(λ łλ ). k 1 2 1 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "6. The numbersλk andλk satisfy the Fibonacci rule F =F +F : 1 2 k+2 k+1 k λk+2 =λk+1+λk and λk+2 =λk+1+λk. 1 1 1 2 2 2 Prove this by using the original equation for the λ’s (multiply it by λk). Then any combination ofλk andλk satisfies the rule. The combination F =(λk−λk)/(λ − 1 2 k 1 2 1 λ ) gives the right start of F =0 and F =1. 2 0 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 DifferenceEquationsandPowersAk 293"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "7. Lucas started with L = 2 and L = 1. The rule L = L +L is the same, so A 0 1 k+2 k+1 k is still Fibonacci’s matrix. Add its eigenvectors x +x : 1 2 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) √ (cid:35) (cid:34) √ (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) λ λ 1(1+ 5) 1(1− 5) 1 L 1 + 2 = 2 + 2 = = 1 . 1 1 1 1 2 L 0 Multiplying by Ak, the second component is L = λk +λk. Compute the Lucas k 1 2 number L slowly by L =L +L , and compute approximately byλ10. 10 k+2 k+1 k 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "8. Suppose there is an epidemic in which every month half of those who are well be- come sick, and a quarter of those who are sick become dead. Find the steady state for the corresponding Markov process      d 1 1 0 d k+1 4 k      s =0 3 1 s . k+1 4 2 k w 0 0 1 w k+1 2 k"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "9. Writethe3by3transitionmatrixforachemistrycoursethatistaughtintwosections, if every week 1 of those in Section A and 1 of those in Section B drop the course, 4 3 and 1 of each section transfer to the other section. 6"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "10. Find the limiting values of y and (k →∞) if k k y =.8y +.3z y =0 k+1 k k 0 z =.2y +.7z z =5. k+1 k k 0 Also find formulas for y and z from Ak =SΛkS−1. k k"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "11. (a) From the fact that column 1 + column 2 = 2(column 3), so the columns are linearly dependent find one eigenvalue and one eigenvector of A:   .2 .4 .3   A=.4 .2 .3. .4 .4 .4 (b) Find the other eigenvalues of A (it is Markov). (c) If u =(0,10,0), find the limit of Aku as k →∞. 0 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "12. SupposetherearethreemajorcentersforMove-It-Yourselftrucks. Everymonthhalf of those in Boston and in Los Angeles go to Chicago, the other half stay here they are, and the trucks in Chicago are split equally between Boston and Los Angeles Set up the 3 by 3 transition matrix A, and find the steady state u corresponding to the ∞ eigenvalueλ=1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "13. (a) In what range of a and b is the following equation a Markov process? (cid:34) (cid:35) (cid:34) (cid:35) a b 1 u =Au = u , u = . k+1 k k 0 1−a 1−b 1 (b) Compute u =SΛkS−1u for any a and b. k 0 (c) Under what condition on a and b does u approach a finite limit as k → ∞, and k what is the limit? Does A have to be a Markov matrix?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "14. MultinationalcompaniesintheAmericas,Asia,andEuropehaveassetsof$4trillion. At the start, $2 trillion are in the Americas and $2 trillion in Europe. Each year 1 the 2 American money stays home, and 1 goes to each of Asia and Europe. For Asia and 4 Europe, 1 stays home and 1 is sent to the Americas. 2 2 (a) Find the matrix that gives     Americas Americas      Asia  =A Asia  Europe Europe yeark+1 yeark . (b) Find the eigenvalues and eigenvectors of A. (c) Find the limiting distribution of the $4 trillion as the world ends. (d) Find the distribution of the $4 trillion at year k."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "15. If A is a Markov matrix, show that the sum of the components of Ax equals the sum of the components of x. Deduce that if Ax =λx with λ(cid:54)= 1, the components of the eigenvector add to zero. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "16. Thesolutiontodu/dt =Au= 0 −1 u(eigenvaluesiand−i)goesaroundinacircle: 1 0 u=(cost,sint). Supposeweapproximatedu/dt byforward,backward,andcentered differences F, B, C: (F) u −u =Au or u =(I+A)u (this is Euler’s method). n+1 n n n+1 n (B) u −u =Au or u =(I−A)−1u (backward Euler). n+1 n n+1 n+1 n (C) u −u = 1A(u +u ) or u =(I−1A)−1(I+1A)u . n+1 n 2 n+1 n n+1 2 2 n FindtheeigenvaluesofI+A,(IłA)−1,and(I−1A)−1(I+1A). Forwhichdifference 2 2 equation does the solution u stay on a circle? n"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "17. What values ofαproduce instability in v =α(v +w ), w =α(v +w )? n+1 n n n+1 n n"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "18. Find the largest a, b, c for which these matrices are stable or neutrally stable: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) a −.8 b .8 c .8 , , . .8 .2 0 .2 .2 c"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "19. Multiplying term by term, check that (IłA)(I+A+A2+···) = I. This series rep- resents (IłA)−1. It is nonnegative when A is nonnegative, provided it has a finite"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 DifferenceEquationsandPowersAk 295 sum; the condition for that is λ < 1. Add up the infinite series, and confirm that max it equals (IłA)−1, for the consumption matrix   0 1 1   A=0 0 1 which hasλ =0. max 0 0 0 (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "20. For A= 0 .2 , find the powers Ak (including A0) and show explicitly that their sum 0 .5 agrees with (I−A)−1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "21. Explain by mathematics or economics why increasing the “consumption matrix” A must increaset =λ (and slow down the expansion). max 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "22. What are the limits as k →∞ (the steady states) of the following? (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) k 1 k 0 k .4 .2.6 .8 , .4 .2.6 .8 , .4 .2.6 .8 . 0 1 Problems 23–29 are about A=SΛS−1 and Ak =SΛkS−1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "23. Diagonalize A and compute SΛkS−1 to prove this formula for Ak: (cid:34) (cid:35) (cid:34) (cid:35) 3 2 1 5k+1 5k−1 A= has Ak = . 2 3 2 5k−1 5k+1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "24. Diagonalize B and compute SΛkS−1 to prove this formula for Bk: (cid:34) (cid:35) (cid:34) (cid:35) 3 1 3k 3k−2k B= has Bk = . 0 2 0 2k"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "25. The eigenvalues of A are 1 and 9, the eigenvalues of B are ł1 and 9: (cid:34) (cid:35) (cid:34) (cid:35) 5 4 4 5 A= and B= . 4 5 5 4 √ FindamatrixsquarerootofAfromR=S ΛS−1,Whyistherenorealmatrixsquare root of B?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "26. IfAandBhavethesameλ’swiththesamefullsetofindependenteigenvectors,their factorizations into are the same. So A=B."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "27. Suppose A and B have the same full set of eigenvectors, so that A = SΛ S−1 and 1 B=SΛ S−1. Prove that AB=BA. 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "28. (a) When do the eigenvectors forλ=0 span the nullspace N(A)? (b) When do all the eigenvectors forλ(cid:54)=0 span the column space C(A)?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "29. The powers Ak approach zero if all |λ| < 1, and they blow up if any |λ| > 1. Peter i i Lax gives four striking examples in his book Linear Algebra. (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 3 2 3 2 5 7 5 6.9 A= B= C = D= 1 4 −5 −3 −3 −4 −3 −4 (cid:107)A1024(cid:107)>10700 B1024 =I C1024 =−C (cid:107)D1024(cid:107)<10−78 Find the eigenvaluesλ=eiθ of B andC to show that B4 =I andC3 =−I."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 Differential Equations and eAt Wherever you find a system of equations, rather than a single equation, matrix theory hasaparttoplay. Fordifferenceequations,thesolutionu =Aku dependedontheowen k 0 of A. For differential equations, the solution u(t)=eAtu(0) depends on the exponential of A. To define this exponential. and to understand it, we turn right away to an example: (cid:34) (cid:35) du −2 1 Differential equation =Au= u. (1) dt 1 −2 The first step is always to find the eigenvalues (ł1 and −3) and the eigenvectors: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 1 A =(−1) and A =(−3) . 1 1 −1 −1 Then several approaches lead to u(t). Probably the best is to match the general solution to the initial vector u(0) att =0. The general solution is a combination of pure exponential solutions. These are so- lutions of the special form ceλtx, where λ is an eigenvalue of A and x is its eigenvec- tor. These pure solutions satisfy the differential equation, since d/dt(ceλtx)=A(ceλtx). (They were our introduction to eigenvalues at the start of the chapter.) In this 2 by 2 example, there are two pure exponentials to be combined: (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) 1 1 e−t c Solution u(t)=c eλ 1tx +c eλ 2tx or u= 1 . (2) 1 1 2 2 1 −1 e−3t c 2 At time zero, when the exponentials are e0 =1, u(0) determines c and c : 1 2 (cid:34) (cid:35)(cid:34) (cid:35) 1 1 c 1 Initial condition u(0)=c x +c x = =Sc. 1 1 2 2 1 −1 c 2 You recognize S, the matrix of eigenvectors. The constants c=S−1u(0) are the same as theywerefordifferenceequations. Substitutingthembackintoequation(2),thesolution"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 DifferentialEquationsandeAt 297 is (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 1 1 e−t c e−t u(t)= 1 =S S−1u(0). (3) 1 −1 e−3t c e−3t 2 Here is the fundamental formula of this section: SeΛtS−1u(0) solves the differential equation, just as SΛkS−1u solved the difference equation: 0 (cid:34) (cid:35) (cid:34) (cid:35) −1 e−t u(t)=SeΛtS−1u(0) with Λ= and eΛt = . (4) −3 e−3t There are two more things to be done with this example. One is to complete the mathematics, by giving a direct definition of the exponential of a matrix. The other is to give a physical interpretation of the equation and its solution. It is the kind of differential equation that has useful applications. The exponential of a diagonal matrix Λ is easy; eΛt just has the n numbers eλt on the diagonal. For a general matrix A, the natural idea is to imitate the power series ex =1+x+x2/2!+x3/3!+···. If we replace x by At and 1 by I, this sum is an n by n matrix: (At)2 (At)3 Matrix exponential eAt =I+At+ + +···. (5) 2! 3! The series always converges, and its sum eAt has the right properties: d (eAs)(eAt)=(eA(s+t)), (eAt)(e−At)=I, and (eAt)=AeAt. (6) dt From the last one, u(t) = eAtu(0) solves the differential equation. This solution must be the same as the form SeΛtS−1u(0) used for computation. To prove directly that those solutions agree, remember that each power (SΛS−1)k telescopes into Ak = SΛkS−1 (be- cause S−1 cancels S). The whole exponential is diagonalized by S: SΛ2S−1t2 SΛ3S−1t3 eAt =I+SΛS−1t+ + +··· 2! 3! (cid:181) (cid:182) (Λt)2 (Λt)3 =S I+Λt+ + +··· S−1 =SeΛtS−1. 2! 3! (cid:163) (cid:164) (cid:163) (cid:164) Example 1. In equation (1), the exponential of A= −2 1 has Λ= 1 : 1 −2 −3 (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) −1 1 1 e−t 1 1 1 e−t+e−3t e−t−e−3t eAt =SeΛtS−1 = = . 1 −1 e−3t 1 −1 2 e−t−e−3t e−t+e−3t Att =0wegete0=I. TheinfiniteserieseAt givestheanswerforallt,butaseriescanbe hard to compute. The form SeΛtS−1 gives the same answer when A can be diagonalized; it requires n independent eigenvectors in S. This simpler form leads to a combination of n exponentials eλtx—which is the best solution of all: 5L If A can be diagonalized, A=SΛS−1, then du/dt =Au has the solution u(t)=eAtu(0)=SeΛtS−1u(0). (7) The columns of S are the eigenvectors x ,...,x of A. Multiplying gives 1 n    eλ 1t u(t)= x ··· x    ...  S−1u(0) 1 n (8) eλnt =c eλ 1tx +···+c eλntx =combination of eλtx. 1 1 n n The constants c that match the initial conditions u(0) are c=S−1u(0). i This gives a complete analogy with difference equations and SΛS−1u . In both cases 0 we assumed that A could be diagonalized. since otherwise it has fewer than n eigenvec- tors and we have not found enough special solutions. The missing Solutions do exist, but they are more complicated than pure exponentials eλtx. They involve “generalized eigenvectors” and factors like teλt. (To compute this defective case we can use the Jor- danforminAppendixB,andfindeJt.) Theformulau(t)=eAtu(0)remainscompletely correct. The matrix eAt is never singular. One proof is to look at its eigenvalues; if λ is an eigenvalue of A, then eλt is the corresponding eigenvalue of eAt—and eλt can never be zero. Another approach is to compute the determinant of the exponential: deteAt =eλ 1teλ 2t···eλnt =etrace(At). (9) Quick proof that eAt is invertible: Just recognize e−At as its inverse. This invertibility is fundamental for differential equations. If n solutions are linearly independent at t =0, they remain linearly independent forever. If the initial vectors are v ,...,v , we can put the solutions eAtv into a matrix: 1 n (cid:104) (cid:105) (cid:104) (cid:105) eAtv ··· eAtv =eAt v ··· v . 1 n 1 n The determinant of the left-hand side is the Wronskian. It never becomes zero, because it is the product of two nonzero determinants. Both matrices on the right-hand side are invertible. Remark. Not all differential equations come to us as a first-order system du/dt = Au. We may start from a single equation of higher order, like y(cid:48)(cid:48)(cid:48)−3y(cid:48)(cid:48)+2y(cid:48) =0. To convert to a 3 by 3 system, introduce v = y(cid:48) and w = v(cid:48) as additional unknowns along with y itself. Then these two equations combine with the original one to give u(cid:48) =Au:    y(cid:48) =v 0 1 0 y    v(cid:48) =w or u(cid:48) =0 0 1v=Au. w(cid:48) =3w−2v 0 −2 3 w"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 DifferentialEquationsandeAt 299 concentration 0 v w 0 S S S S 0 1 2 3 Figure5.1: Amodelofdiffusionbetweenfoursegments. We are back to a first-order system. The problem can be solved two ways. In a course on differential equations, you would substitute y=eλt into y(cid:48)(cid:48)(cid:48)−3y(cid:48)(cid:48)+2y(cid:48) =0: (λ3−3λ2+2λ)eλt =0 or λ(λ−1)(λ−2)eλt =0. (10) Thethreepureexponentialsolutionsarey=e0t,y=et,andy=e2t. Noeigenvectorsare involved. In a linear algebra course, we find the eigenvalues of A:   −λ 1 0   det(A−λI)= 0 −λ I =−λ3+3λ2−2λ=0. (11) 0 −2 3−λ Equations (10) and (11) are the same! The same three exponents appear: λ=0,λ=1, and λ= 2. This is a general rule which makes the two methods consistent; the growth rates of the solutions stay fixed when the equations change form. It seems to us that solving the third-order equation is quicker. (cid:163) (cid:164) The physical significance of du/dt = −2 1 u is easy to explain and at the same 1 −2 time genuinely important. This differential equation describes a process of diffusion. Divide an infinite pipe into four segments (Figure 5.1). At time t = 0, the middle seg- ments contain concentrations v(0) and w(0) of a chemical. At each timet, the diffusion rate between two adjacent segments is the difference in concentrations. Within each segment, the concentration remains uniform (zero in the infinite segments). The process is continuous in time but discrete in space; the unknowns are v(t) and w(t) in the two inner segments S and S . 1 2 The concentration v(t) in S is changing in two ways. There is diffusion into S , and 1 0 into or out of S . The net rate of change is dv/dt, and dw/dt is similar: 2 dv Flow rate into S =(w−v)+(0−v) 1 dt dw Flow rate into S =(0−w)+(v−w). 2 dt This law of diffusion exactly matches our example du/dt =Au: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) v du −2v+w −2 1 u= and = = u. w dt v−2w 1 −2 The eigenvalues −1 and −3 will govern the solution. They give the rate at which the concentrations decay, and λ is the more important because only an exceptional set of 1 starting conditions can lead to “superdecay” at the rate e−3t, In fact, those conditions must come from the eigenvector (1,−1). If the experiment admits only nonnegative concentrations, superdecay is impossible and the limiting rate must be e−t. The solution that decays at this slower rate corresponds to the eigenvector (1,1). Therefore the two concentrations will become nearly equal (typical for diffusion) ast →∞. One more comment on this example: It is a discrete approximation, with only two unknowns, to the continuous diffusion described by this partial differential equation: ∂u ∂2u Heat equation = . ∂t ∂x2 Thatheatequationisapproachedbydividingthepipeintosmallerandsmallersegments, of length 1/N. The discrete system with N unknowns is governed by      u −2 1 u 1 1      d  ·   1 −2 ·  ·   =  =Au. (12) dt  ·   · · 1  ·  u 1 −2 u N N Thisisthefinitedifferencematrixwiththe1,−2,1pattern. TherightsideAuapproaches the second derivative d2u/dx2, after a scaling factor N2 comes from the flow problem. In the limit as N → ∞, we reach the heat equation ∂u/∂t = ∂2u/∂x2. Its solutions are still combinations of pure exponentials, but now there are infinitely many. Instead of eigenvectors from Ax =λx, we have eigenfunctions from d2u/dx2 =λu. Those are u(x)=sinnπx withλ=−n2π2. Then the solution to the heat equation is ∞ u(t)= ∑ c e−n2π2tsinnπx. n n=1 The constants c are determined by the initial condition. The novelty is that the eigen- n vectors are functions u(x), because the problem is continuous and not discrete. stability of differential equations Just as for difference equations. the eigenvalues decide how u(t) behaves as t → ∞. As long as A can be diagonalized, there will be n pure exponential solutions to the differential equation, and any specific solution u(t) is some combination u(t)=SeΛtS−1u =c egl 1tx +···+c eglntx . 0 1 1 n n Stabilityisgovernedbythosefactorsegl it. Iftheyallapproachzero,thenu(t)approaches zero: if they all stay bounded, then u(t) stays bounded; if one of them blows up, then except for very special starting conditions the solution will blow up. Furthermore, the sizeofeλt dependsonlyontherealpartofλ. Itisonlytherealpartsoftheeigenvalues that govern stability: Ifλ=a+ib, then eλt =eateibt =eat(cosbt+isinbt) and the magnitude is |eλt|=eat."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 DifferentialEquationsandeAt 301 This decays for a<0, it is constant for a=0, and it explodes for a>0. The imaginary part is producing oscillations, but the amplitude comes from the real part. 5M The differential equation du/dt =Au is stable and eAt →0 whenever all Reλ <0, i neutrally stable when all Reλ ≤0 and Reλ =0, and i 1 unstable and eAt is unbounded if any eigenvalue has Reλ >0. i In some texts the condition Reλ<0 is called asymptotic stability, because it guarantees decay for large timest. Our argument depended on having n pure exponential solutions, but even if A is not diagonalizable (and there are terms like teλt) the result is still true: All solutions approach zero if and only if all eigenvalues have Reλ<0. Stability is especially easy to decide for a 2 by 2 system (which is very common in applications). The equation is (cid:34) (cid:35) du a b = u. dt c d and we need to know when both eigenvalues of that matrix have negative real parts. (Note again that the eigenvalues can be complex numbers.) The stability tests are Reλ <0 The trace a+d must be negative. 1 Reλ <0 The determinant ad−bc must be positive. 2 When the eigenvalues are real, those tests guarantee them to be negative. Their product is the determinant; it is positive when the eigenvalues have the same sign. Their sum is the trace; it is negative when both eigenvalues are negative. When the eigenvalues are a complex pair x±iy, the tests still succeed. The trace is their sum 2x (which is < 0) and the determinant is (x+iy)(x−iy) = x2+y2 > 0. Figure 5.2 shows the one stable quadrant, trace <0 and determinant >0. It also shows the parabolic boundary line between real and complex eigenvalues. The reason for the parabola is in the quadratic equation for the eigenvalues: (cid:34) (cid:35) a−λ b det =λ2−(trace)λ+(det)=0. (13) c d−λ The quadratic formula forλ leads to the parabola (trace)2 =4(det): (cid:183) (cid:113) (cid:184) 1 λ andλ = trace± (trace)2−4(det) . (14) 1 2 2 Above the parabola, the number under the square root is negative—soλ is not real. On the parabola, the square root is zero and λ is repeated. Below the parabola the square roots are real. Every symmetric matrix has real eigenvalues, since if b=c, then (trace)2−4(det)=(a+d)2−4(ad−b2)=(a−d)2+4b2 ≥0. determinant D λ 1 = λ 2 and both Reλ < 0 both Reλ > 0 T2 = 4D stable unstable complex eigenvalues both λ < 0 both λ > 0 real and stable real and unstable trace T det<0givesλ <0andλ >0: realandunstable 1 2 Figure5.2: Stabilityandinstabilityregionsfora2by2matrix. For complex eigenvalues, b and c have opposite signs and are sufficiently large. Example 2. One from each quadrant: only #2 is stable: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 −1 0 1 0 −1 0 0 2 0 −2 0 −2 0 2 On the boundaries of the second quadrant, the equation is neutrally stable. On the hori- zontalaxis,oneeigenvalueiszero(becausethedeterminantisλ λ =0). Onthevertical 1 2 axis above the origin, both eigenvalues are purely imaginary (because the trace is Zero). Crossing those axes are the two ways that stability is lost. The n by n case is more difficult. A test for Reλ <0 came from Routh and Hurwitz, i who found a series of inequalities on the entries a . I do not think this approach is ij muchgoodforalargematrix;thecomputercanprobablyfindtheeigenvalueswithmore certainty than it can test these inequalities. Lyapunov’s idea was to find a weighting matrix W so that the weighted length (cid:107)Wu(t)(cid:107) is always decreasing. If there exists such a W, then (cid:107)Wu(cid:107) will decrease steadily to zero, and after a few ups and downs u must get there too (stability). The real value of Lyapunov’s method is for a nonlinear equation—then stability can be proved without knowing a formula for u(t). (cid:163) (cid:164) Example 3. du/dt = 0 −1 u sends u(t) around a circle, starting from u(0)=(1,0). 1 0 Since trace =0 and det=1, we have purely imaginary eigenvalues: (cid:34) (cid:35) −λ −1 =λ2+1=0 so λ=+i and −i. 1 −λ The eigenvectors are (1,−i) and (1,i). and the solution is (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 1 u(t)= eit + e−it . 2 −i 2 i"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 DifferentialEquationsandeAt 303 That is correct but not beautiful. By substituting cost±isint for eit and e−it, real num- bers will reappear: The circling solution is u(t)=(cost,sint). Starting from a different u(0)=(a,b), the solution u(t) ends up as (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) acost−bsint cost −sint a u(t)= = . (15) bcost+asint sint cost b There we have something important! The last matrix is multiplying u(0), so it must be the exponential eAt. (Remember that u(t) = eAtu(0).) That matrix of cosines and sines is our leading example of an orthogonal matrix. The columns have length 1, their inner product is zero, and we have a confirmation of a wonderful fact: If A is skew-symmetric (AT =−A) then eAt is an orthogonal matrix. AT =−A gives a conservative system. No energy is lost in damping or diffusion: AT =−A, (eAt)T =e−At, and (cid:107)eAtu(0)(cid:107)=(cid:107)u(0)(cid:107). That last equation expresses an essential property of orthogonal matrices. When they multiply a vector, the length is not changed. The vector u(0) is just rotated, and that describes the solution to du/dt =Au: It goes around in a circle. In this very unusual case, eAt can also be recognized directly from the infinite series. (cid:163) (cid:164) Note that A= 0 −1 has A2 =−I, and use this in the series for eAt: 1 0 (cid:179) (cid:180) (cid:179) (cid:180) (At)2 (At)3 1−t2 +··· −t+t3 −··· I+At+ + +···=(cid:179) 2 (cid:180) (cid:179) 6 (cid:180)  2 6 t−t3 +··· 1−t2 +··· 6 2 (cid:34) (cid:35) cost −sint = sint cost (cid:34) (cid:35) −2 1 Example 4. Thediffusionequationisstable: A= hasλ=−1andλ=−3. 1 −2 Example 5. If we close off the infinite segments, nothing can escape: (cid:34) (cid:35) du −1 1 dv/dt =w−v = u or dt 1 −1 dw/dt =v−w. This is a continuous Markov process. Instead of moving every year, the particles move every instant. Their total number v+w is constant. That comes from adding the two equations on the right-hand side: the derivative of v+w is zero. A discrete Markov matrix has its column sums equal to λ = 1. A continuous max Markov matrix, for differential equations, has its column sums equal to λ = 0. A is max a discrete Markov matrix if and only if B = A−I is a continuous Markov matrix. The Figure5.3: Theslowandfastmodesofoscillation. steady state for both is the eigenvector for λ . It is multiplied by 1k =1 in difference max equations and by e0t =1 in differential equations, and it doesn’t move. In the example, the steady state has v=w. Example 6. In nuclear engineering, a reactor is called critical when it is neutrally stable; the fission balances the decay. Slower fission makes it stable, or subcritical, and eventually it runs down. Unstable fission is a bomb. Second-Order Equations The laws of diffusion led to a first-order system du/dt =Au. So do a lot of other appli- cations, in chemistry, in biology, and elsewhere, but the most important law of physics does not. It is Newton’s law F = ma, and the acceleration a is a second derivative. In- ertial terms produce second-order equations (we have to solve d2u/dt2 = Au instead of du/dt = Au), and the goal is to understand how this switch to second derivatives alters the solution5. It is optional in linear algebra, but not in physics. The comparison will be perfect if we keep the same A: (cid:34) (cid:35) d2u −2 1 =Au= u. (16) dt2 1 −2 Two initial conditions get the system started—the “displacement” u(0) and the “veloc- ity” u(cid:48)(0). To match these conditions, there will be 2n pure exponential solutions. Suppose we useωrather thanλ, and write these special solutions as u=eiωtx. Sub- stituting this exponential into the differential equation, it must satisfy d2 (eiωtx)=A(eiωtx), or −ω2x=Ax. (17) dt2 The vector x must be an eigenvector of A, exactly as before. The corresponding eigen- value is now −ω2, so the frequency ω is connected to the decay rate λ by the law 5Fourthderivativesarealsopossible,inthebendingofbeams,butnatureseemstoresistgoinghigherthanfour."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. Following the first example in this section, find the eigenvalues and eigenvectors, and the exponential eAt, for (cid:34) (cid:35) −1 1 A= . 1 −1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. For the previous matrix, write the general solution to du/dt = Au, and the specific solution that matches u(0) = (3,1). What is the steady state as t → ∞? (This is a continuous Markov process;λ=0 in a differential equation corresponds toλ=1 in a difference equation, since e0t =1.)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. Suppose the time direction is reversed to give the matrix −A: (cid:34) (cid:35) (cid:34) (cid:35) du 1 −1 3 = u with u = . 0 dt −1 1 1 Find u(t) and show that it blows up instead of decaying as t → ∞. (Diffusion is irreversible, and the heat equation cannot run backward.)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "4. If P is a projection matrix, show from the infinite series that eP ≈I+1.718P. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5. A diagonal matrix like Λ = 1 0 satisfies the usual rule eΛ(t+T) = eΛteΛT, because 0 2 the rule holds for each diagonal entry. (a) Explain why eA(t+T) =eAteAT, using the formula eAt =SeΛtS−1. (b) Show that eA+B =eAeB is not true for matrices, from the example (cid:34) (cid:35) (cid:34) (cid:35) 0 0 0 −1 A= B= (use series for eA and eB). 1 0 0 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 DifferentialEquationsandeAt 307"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "6. The higher order equation y(cid:48)(cid:48)+y=0 can be written as a first-order system by intro- ducing the velocity y(cid:48) as another unknown: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) d y y(cid:48) y(cid:48) = = . dt y(cid:48) y(cid:48)(cid:48) −y If this is du/dt = Au, what is the 2 by 2 matrix A? Find its eigenvalues and eigen- vectors, and compute the solution that starts from y(0)=2, y(cid:48)(0)=0."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "7. Convert y(cid:48)(cid:48) =0 to a first-order system du/dt =Au: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) d y y(cid:48) 0 1 y = = . dt y(cid:48) 0 0 0 y(cid:48) This 2 by 2 matrix A has only one eigenvector and cannot be diagonalized. Compute eAt from the series I+At+··· and write the solution eAtu(0) starting from y(0)=3, y(cid:48)(0)=4. Check that your (y,y(cid:48)) satisfies y(cid:48)(cid:48) =0."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "8. Suppose the rabbit population r and the wolf population w are governed by dr =4r−2w dt dw =r+w. dt (a) Is this system stable, neutrally stable, or unstable? (b) If initially r =300 and w=200, what are the populations at timet? (c) After a long time, what is the proportion of rabbits to wolves?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "9. Decide the stability of u(cid:48) =Au for the following matrices: (cid:34) (cid:35) (cid:34) (cid:35) 2 3 1 2 (a) A= . (b) A= . 4 5 3 −1 (cid:34) (cid:35) (cid:34) (cid:35) 1 1 −1 −1 (c) A= . (d) A= . 1 −2 −1 −1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "10. Decide on the stability or instability of dv/dt = w, dw/dt = v. Is there a solution that decays?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "11. From their trace and determinant, at what time t do the following matrices change betweenstablewithrealeigenvalues,stablewithcomplexeigenvalues,andunstable? (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 −1 0 4−t t −1 A = , A = , A = . 1 2 3 t −1 1 −2 1 t"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "12. Find the eigenvalues and eigenvectors for   0 3 0 du   =Au=−3 0 4u. dt 0 −4 0 Why do you know, without computing, that eAt will be an orthogonal matrix and (cid:107)u(t)(cid:107)2 =u2+u2+u2 will be constant? 1 2 3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "13. For the skew-symmetric equation    0 c −b u 1 du    =Au=−c 0 a u , 2 dt b −a 0 u 3 (a) write out u(cid:48), u(cid:48), u(cid:48) and confirm that u(cid:48)u +u(cid:48)u +u(cid:48)u =0. 1 2 3 1 1 2 2 3 3 (b) deduce that the length u2+u2+u2 is a constant. 1 2 3 (c) find the eigenvalues of A. The solution will rotate around the axis w=(a,b,c), because Au is the “cross prod- uct” u×w—which is perpendicular to u and w."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "14. What are the eigenvalues λ and frequencies ω, and the general solution, of the fol- lowing equation? (cid:34) (cid:35) d2u −5 4 = u. dt2 4 −5"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "15. Solve the second-order equation (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) d2u −5 −1 1 0 = u with u(0)= and u(cid:48)(0)= . dt2 −1 −5 0 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "16. Inmostapplicationsthesecond-orderequationlookslikeMu(cid:48)(cid:48)+Ku=0,withamass matrix multiplying the second derivatives. Substitute the pure exponential u=eiωtx and find the “generalized eigenvalue problem” that must be solved for the frequency ωand the vector x."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "17. With a friction matrix F in the equation u(cid:48)(cid:48)+Fu(cid:48)−Au = 0, substitute a pure expo- nential u=eλtx and find a quadratic eigenvalue problem forλ. √"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "18. For equation (16) in the text, withω=1 and 3, find the motion if the first mass is hit att =0; u(0)=(0,0) and u(cid:48)(0)=(1,0)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "19. Every 2 by 2 matrix with trace zero can be written as (cid:34) (cid:35) a b+c A= . b−c −a Show that its eigenvalues are real exactly when a2+b2 ≥c2."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 DifferentialEquationsandeAt 309"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "20. By back-substitution or by computing eigenvectors, solve     1 2 1 1 du     =0 3 6u with u(0)=0. dt 0 0 4 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "21. Findλ’s and x’s so that u=eλtx solves (cid:34) (cid:35) du 4 3 = u. dt 0 1 What combination u=c eλ 1tx +c eλ 2tx starts from u(0)=(5,−2)? 1 1 2 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "22. Solve Problem 21 for u(t)=(y(t),z(t)) by back-substitution: dz First solve =z, starting from z(0)=−2. dt dy Then solve =4y+3z, starting from y(0)=5. dt The solution for y will be a combination of e4t and et."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "23. Find A to change y(cid:48)(cid:48) =5y(cid:48)+4y into a vector equation for u(t)=(y(t),y(cid:48)(t)): (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) du y(cid:48) y = = =Au. dt y(cid:48)(cid:48) y(cid:48) What are the eigenvalues of A? Find them also by substituting y=eλt into the scalar equation y(cid:48)(cid:48) =5y(cid:48)+4y."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "24. A door is opened between rooms that hold v(0)=30 people and w(0)=10 people. The movement between rooms is proportional to the difference v−w: dv dw =w−v and =v−w. dt dt Showthatthetotalv+wisconstant(40people). Findthematrixindu/dt =Au, and its eigenvalues and eigenvectors. What are v and w att =1?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "25. Reverse the diffusion of people in Problem 24 to du/dt =−Au: dv dw =v−w and =w−v. dt dt Thetotalv+wstillremainsconstant. Howaretheλ’schangednowthatAischanged to −A? But show that v(t) grows to infinity from v(0)=30."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "26. The solution to y(cid:48)(cid:48) =0 is a straight line y=C+Dt. Convert to a matrix equation: (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) d y 0 1 y y y(0) = has the solution =eAt . dt y(cid:48) 0 0 y(cid:48) y(cid:48) y(cid:48)(0) This matrix A cannot be diagonalized. Find A2 and compute eAt =I+At+ 1A2t2+ 2 ···. Multiply your eAt times (y(0),y(cid:48)(0)) to check the straight line y(t) = y(0)+ y(cid:48)(0)t."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "27. Substitute y = eλt into y(cid:48)(cid:48) = 6y(cid:48)−9y to show that λ = 3 is a repeated root. This is trouble; we need a second solution after e3t. The matrix equation is (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) d y 0 1 y = . dt y(cid:48) −9 6 y(cid:48) Show that this matrix has λ = 3,3 and only one line of eigenvectors. Trouble here too. Show that the second solution is y=te3t."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "28. Figure out how to write my(cid:48)(cid:48)+by(cid:48)+ky=0 as a vector equation Mu(cid:48) =Au."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "29. (a) Find two familiar functions that solve the equation d2y/dt2 = −y. Which one starts with y(0)=1 and y(cid:48)(0)=0? (b) This second-order equation y(cid:48)(cid:48) =−y produces a vector equation u(cid:48) =Au: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) y du y(cid:48) 0 1 y u= = = =Au. y(cid:48) dt y(cid:48)(cid:48) −1 0 y(cid:48) Put y(t) from part (a) into u(t)=(y,y(cid:48)). This solves Problem 6 again."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "30. Aparticularsolutiontodu/dt =Au−bisu =A−1b,ifAisinvertible. Thesolutions p to du/dt =Au give u . Find the complete solution u +u to n p n (cid:34) (cid:35) (cid:34) (cid:35) du du 2 0 8 (a) =2u−8. (b) = u− . dt dt 0 3 6"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "31. If c is not an eigenvalue of A, substitute u = ectv and find v to solve du/dt = Au− ectb. This u = ectv is a particular solution. How does it break down when c is an eigenvalue?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "32. Find a matrix A to illustrate each of the unstable regions in Figure 5.2: (a) λ <0 andλ >0. 1 2 (b) λ >0 andλ >0. 1 2 (c) Complexλ’s with real part a>0. Problems 33–41 are about the matrix exponential eAt."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 DifferentialEquationsandeAt 311"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "33. WritefivetermsoftheinfiniteseriesforeAt. Takethet derivativeofeachterm. Show that you have four terms of AeAt. Conclusion: eAtu(0) solves u(cid:48) =Au. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "34. The matrix B= 0 −1 has B2 =0. Find eBt from a (short) infinite series. Check that 0 0 the derivative of eBt is BeBt."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "35. Startingfromu(0),thesolutionattimeT iseATu(0). Goanadditionaltimet toreach eAt(eATu(0)). This solution at time t+T can also be written as . Conclusion: eAt times eAT equals . (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "36. Write A= 1 1 in the form SΛS−1. Find eAt from SeΛtS−1. 0 0 (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "37. If A2 =A, show that the infinite series produces eAt =I+(et−1)A. For A= 1 1 in 0 0 Problem 36, this gives eAt ="
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "38. GenerallyeAeB isdifferentfromeBeA. TheyarebothdifferentfromeA+B. Checkthis using Problems 36–37 and 34: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 1 0 −1 1 0 A= B= A+B= . 0 0 0 0 0 0 (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "39. Write A = 1 1 as SΛS−1. Multiply SeΛtS−1 to find the matrix exponential eAt. 0 3 Check eAt =I whent =0. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "40. Put A= 1 3 into the infinite series to find eAt. First compute A2: 0 0 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 t 3t 1 et eAt = + + +···= . 0 1 0 0 2 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "41. Give two reasons why the matrix exponential eAt is never singular: (a) Write its inverse. (b) Write its eigenvalues. If Ax=λx then eAtx= x."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "42. Find a solution x(t), y(t) of the first system that gets large as t → ∞. To avoid this instability a scientist thought of exchanging the two equations! dx/dt = 0x − 4y dy/dt = −2x + 2y becomes dy/dt = −2x + 2y dx/dt = 0x − 4y. (cid:163) (cid:164) Now the matrix −2 2 is stable. It hasλ<0. Comment on this craziness. 0 −4"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "43. From this general solution to du/dt =Au, find the matrix A: (cid:34) (cid:35) (cid:34) (cid:35) 2 1 u(t)=c e2t +c e5t . 1 2 1 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.5 Complex Matrices ItisnolongerpossibletoworkonlywithrealvectorsandrealmatricesInthefirsthalfof this book, when the basic problem was Ax−b, the solution was real when A and b were real. Complex numbers could have been permitted. but would have contributed nothing new. Now we cannot avoid them. A real matrix has real coefficients in det(A−λI), but the eigenvalues (as in rotations) may be complex. We now introduce the space Cn of vectors with n complex components. Addition and matrix multiplication follow the same rules as before. Length is computed differently. Theoldway,thevectorinC2 withcomponents(1,i)wouldhavezerolength: 12+i2=0, not good. The correct length squared is 12+|i|2 =2. This change to (cid:107)x(cid:107)2 =|x |2+···+|x |2 forces a whole series of other changes. The 1 n inner product, the transpose, the definitions of symmetric and orthogonal matrices, all need to be modified for complex numbers. The new definitions coincide with the old when the vectors and matrices are real. We have listed these changes in a table at the end of the section. and we explain them as we go. That table virtually amounts to a dictionary for translating real into complex. We hope it will be useful to the reader. We particularly want to find out about symmetric matrices and Hermitian matrices: Where are their eigenvalues, and what is special about their eigenvectors? Forpractical purposes, those are the most important questions in the theory of eigenvalues. We call attention in advance to the answers:"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. Every symmetric matrix (and Hermitian matrix) has real eigenvalues."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. Its eigenvectors can be chosen to be orthonormal. Strangely,toprovethattheeigenvaluesarerealwebeginwiththeoppositepossibility— and that takes us to complex numbers, complex vectors, and complex matrices. Complex Numbers and Their Conjugates Probably the reader has already met complex numbers; a review is easy to give. The importantideasarethecomplexconjugatex¯andtheabsolutevalue|x|. Everyoneknows that whatever i is, it satisfies the equation i2 = −1. It is a pure imaginary number, and so are its multiples ib; b is real. The sum a+ib is a complex number, and it is plotted in a natural way on the complex plane (Figure 5.4). The real numbers a and the imaginary numbers ib are special cases of complex num- bers; they lie on the axes. Two complex numbers are easy to add: Complex addition (a+ib)+(c+id)=(a+c)+i(b+d)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.5 ComplexMatrices 313 imaginary axis b ba+ib = reiθ r = |a+ib| r2 = a2 +b2 r θ −θ a real axis r complex conjugate −b ba−ib = a+ib = re−iθ Figure5.4: Thecomplexplane,witha+ib=reiθanditsconjugatea−ib=re−iθ. Multiplying a+ib times c+id uses the rule that i2 =−1: Multiplication (a+ib)(c+id)=ac+ibc+iad+i2bd =(ac−bd)+i(bc+ad). The complex conjugate of a+ib is the number a−ib. The sign of the imaginary part is reversed. It is the mirror image across the real axis; any real number is its own conjugate, since b=0. The conjugate is denoted by a bar or a star: (a+ib)∗ =a+ib= a−ib. It has three important properties:"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. The conjugate of a product equals the product of the conjugates: (a+ib)(c+id)=(ac−bd)−i(bc+ad)=(a+ib)(c+id). (1)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. The conjugate of a sum equals the sum of the conjugates: (a+c)+i(b+d)=(a+c)−i(b+d)=(a+ib)+(c+id)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. Multiplying any a+ib by its conjugate a−ib produces a real number a2+b2: Absolute value (a+ib)(a−ib)=a2+b2 =r2. (2) √ This distance r is the absolute value |a+ib|= a2+b2. Finally, trigonometry connects the sides a and b to the hypotenuse r by a = rcosθ and b=rsinθ. Combining these two equations moves us into polar coordinates: Polar form a+ib=r(cosθ+isinθ)=reiθ. (3) The most important special case is when r = 1. Then a+ib is eiθ = cosθ+isinθ. It falls on the unit circle in the complex plane. As θ varies from 0 to 2π, this number eiθ (cid:112) circles around zero at the constant radial distance |eiθ|= cos2θ+sin2θ=1. Example 1. x=3+4i times its conjugate x=3−4i is the absolute value squared: xx=(3+4i)(3−4i)=25=|x|2 so r =|x|=5. To divide by 3+4i, multiply numerator and denominator by its conjugate 3−4i: 2+i 2+i 3−4i 10−5i = = . 3+4i 3+4i3−4i 25 In polar coordinates, multiplication and division are easy: reiθ times Reiα has absolute value rR and angleθ+α. reiθ divided by Reiα has absolute value r/R and angleθ−α. Lengths and Transposes in the Complex Case Wereturntolinearalgebra,andmaketheconversionfromrealtocomplex. Bydefinition, the complex vector space Cn contains all vectors x with n complex components:   x 1   x 2 Complex vector x=  . . .   with components x j =a j+ib j. x n Vectors x and y are still added component by component. Scalar multiplication cx is now done with complex numbers c. The vectors v ,...,v are linearly dependent if 1 k some nontrivial combination gives c v +...+c v = 0; the c may now be complex. 1 1 k k j The unit coordinate vectors are still in Cn; they are still independent; and they still form a basis. Therefore Cn is a complex vector space of dimension n. In the new definition of length, each x2 is replaced by its modulus |x |2: j j Length squared (cid:107)x(cid:107)2 =|x |2+···+|x |2. (4) 1 n (cid:34) (cid:35) (cid:34) (cid:35) 1 2+i Example 2. x= and (cid:107)x(cid:107)2 =2; y= and (cid:107)y(cid:107)2 =25. i 2−4i For real vectors there was a close connection between the length and the inner product: (cid:107)x(cid:107)2 = xTx. This connection we want to preserve. The inner product must be modified to match the new definition of length, and we conjugate the first vector in the inner product. Replacing x by x, the inner product becomes Inner product xTy=x y +···+x y . (5) 1 1 n n"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.5 ComplexMatrices 315 If we take the inner product of x=(1+3i,3i) with itself, we are back to (cid:107)x(cid:107)2: Length squared xTx=(1+i)(1+i)+(3i)(3i)=2+9 and (cid:107)x(cid:107)2 =11. Note that yTx is different from xTy; we have to watch the order of the vectors. This leaves only one more change in notation, condensing two symbols into one. Instead of a bar for the conjugate and a T for the transpose, those are combined into the conjugatetranspose. Forvectorsandmatrices,asuperscriptH(orastar)combinesboth operations. This matrix AT =AH =A∗ is called “A Hermitian”: “A Hermitian” AH =AT has entries (AH) =A . (6) ij ji You have to listen closely to distinguish that name from the phrase “A is Hermitian,” which means that A equals AH. If A is an m by n matrix, then AH is n by m:   H (cid:34) (cid:35) 2+i 3i Conjugate   2−i 4+i 0 4−i 5 = . transpose −3i 5 0 0 0 This symbol AH gives official recognition to the fact that, with complex entries, it is very seldom that we want only the transpose of A. It is the conjugate transpose AH that becomes appropriate, and xH is the row vector [x ··· x ]. 1 n 5N"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. The inner product of x and y is xHy. Orthogonal vectors have xHy=0."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. The squared length of x is (cid:107)x(cid:107)2 =xHx=|x |2+···+|x |2. 1 n"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. Conjugating (AB)T =BTAT produces (AB)H =BHAH. Hermitian Matrices We spoke in earlier chapters about symmetric matrices: A =AT. With complex entries, thisideaofsymmetryhastobeextended. Therightgeneralizationisnottomatricesthat equal their transpose, but to matrices that equal their conjugate transpose. These are the Hermitian matrices, and a typical example is A: (cid:34) (cid:35) 2 3−3i Hermitian matrix A= =AH. (7) 3+3i 5 Thediagonalentriesmustbereal;theyareunchangedbyconjugation. Eachoff-diagonal entry is matched with its mirror image across the main diagonal, and 3−3i is the conju- gate of 3+3i. In every case, a =a . ij ji Our main goal is to establish three basic properties of Hermitian matrices. These properties apply equally well to symmetric matrices. A real symmetric matrix is cer- tainly Hermitian. (For real matrices there is no difference between AT and AH.) The eigenvalues of A are real—as we now prove. Property1 IfA=AH,thenforallcomplexvectorsx,thenumberxHAxisreal. Every entry of A contributes to xHAx. Try the 2 by 2 case with x=(u,v): (cid:34) (cid:35)(cid:34) (cid:35) (cid:104) (cid:105) 2 3−3i u xHAx= u v 3+3i 5 v =2uu+5vv+(3−3i)uv+(3+3i)uv =real+real+(sum of complex conjugates). For a proof in general. (xHAx)H is the conjugate of the 1 by 1 matrix xHAx, but we actually get the same number back again: (xHAx)H =xHAHxHH =xHAx. So that number must be real. Property 2 If A=AH, every eigenvalue is real. Proof. Suppose Ax =λx. The trick is to multiply by xH: xHAx =λxHx. The left-hand sideisrealbyProperty1,andtheright-handsidexHx=(cid:107)x(cid:107)2 isrealandpositive,because x(cid:54)=0. Thereforeλ=xHAx/xHx must be real. Our example hasλ=8 andλ=−1: (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)2−λ 3−3i(cid:175) |A−λI|=(cid:175) (cid:175)=λ2−7λ+10−|3−3i|2 (cid:175)3+3i 5−λ(cid:175) (8) =λ2−7λ−8=(λ−8)(λ+1). Note. This proof of real eigenvalues looks correct for any real matrix: xTAx False proof Ax=λx gives xTAx=λxTx, so λ= is real. xTx There must be a catch: The eigenvector x might be complex. It is when A = AT that we can be sure λ and x stay real. More than that, the eigenvectors are perpendicular: xTy=0 in the real symmetric case and xHy=0 in the complex Hermitian case. Property 3 Two eigenvectors of a real symmetric matrix or a Hermitian ma- trix, if they come from different eigenvalues, are orthogonal to one another. The proof starts with Ax=λ x, Ay=λ y, and A=AH: 1 1 (λ x)Hy=(Ax)Hy=xHAy=xH(λ y). (9) 1 2 Theoutsidenumbersareλ xHy=λ xHy,sincetheλ’sarereal. Nowwcusetheassump- 1 2 tionλ (cid:54)=λ , which forces the conclusion that xHy=0. In our example, 1 2 (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) −6 3−i x 0 1 1 (A−8I)x= = , x= 3+3i −3 x 0 1+i 2 (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 3 3−3i y 0 1−i 1 (A+I)y= = , y= . 3+3i 6 y 0 −1 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.5 ComplexMatrices 317 These two eigenvectors are orthogonal: (cid:34) (cid:35) (cid:104) (cid:105) 1−i xHy= 1 1−i =0. −1 Of course any multiples x/α and y/β are equally good as eigenvectors. MATLAB picks α = (cid:107)x(cid:107) and β = (cid:107)y(cid:107), so that x/α and y/β are unit vectors; the eigenvectors are normalized to have length 1. They are now orthonormal. If these eigenvectors are chosen to be the columns of S, then we have S−1AS = Λ as always. The diagonalizing matrix can be chosen with orthonormal columns when A=AH. In case A is real and symmetric, its eigenvalues are real by Property 2. Its unit eigenvectors are orthogonal by Property 3. Those eigenvectors are also real; they solve (A−λI)x = 0. These orthonormal eigenvectors go into an orthogonal matrix Q, with QTQ = I and QT = Q−1. Then S−1AS = Λ becomes special—it is Q−1AQ = Λ or A=QΛQ−1 =QΛQT. We can state one of the great theorems of linear algebra: 5O ArealsymmetricmatrixcanbefactoredintoA=QΛQT. Itsorthonormal eigenvectors are in the orthogonal matrix Q and its eigenvalues are in Λ. In geometry or mechanics, this is the principal axis theorem. It gives the right choice of axes for an ellipse. Those axes are perpendicular, and they point along the eigen- vectors of the corresponding matrix. (Section 6.2 connects symmetric matrices to n- dimensional ellipses.) In mechanics the eigenvectors give the principal directions, along which there is pure compression or pure tension—with no shear. In mathematics the formula A = QΛQT is known as the spectral theorem. If we multiply columns by rows, the matrix A becomes a combination of one-dimensional projections—which are the special matrices xxT of rank 1, multiplied byλ:     | | λ — xT — 1 1 A=QΛQT = x ··· x    ...    . . .   1 n (10) | | λ — xT — n n =λ x xT+λ x xT+···+λ x xT. 1 1 1 2 2 2 n n n Our 2 by 2 example has eigenvalues 3 and 1: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 2 −1 1 −1 1 1 Example3. A= =3 2 2 + 2 2 =combination of two projections. −1 2 −1 1 1 1 2 2 2 2 The eigenvectors, with length scaled to 1, are (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 1 x = √ and x = √ . 1 2 2 −1 2 1 Then the matrices on the right-hand side are x xT and x xT—columns times rows—and 1 1 2 2 they are projections onto the line through x and the line through x . 1 2 Allsymmetricmatricesarecombinationsofone-dimensionalprojections—whichare symmetric matrices of rank 1. Remark. If A is real and its eigenvalues happen to be real, then its eigenvectors are also real. They solve (A−λI)x = 0 and can be computed by elimination. But they will not be orthogonal unless A is symmetric: A=QΛQT leads to AT =A. If A is real, all complex eigenvalues come in conjugate pairs: Ax =λx and Ax =λx. If a+ib is an eigenvalue of a real matrix, so is a−ib. (If A=AT then b=0.) Strictly speaking, the spectral theorem A = QΛQT has been proved only when the eigenvalues of A are distinct. Then there are certainly n independent eigenvectors, and A can be safely diagonalized. Nevertheless it is true (see Section 5.6) that even with repeated eigenvalues, a symmetric matrix still has a complete set of orthonormal eigen- vectors. Theextremecaseistheidentitymatrix,whichhasλ=1repeatedntimes—and no shortage of eigenvectors. Tofinishthecomplexcaseweneedtheanalogueofarealorthogonalmatrix—andyou can guess what happens to the requirement QTQ=I. The transpose will be replaced by the conjugate transpose. The condition will becomeUHU =I. The new letterU reflects thenewname: Acomplexmatrixwithorthonormalcolumnsiscalledaunitarymatrix. Unitary Matrices May we propose two analogies? A Hermitian (or symmetric) matrix can be compared to a real number. A unitary (or orthogonal) matrix can be compared to a number on the unit circle—a complex number of absolute value 1. Theλ’s are real if AH =A, and they are on the unit circle ifUHU =I. The eigenvectors can be scaled to unit length and made orthonormal.6 Those statements are not yet proved for unitary (including orthogonal) matrices. Therefore we go directly to the three properties ofU that correspond to the earlier Prop- erties 1–3 of A. Remember thatU has orthonormal columns: Unitary matrix UHU =I, UUH =I, and UH =U−1. This leads directly to Property 1(cid:48), that multiplication by U has no effect on inner prod- ucts, angles, or lengths. The proof is on one line, just as it was for Q: Property 1(cid:48) (Ux)H(Uy)=xHUHUy=xHy and lengths are preserved byU: Length unchanged (cid:107)Ux(cid:107)2 =xHUHUx=(cid:107)x(cid:107)2. (11) Property 2(cid:48) Every eigenvalue ofU has absolute value |λ|=1. This follows directly from Ux = λx, by comparing the lengths of the two sides: (cid:107)Ux(cid:107)=(cid:107)x(cid:107) by Property 1(cid:48), and always (cid:107)λx(cid:107)=|λ|(cid:107)x(cid:107). Therefore |λ|=1. 6Later we compare “skew-Hermitian” matrices with pure imaginary numbers, and “normal” matrices with all complexnumbersa+ib. Anonnormalmatrixwithoutorthogonaleigenvectorsbelongstononeoftheseclasses, andisoutsidethewholeanalogy."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.5 ComplexMatrices 319 Property3(cid:48) Eigenvectorscorrespondingtodifferenteigenvaluesareorthonor- mal. Start withUx=λ x andUy=λ y, and take inner products by Property 1(cid:48): 1 2 xHy=(Ux)H(Uy)=(λ x)H(λ y)=λ λ xHy. 1 2 1 2 Comparingthelefttotheright,λ λ =1orxHy=0. ButProperty2(cid:48) isλ λ =1, sowe 1 2 1 1 cannot also haveλ λ =1. Thus xHy=0 and the eigenvectors are orthogonal. 1 2 (cid:34) (cid:35) cost −sint Example 4. U = has eigenvalues eit and e−it. sint cost Theorthogonaleigenvectorsarex=(1,−i)andy=(1,i). (Remembertotakeconjugates √ in xHy=1+i2 =0.) After division by 2 they are orthonormal. Here is the most important unitary matrix by far.   1 1 · 1   1 1 w · wn−1  Fourier matrix Example 5. U = √  = √ . n· · · ·  n 1 wn−1 · w(n−1)2 The complex number w is on the unit circle at the angle θ= 2π/n. It equals e2πi/n. Its powers are spaced evenly around the circle. That spacing assures that the sum of all n powers of w—all the nth roots of 1—is zero. Algebraically, the sum 1+w+···+wn−1 is (wn−1)/(w−1). And wn−1 is zero! 1 wn−1 row 1 ofUH times column 2 ofU is (1+w+w2+···+wn−1)= =0. n w−1 1 Wn−1 row i ofUH times column j ofU is (1+W +W2+···+Wn−1)= =0. n W −1 In the second case, W = wj−i. Every entry of the original F has absolute value 1. The √ factor n shrinks the columns ofU into unit vectors. The fundamental identity of the finite Fourier transform isUHU =I. Thus U is a unitary matrix. Its inverse looks the same except that w is replaced by w−1 =e−iθ =w. SinceU is unitary, its inverse is found by transposing (which changes nothing) and conjugating (which changes w to w). The inverse of thisU isU. Ux can be computed quickly by the Fast Fourier Transform as found in Section 3.5. By Property 1(cid:48) of unitary matrices, the length of a vector x is the same as the length of Ux. The energy in state space equals the energy in transform space. The energy is the sum of |x |2, and it is also the sum of the energies in the separate frequencies. The j vector x = (1,0,...,0) contains equal amounts of every frequency component, and its √ Discrete Fourier TransformUx=(1,1,...,1)/ n also has length 1. Example 6.   0 1 0 0   0 0 1 0 P= . 0 0 0 1 1 0 0 0 This is an orthogonal matrix, so by Property 3(cid:48) it must have orthogonal eigenvectors. They are the columns of the Fourier matrix! Its eigenvalues must have absolute value 1. They are the numbers 1,w,...,wn−1 (or 1,i,i2,i3 in this 4 by 4 ease). It is a real matrix, but its eigenvalues and eigenvectors are complex. One final note, Skew-Hermitian matrices satisfy KH = −K, just as skew-symmetric matrices satisfy KT =−K. Their properties follow immediately from their close link to Hermitian matrices: If A is Hermitian then K =iA is skew-Hermitian. The eigenvalues of K are purely imaginary instead of purely real; we multiply i. The eigenvectors are not changed. The Hermitian example on the previous pages would lead to (cid:34) (cid:35) 2i 3+3i K =iA= =−KH. −3+3i 5i The diagonal entries are multiples of i (allowing zero). The eigenvalues are 8i and −i. The eigenvectors are still orthogonal, and we still have K =UΛUH—with a unitary U instead of a real orthogonal Q, and with 8i and −i on the diagonal of Λ. This section is summarized by a table of parallels between real and complex. Real versus Complex Rn (n real components) ↔ Cn (n complex components) length: (cid:107)x(cid:107)2 =x2+···+x2 ↔ length: (cid:107)x(cid:107)2 =|x |2+···+|x |2 1 n 1 n transpose: AT =A ↔ Hermitian transpose: AH =A ij ji ij ji (AB)T =BTAT ↔ (AB)H =BHAH inner product: xTy=x y +···+x y ↔ inner product: xHy=x y +···+x y 1 1 n n 1 1 n n (Ax)Ty=xT(ATy) ↔ (Ax)Hy=xH(AHy) orthogonality: xTy=0 ↔ orthogonality: xHy=0 symmetric matrices: AT =A ↔ Hermitian matrices: AH =A A=QΛQ−1 =QΛQT (real Λ) ↔ A=UΛU−1 =UΛUH (real Λ) skew-symmetric KT =−K ↔ skew-Hermitian KH =−K orthogonal QTQ=I or QT =Q−1 ↔ unitaryUHU =I orUH =U−1 (Qx)T(Qy)=xTy and (cid:107)Qx(cid:107)=(cid:107)x(cid:107) ↔ (Ux)H(Uy)=xHy and (cid:107)Ux(cid:107)=(cid:107)x(cid:107) The columns, rows, and eigenvectors of Q andU are orthonormal, and every |λ|=1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. For the complex numbers 3+4i and 1−i, (a) find their positions in the complex plane. (b) find their sum and product. (c) find their conjugates and their absolute values. Do the original numbers lie inside or outside the unit circle?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. What can you say about (a) the sum of a complex number and its conjugate? (b) the conjugate of a number on the unit circle? (c) the product of two numbers on the unit circle? (d) the sum of two numbers on the unit circle?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. Ifx=2+iandy=1+3i, findx, xx, 1/x, andx/y. Checkthattheabsolutevalue|xy| equals |x| times |y|, and the absolute value |1/x| equals 1 divided by |x|."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "4. Find a and b for the complex numbers a+ib at the angles θ = 30°,60°,90° on the unit circle. Verify by direct multiplication that the square of the first is the second, and the cube of the first is the third."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5. (a) If x = reiθ what are x2, x−1, and x in polar coordinates? Where are the complex numbers that have x−1 =x? (b) Att =0,thecomplexnumbere(−1+i)t equalsone. Sketchitspathinthecomplex plane ast increases from 0 to 2π."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "6. Find the lengths and the inner product of (cid:34) (cid:35) (cid:34) (cid:35) 2−4i 2+4i x= and y= . 4i 4i"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "7. Write out the matrix AH and computeC =AHA if (cid:34) (cid:35) 1 i 0 A= . i 0 1 WhatistherelationbetweenC andCH? DoesitholdwheneverC isconstructedfrom some AHA?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "8. (a) With the preceding A, use elimination to solve Ax=0. (b) Show that the nullspace you just computed is orthogonal to C(AH) and not to the usual row space C(AT). The four fundamental spaces in the complex case are N(A) and C(A) as before, and then N(AH) and C(AH)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "9. (a) How is the determinant of AH related to the determinant of A? (b) Prove that the determinant of any Hermitian matrix is real."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "10. (a) How many degrees of freedom are there in a real symmetric matrix, a real diag- onal matrix, and a real orthogonal matrix? (The first answer is the sum of the other two, because A=QΛQT.) (b) Show that 3 by 3 Hermitian matrices A and also unitaryU have 9 real degrees of freedom (columns ofU can be multiplied by any eiθ)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "11. Write P, Q and R in the formλ x xH+λ x xH of the spectral theorem: 1 1 1 2 2 2 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 1 0 1 3 4 P= 2 2 , Q= , R= . 1 1 1 0 4 −3 2 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "12. Give a reason if true or a counterexample if false: (a) If A is Hermitian, then A+iI is invertible. (b) If Q is orthogonal. then Q+1I is invertible. 2 (c) If A is real, then A+iI is invertible."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "13. Suppose A is a symmetric 3 by 3 matrix with eigenvalues 0, 1, 2. (a) What properties can be guaranteed for the corresponding unit eigenvectors u, v, w? (b) In terms of u, v, w, describe the nullspace, left nullspace, row space and column space of A. (c) Find a vector x that satisfies Ax=v+w. Is x unique? (d) Under what conditions on b does Ax=b have a solution? (e) If u, v, w are the columns of S, what are S−1 and S−1AS?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "14. In the list below, which classes of matrices contain A and which contain B?     0 1 0 0 1 1 1 1     0 0 1 0 11 1 1 1 A=  and B=  . 0 0 0 1 41 1 1 1 1 0 0 0 1 1 1 1 Orthogonal, invertible, projection, permutation, Hermitian, rank-1, diagonalizable, Markov. Find the eigenvalues of A and B."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "15. What is the dimension of the space S of all n by n real symmetric matrices? The spectral theorem says that every symmetric matrix is a combination of n projection matrices. Since the dimension exceeds n, how is this difference explained?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "16. Write one significant fact about the eigenvalues of each of the following."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.5 ComplexMatrices 323 (a) A real symmetric matrix. (b) A stable matrix: all solutions to du/dt =Au approach zero. (c) An orthogonal matrix. (d) A Markov matrix. (e) A defective matrix (nondiagonalizable). (f) A singular matrix."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "17. Show that ifU andV are unitary, so isUV. Use the criterionUHU =I."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "18. Show that a unitary matrix has |detU| = 1, but possibly detU is different from detUH. Describe all 2 by 2 matrices that are unitary."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "19. Find a third column so thatU is unitary. How much freedom in column 3?  √ √  1/ 3 i/ 2 √   U =1/ 3 0 . √ √ i/ 3 1/ 2 (cid:163) (cid:164) √"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "20. Diagonalize the 2 by 2 skew-Hermitian matrix K = i i , whose entries are all −1. i i Compute eKt = SeΛtS−1, and verify that eKt is unitary. What is the derivative of eKt att =0?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "21. Describeall3by3matricesthataresimultaneouslyHermitian,unitary,anddiagonal. How many are there?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "22. Every matrix Z can be split into a Hermitian and a skew-Hermitian part, Z =A+K, just as a complex number z is split into a+ib, The real part of z is half of z+z, and the“realpart”ofZ ishalfofZ+ZH. Findasimilarformulaforthe“imaginarypart” K, and split these matrices into A+K: (cid:34) (cid:35) (cid:34) (cid:35) 3+i 4+2i i i Z = and Z = . 0 5 −i i"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "23. Show that the columns of the 4 by 4 Fourier matrix F in Example 5 are eigenvectors of the permutation matrix P in Example 6."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "24. For the permutation of Example 6, write out the circulant matrix C = c I+c P+ 0 1 c P2+c P3. (Its eigenvector matrix is again the Fourier matrix.) Write out also 2 3 the four components of the matrix-vector product Cx, which is the convolution of c=(c ,c ,c ,c ) and x=(x ,x ,x ,x ). 0 1 2 3 0 1 2 3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "25. For a circulantC =FΛF−1, why is it faster to multiply by F−1, then Λ, then F (the convolution rule), than to multiply directly byC?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "26. Find the lengths of u=(1+i,1−i,1+2i) and v=(i,i,i). Also find uHv and vHu."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "27. Prove that AHA is always a Hermitian matrix, Compute AHA and AAH: (cid:34) (cid:35) i 1 i A= . 1 i i"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "28. If Az = 0, then AHAz = 0. If AHAz = 0, multiply by zH to prove that Az = 0. The nullspaces of A and AHA are . AHA is an invertible Hermitian matrix when the nullspace of A contains only z= ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "29. When you multiply a Hermitian matrix by a real number c, is cA still Hermitian? If c=i,showthatiAisskew-Hermitian. The3by3Hermitianmatricesareasubspace, provided that the “scalars” are real numbers."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "30. Which classes of matrices does P belong to: orthogonal, invertible, Hermitian, uni- tary, factorizable into LU, factorizable into QR?   0 1 0   P=0 0 1. 1 0 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "31. Compute P2, P3, and P100 in Problem 30. What are the eigenvalues of P?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "32. Find the unit eigenvectors of P in Problem 30, and put them into the columns of a unitary matrixU. What property of P makes these eigenvectors orthogonal?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "33. Write down the 3 by 3 circulant matrix C = 2I+5P+4P2. It has the same eigen- vectors as P in Problem 30. Find its eigenvalues."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "34. IfU is unitary and Q is a real orthogonal matrix, show that U−1 is unitary and also UQ is unitary. Start fromUHU =I and QTQ=I."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "35. Diagonalize A (realλ’s) and K (imaginaryλ’s) to reachUΛUH: (cid:34) (cid:35) (cid:34) (cid:35) 0 1−i 0 −1+i A= K = i+1 1 1+i i"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "36. Diagonalize this orthogonal matrix to reach Q=UΛUH. Now allλ’s are : (cid:34) (cid:35) cosθ −sinθ Q= . sinθ cosθ"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "37. Diagonalize this unitary matrixV to reachV =UΛUH. Again all |λ|=1: (cid:34) (cid:35) 1 1 1−i V = √ . 3 1+i −1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 325"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "38. If v ,...,v is an orthonormal basis for Cn, the matrix with those columns is a 1 n matrix. Show that any vector z equals (vHz)v +···+(vHz)v . 1 1 n n"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "39. The functions e−ix and e−ix are orthogonal on the interval 0 ≤ x ≤ 2πbecause their (cid:82) 2π complex inner product is =0. 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "40. The vectors v=(1,i,1), w=(i,1,0) and z= are an orthogonal basis for ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "41. If A=R+iS is a Hermitian matrix, are the real matrices R and S symmetric?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "42. The (complex) dimension of Cn is . Find a nonreal basis for Cn."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "43. Describe all 1 by 1 matrices that are Hermitian and also unitary. Do the same for 2 by 2 matrices."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "44. How are the eigenvalues of AH (square matrix) related to the eigenvalues of A?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "45. If uHu=1, show that I−2uuH is Hermitian and also unitary. The rank-1 matrix uuH is the projection onto what line in Cn? (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "46. IfA+iBisaunitarymatrix(AandBarereal),showthatQ= A −B isanorthogonal B A matrix. (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "47. If A+iB is a Hermitian matrix (A and B are real), show that A −B is symmetric. B A"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "48. Prove that the inverse of a Hermitian matrix is again a Hermitian matrix."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "49. Diagonalize this matrix by constructing its eigenvalue matrix Λ and its eigenvector matrix S: (cid:34) (cid:35) 2 1−i A= =AH. 1+i 3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "50. A matrix with orthonormal eigenvectors has the form A=UΛU−1 =UΛUH. Prove that AAH =AHA. These are exactly the normal matrices."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 Similarity Transformations Virtually every step in this chapter has involved the combination S−1AS. The eigenvec- tors of A went into the columns of S, and that made S−1AS a diagonal matrix (called Λ). When A was symmetric, we wrote Q instead of S, choosing the eigenvectors to be orthonormal. Inthecomplexcase,whenAisHermitianwewriteU—itisstillthematrix of eigenvectors. Now we look at all combinations M−1AM—formed with any invertible M on the right and its inverse on the left. The invertible eigenvector matrix S may fail to exist (the defective case), or we may not know it, or we may not want to use it. First a new word: The matrices A and M−1AM are “similar”. Going from one to the other is a similarity transformation. It is the natural step for differential equations or matrix powers or eigenvalues—just as elimination steps were natural for Ax = b. Elimination multiplied A on the left by L−1, but not on the right by L. So U is not similar to A, and the pivots are not the eigenvalues. A whole family of matrices M−1AM is similar to A, and there are two questions:"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. What do these similar matrices M−1AM have in common?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. With a special choice of M, what special form can be achieved by M−1AM? The final answer is given by the Jordan form, with which the chapter ends. These combinations M−1AM arise in a differential or difference equation, when a “change of variables” u=Mv introduces the new unknown v: du dv dv =Au becomes M =AMv, or =M−1AMv dt dt dt u =Au becomes Mv =AMv , or v =M−1AMv . n+1 n n+1 n n+1 n The new matrix in the equation is M−1AM. In the special case M = S, the system is uncoupledbecauseΛ=S−1ASisdiagonal. Theeigenvectorsevolveindependently. This is the maximum simplification, but other M’s are also useful. We try to make M−1AM easier to work with than A. The family of matrices M−1AM includes A itself, by choosing M = I. Any of these similar matrices can appear in the differential and difference equations, by the change u = Mv, so they ought to have something in common, and they do: Similar matrices share the same eigenvalues. 5P Suppose that B = M−1AM. Then A and B have the same eigenvalues. Every eigenvector x of A corresponds to an eigenvector M−1x of B. Start from Ax=λx and substitute A=MBM−1: Same eigenvaluc MBM−1x=λx which is B(M−1x)=λ(M−1x). (1) The eigenvalue of B is stillλ. The eigenvector has changed from x to M−1x. We can also check that A−λI and B−λI have the same determinant: Product of matrices B−λI =M−1AM−λI =M−1(A−λI)M Product rule det(B−λI)=detM−1det(A−λI)detM =det(A−λI). The polynomials det(A−λI) and det(B−λI) are equal. Their roots—the eigenvalues of A and B—are the same. Here are matrices B similar to A."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 327 (cid:163) (cid:164) Example 1. A= 1 0 has eigenvalues 1 and 0. Each B is M−1AM: 0 0 (cid:34) (cid:35) (cid:34) (cid:35) 1 b 1 b If M = , then B= : triangular withλ=0 and 0. 0 1 0 0 (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 1 If M = , then B= 2 2 : projection withλ=0 and 0. −1 1 1 1 2 2 (cid:34) (cid:35) a b If M = , then B=an arbitrary matrix withλ=0 and 0. c d In this case we can produce any B that has the correct eigenvalues. It is an easy case, because the eigenvalues 1 and 0 are distinct. The diagonal A was actually Λ, the out- standing member of this family of similar matrices (the capo). The Jordan form will worry about repeated eigenvalues and a possible shortage of eigenvectors. All we say no is that every M−1AM has the same number of independent eigenvectors as A (each eigenvector is multiplied by M−1). The first step is to look at the linear transformations that lie behind the matrices. Rotations, reflections, and projections act on n-dimensional space. The transformation can happen without linear algebra, but linear algebra turns it into matrix multiplication. Change of Basis = Similarity Transformation The similar matrix B=M−1AM is closely connected to A, if we go back to linear trans- formations. Remember the key idea: Every linear transformation is represented by a matrix. The matrix depends on the choice of basis! If we change the basis by M we change the matrix A to a similar matrix B. Similar matrices represent the same transformation T with respect so different bases. The algebra is almost straightforward. Suppose we have a basis v ,...,v . The 1 n jth column of A comes from applying T to v : j Tv =combination of the basis vectors=a v +···+a v . (2) j 1j 1 nj n For a new basis V ,...,V , the new matrix B is constructed in the same way: TV = 1 n j combination of the V’s = b V +···+b V . But also each V must be a combination 1j 1 nj n of the old basis vectors: V = ∑m v . That matrix M is really representing the identity j ij i transformation(!) whentheonlythinghappeningisthechangeofbasis(T isI). Thein- verse matrix M−1 also represents the identity transformation. when the basis is changed from the v’s back to theV’s. Now the product rule gives the result we want: 5Q The matrices A and B that represent the same linear transformation T with respect to two different bases (the v’s and theV’s) are similar: [T] = [I] [T] [I] V toV vtoV vtov V tov (3) B = M−1 A M. I think an example is the best way to explain B = M−1AM. Suppose T is projection ontothelineLatangleθ. Thislineartransformationiscompletelydescribedwithoutthe helpofabasis. ButtorepresentT byamatrix, wedoneedabasis. Figure5.5offerstwo choices, the standard basis v = (1,0), v = (0,1) and a basis V , V chosen especially 1 2 1 2 for T. 135° y=−x 0 \" # 135° y=−x 1 V2=\"1 # 1 −.5 projection \" # projects tozero .5 1 \" # 0 1 A=\" .5 −.5 # projection \" .5 # Λ=\"1 0 # V1=\" −1# −.5 .5 −.5 0 0 projects toV1 Figure5.5: Changeofbasistomaketheprojectionmatrixdiagonal. In fact TV =V (sinceV is already on the line L) and TV = 0 (sinceV is perpen- 1 1 1 2 2 dicular to the line). In that eigenvector basis, the matrix is diagonal: (cid:34) (cid:35) 1 0 Elgenvector basis B=[T] = . V toV 0 0 TheotherthingisthechangeofbasismatrixM. ForthatweexpressV asacombination 1 v cosθ⊥v sinθ and put those coefficients into column 1. Similarly V (or IV , the 1 2 2 2 transformation is the identity) is −v sinθ+v cosθ, producing column 2: 1 2 (cid:34) (cid:35) c −s Change of basis M =[I] = . V tov s c The inverse matrix M−1 (which is here the transpose) goes from v toV. Combined with B and M, it gives the projection matrix in the standard basis of v’s: (cid:34) (cid:35) c2 cs Standard basis A=MBM−1 = . cs s2 Wecansummarizethemainpoint. ThewaytosimplifythatmatrixA—infacttodiag- onalizeit—istofinditseigenvectors. TheygointothecolumnsofM (orS)andM−1AM isdiagonal. Thealgebraistsaysthesamethinginthelanguageoflineartransformations: Choose a basis consisting of eigenvectors. The standard basis led to A, which was not simple. The right basis led to B, which was diagonal. We emphasize again that M−1AM does not arise in solving Ax = b. There the basic operation was to multiply A (on the left side only!) by a matrix that subtracts a multiple"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 329 of one row from another. Such a transformation preserved the nullspace and row space of A; it normally changes the eigenvalues. Eigenvaluesareactuallycalculatedbyasequenceofsimplesimilarities. Thematrix goes gradually toward a triangular form, and the eigenvalues gradually appear on the main diagonal. (Such a sequence is described in Chapter 7.) This is much better than tryingtocomputedet(A−λI),whoserootsshouldbetheeigenvalues. Foralargematrix, it is numerically impossible to concentrate all that information into the polynomial and then get it out again. Triangular Forms with a Unitary M Our first move beyond the eigenvector matrix M = S is a little bit crazy: Instead of a more general M, we go the other way and restrict M to be unitary. M−1AM can achieve a triangular form T under this restriction. The columns of M =U are orthonormal (in the real case, we would write M = Q). Unless the eigenvectors of Λ are orthogonal, a diagonal U−1AU is impossible. But “Schur’s lemma” in 5R is very useful—at least to the theory. (The rest of this chapter is devoted more to theory than to applications. The Jordan form is independent of this triangular form.) 5R There is a unitary matrix M =U such thatU−1AU =T is triangular. The eigenvalues of A appear along the diagonal of this similar matrix T. Proof. Every matrix, say 4 by 4, has at least one eigenvalue λ . In the worst case, it 1 could be repeated four times. Therefore A has at least one unit eigenvector x , which we 1 place in the first column of U. At this stage the other three columns are impossible to determine, so we complete the matrix in any way that leaves it unitary, and call it U . 1 (The Gram-Schmidt process guarantees that this can be done.) Ax = λ x column 1 1 1 1 means that the productU−1AU starts in the right form: 1 1     λ ∗ ∗ ∗ λ ∗ ∗ ∗ 1 1     0 ∗ ∗ ∗ 0 ∗ ∗ ∗ AU =U   leads to U−1AU = . 1 1 0 ∗ ∗ ∗ 1 1 0 ∗ ∗ ∗ 0 ∗ ∗ ∗ 0 ∗ ∗ ∗ Now work with the 3 by 3 submatrix in the lower right-hand corner. It has a unit eigenvector x , which becomes the first column of a unitary matrix M : 2 2     1 0 0 0 λ ∗ ∗ ∗ 1     0  0 λ ∗ ∗ If U =  then U−1(U−1AU )U = 2 . 2 0 M  2 1 1 2 0 0 ∗ ∗ 2 0 0 0 ∗ ∗ At the last step, an eigenvector of the 2 by 2 matrix in the lower right-hand corner goes into a unitary M , which is put into the corner ofU : 3 3   λ ∗ ∗ ∗ 1   (cid:161) (cid:162) 0 λ ∗ ∗ Triangular U−1 U−1U−1AU U U = 2 =T. 3 2 1 1 2 3 0 0 λ ∗ 3 0 0 0 ∗ The productU =U U U is still a unitary matrix, andU−1AU =T. 1 2 3 This lemma applies to all matrices, with no assumption that A is diagoalizable. We could use it to prove that the powers Ak approach zero when all |λ|<1, and the expo- i nentialseAt approachzerowhenallReλ <0—evenwithoutthefullsetofeigenvectors i which was assumed in Sections 5.3 and 5.4. (cid:34) (cid:35) 2 −1 Example 2. A= has the eigenvalueλ=1 (twice). 1 0 √ The only line of eigenvectors goes through (1,1). After dividing by 2, this is the first column ofU, and the triangularU−1AU =T has the eigenvalues on its diagonal: (cid:34) √ √ (cid:35)(cid:34) (cid:35)(cid:34) √ √ (cid:35) (cid:34) (cid:35) 1/ 2 1/ 2 2 −1 1/ 2 1/ 2 1 2 U−1AU = √ √ √ √ = =T. (4) 1/ 2 −1/ 2 1 0 1/ 2 −1/ 2 0 1 Diagonalizing Symmetric and Hermitian Matrices This triangular form will show that any symmetric or Hermitian matrix—whether its eigenvalues are distinct or not—has a complete set of orthonormal eigenvectors. We need a unitary matrix such that U−1AU is diagonal. Schur’s lemma has just found it. This triangular T must be diagonal, because it is also Hermitian when A=AH: T =TH (U−1AU)H =UHAH(U−1)H =U−1AU. The diagonal matrixU−1AU represents a key theorem in linear algebra. 5S (Spectral Theorem) Every real symmetric A can be diagonalized by an orthogonalmatrixQ. EveryHermitianmatrixcanbediagonalizedbyaunitary U: (real) Q−1AQ=Λ or A=QΛQT (complex) U−1AU =Λ or A=UΛUH The columns of Q (orU) contain orthonormal eigenvectors of A. Remark1. Intherealsymmetriccase,theeigenvaluesandeigenvectorsarerealatevery step. That produces a real unitaryU—an orthogonal matrix."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 331 Remark 2. A is the limit of symmetric matrices with distinct eigenvalues. As the limit approaches, the eigenvectors stay perpendicular. This can fail if A(cid:54)=AT: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 0 cosθ 1 cosθ A(θ)= has eigenvectors and . 0 sinθ 0 sinθ (cid:163) (cid:164) (cid:163) (cid:164) Asθ→0, the only eigenvector of the nondiagonalizable matrix 0 1 is 1 . 0 0 0 Example 3. The spectral theorem says that this A=AT can be diagonalized:   0 1 0   A=1 0 0 with repeated eigenvalues λ =λ =1 andλ =−1. 1 2 3 0 0 1 λ=1 has a plane of eigenvectors, and we pick an orthonormal pair x and x : 1 2       1 0 1 1     1   x = √ 1 and x =0 and x = √ −1 forλ =−1. 1 2 3 3 2 2 0 1 0 These are the columns of Q. Splitting A=QΛQT into 3 columns times 3 rows gives         0 1 0 1 1 0 0 0 0 1 −1 0 2 2 2 2         A=1 0 0=λ 1 1 0+λ 0 0 0+λ −1 1 0. 1 2 2 2 3 2 2 0 0 1 0 0 0 0 0 1 0 0 0 Sinceλ =λ ,thosefirsttwoprojectionsx xT andx xT (eachofrank1)combinetogive 1 2 1 1 2 2 a projection P of rank 2 (onto the plane of eigenvectors). Then A is 1       0 1 0 1 1 0 1 −1 0 2 2 2 2       1 0 0=λ P +λ P =(+1)1 1 0+(−1)−1 1 0. (5) 1 1 3 3 2 2 2 2 0 0 1 0 0 1 0 0 0 Every Hermitian matrix with k different eigenvalues has a spectral decomposition into A=λ P +···+λ P ,whereP istheprojectionontotheeigenspaceforλ. Sincethereis 1 1 k k i i afullsetofeigenvectors,theprojectionsadduptotheidentity. Andsincetheeigenspace are orthogonal, two projections produce zero: P P =0. j i We are very close to answering an important question, so we keep going: For which matrices is T = Λ? Symmetric, skew-symmetric, and orthogonal T’s are all diagonal! Hermitian, skew-Hermitian, and unitary matrices are also in this class. They correspond to numbers on the real axis, the imaginary axis, and the unit circle. Now we want the whole class, corresponding to all complex numbers. The matrices are called “normal”. 5T The matrix N is normal if it commutes with NH: NNH = NHN. For such matrices, and no others, the triangular T =U−1NU is the diagonal Λ. Normal matrices are exactly those that have a complete set of orthonormal eigenvectors. Symmetric and Hermitian matrices are certainly normal: If A = AH, then AAH and AHA both equal A2. Orthogonal and unitary matrices are also normal: UUH and UHU both equal I. Two steps will work for any normal matrix:"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. If N is normal, then so is the triangular T =U−1NU: TTH =U−1NUUHNHU =U−1NNHU =U−1NHNU =UHNHUU−1NU =THT."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. A triangular T that is normal must be diagonal! (See Problems 19–20 at the end of this section.) Thus,ifN isnormal,thetriangularT =U−1NU mustbediagonal. SinceT hasthesame eigenvalues as N, it must be Λ. The eigenvectors of N are the columns of U, and they are orthonormal. That is the good case. We turn now from the best possible matrices (normal) to the worst possible (defective). (cid:34) (cid:35) (cid:34) (cid:35) 2 1 2 1 Normal N = Defective A= . −1 2 0 2 The Jordan Form This section has done its best while requiring M to be a unitary matrix U. We got M−1AM into a triangular form T. Now we lift this restriction on M. Any matrix is allowed, and the goal is to make M−1AM as nearly diagonal as possible. TheresultofthissupremeeffortatdiagonalizationistheJordanformJ. IfAhasafull set of eigenvectors, we take M =S and arrive at J =S−1AS =Λ. Then the Jordan form coincides with the diagonal Λ. This is impossible for a defective (nondiagonalizable) matrix. For every missing eigenvector, the Jordan form will have a 1 just above its main diagonal. The eigenvalues appear on the diagonal because J is triangular. And distinct eigenvalues can always be decoupled. It is only a repeatedλ that may (or may not!) require an off-diagonal 1 in J. 5U IfAhassindependenteigenvectors,itissimilartoamatrixwithsblocks:   J 1 Jordan form J =M−1AM =  ...  . (6) J s Each Jordan block J is a triangular matrix that has only a single eigenvalueλ i i and only one eigenvector:   λ 1 i    λ ·  i Jordan block J = . (7) i  · 1 λ i"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 333 The same λ will appear in several blocks, if it has several independent eigen- i vectors. Two matrices are similar if and only if they share the same Jordan form J. Many authors have made this theorem the climax of their linear algebra course. Frankly, I think that is a mistake. It is certainly true that not all matrices are diagonaliz- able, and the Jordan form is the most general case. For that very reason, its construction is both technical and extremely unstable. (A slight change in A can put back all the missing eigenvectors, and remove the off-diagonal is.) Therefore the right place for the details is in the appendix, and the best way to start on the Jordan form is to look at some specific and manageable examples. (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 2 2 −1 1 0 1 1 Example 4. T = and A= and B= all lead to J = . 0 1 1 0 1 1 0 1 These four matrices have eigenvalues 1 and 1 with only one eigenvector—so J con- sists of one block. We now check that. The determinants all equal 1. The traces (the sumsdownthemaindiagonal)are2. Theeigenvaluessatisfy1·1=1and1+1=2. For T, B, and J, which are triangular, the eigenvalues are on the diagonal. We want to show that these matrices are similar—they all belong to the same family. (T) From T to J, the job is to change 2 to 1. and a diagonal M will do it: (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 2 1 0 1 1 M−1TM = = =J. 0 2 0 1 0 1 0 1 2 (B) From B to J, the job is to transpose the matrix. A permutation does that: (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 0 1 1 0 0 1 1 1 P−1BP= = =J. 1 0 1 1 1 0 0 1 (A) From A to J, we go first to T as in equation (4). Then change 2 to 1: (cid:34) (cid:35) (cid:34) (cid:35) 1 2 1 1 U−1AU = =T and then M−1TM = =J. 0 1 0 1     0 1 2 0 0 1     Example 5. A=0 0 1 and B=0 0 0. 0 0 0 0 0 0 Zero is a triple eigenvalue for A and B, so it will appear in all their Jordan blocks. There can be a single 3 by 3 block, or a 2 by 2 and a 1 by I block, or three I by I blocks. Then A and B have three possible Jordan forms:       0 1 0 0 1 0 0 0 0       J =0 0 1, J =0 0 0, J =0 0 0. (8) 1 2 3 0 0 0 0 0 0 0 0 0 The only eigenvector of A is (1,0,0). Its Jordan form has only one block, and A must be similar to J . The matrix B has the additional eigenvector (0,1,0), and its Jordan 1 form is J with two blocks, As for J = zero matrix, it is in a family by itself; the only 2 3 matrix similar to J is M−10M = 0. A count of the eigenvectors will determine J when 3 there is nothing more complicated than a triple eigenvalue. Example 6. Application to difference and differential equations (powers and expo- nentials). If A can be diagonalized, the powers of A=SΛS−1 are easy: Ak =SΛkS−1. In every case we have Jordan’s similarity A=MJM−1, so now we need the powers of J: Ak =(MJM−1)(MJM−1)···(MJM−1)=MJkM−1. J is block-diagonal, and the powers of each block can be taken separately:     k λ 1 0 λk kλk−1 1k(k−1)λk−2 2     (J)k =0 λ 1 = 0 λk kλk−1 . (9) i 0 0 λ 0 0 λk This block J will enter when λ is a triple eigenvalue with a single eigenvector. Its i exponential is in the solution to the corresponding differential equation:   eλt teλt 1t2eλt 2   Exponential eJ it = 0 eλt teλt . (10) 0 0 eλt Here I+Jt+(Jt)2/2!+··· produces 1+λt+λ2t2/2!+···=eλt on the diagonal. i i The third column of this exponential comes directly from solving du/dt =Ju: i        u λ 1 0 u 0 1 1 d        u =0 λ 1u  starting from u =0. 2 2 0 dt u 0 0 λ u 1 3 3 Thiscanbesolvedbyback-substitution(sinceJ istriangular). Thelastequationdu /dt = i 3 λu yields u = eλt. The equation for u is du /dt =λu +u , and its solution is teλt. 3 3 2 2 2 3 The top equation is du /dt =λu +u , and its solution is 1t2eλt. When λ has multi- 1 1 2 2 plicity m with only one eigenvector, the extra factort appears m−1 times. These powers and exponentials of J are a part of the solutions u and u(t). The other k part is the M that connects the original A to the more convenient matrix J: if u =Au then u =Aku =MJkM−1u k+1 k k 0 0 if du/dt =Au then u(t)=eAtu(0)=MeJtM−1u(0). When M and J are S and Λ (the diagonalizable case) those are the formulas of Sections"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 and 5.4. Appendix B returns to the nondiagonalizable case, and shows how the Jordan form can be reached. I hope the following table will be a convenient summary."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 335 Similarity Transformations"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. A is diagonalizable: The columns of S are eigenvectors and S−1AS=Λ."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. A is arbitrary: The columns of M include “generalized eigenvectors” of A, and the Jordan form M−1AM =J is block diagonal."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. A is arbitrary: The unitaryU can be chosen so thatU−1AU =T is triangular."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "1. IfBissimilartoAandCissimilartoB,showthatCissimilartoA. (LetB=M−1AM andC =N−1BN.) Which matrices are similar to I? (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "2. Describe in words all matrices that are similar to 1 0 , and find two of them. 0 −1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "3. Explain why A is never similar to A+I."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "4. Find a diagonal M, made up of 1s and −1s, to show that     2 1 2 −1     1 2 1  −1 2 −1  A=  is similar to B= .  1 2 1  −1 2 −1 1 2 −1 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5. Show (if B is invertible) that BA is similar to AB."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "6. (a) IfCD=−DC (and D is invertible), show thatC is similar to −C. (b) Deduce that the eigenvalues ofC must come in plus-minus pairs. (c) Show directly that ifCx=λx, thenC(Dx)=−λ(Dx)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "7. Consider any A and a “Givens rotation” M in the 1–2 plane:     a b c cosθ −sinθ 0     A=d e f, M =sinθ cosθ 0. g h i 0 0 1 Choose the rotation angleθto produce zero in the (3,1) entry of M−1AM. Note. This “zeroing” is not so easy to continue, because the rotations that produce zero in place of d and h will spoil the new zero in the corner. We have to leave one diagonalbelowthemainone,andfinishtheeigenvaluecalculationinadifferentway. Otherwise,ifwecouldmakeAdiagonalandseeitseigenvalues,wewouldbefinding therootsofthepolynomialdet(A−λI)byusingonlythesquarerootsthatdetermine cosθ—and that is impossible."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "8. What matrix M changes the basis V = (1,1), V = (1,4) to the basis v = (2,5), 1 2 1 v = (1,4)? The columns of M come from expressing V and V as combinations 2 1 2 ∑m v of the v’s. ij i"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "9. For the same two bases, express the vector (3,9) as a combination c V +c V and 1 1 2 2 also as d v +d v . Check numerically that M connects c to d: Mc=d. 1 1 2 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "10. Confirm the last exercise: If V = m v +m v and V = m v +m v , and 1 11 1 21 2 2 12 1 22 2 m c +m c =d andm c +m c =d , thevectorsc V +c V andd v +d v 11 1 12 2 1 21 1 22 2 2 1 1 2 2 1 1 2 2 are the same. This is the “change of basis formula” Mc=d."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "11. If the transformation T is a reflection across the 45° line in the plane, find its matrix with respect to the standard basis v = (1,0), v = (0,1), and also with respect to 1 2 V =(1,1),V =(1,−1). Show that those matrices are similar. 1 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "12. The identity transformation takes every vector to itself: Tx = x. Find the corre- sponding matrix, if the first basis is v = (1,2), v = (3,4) and the second basis is 1 2 w =(1,0), w =(0,1). (It is not the identity matrix!) 1 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "13. The derivative of a+bx+cx2 is b+2cx+0x2. (a) Write the 3 by 3 matrix D such that     a b     Db=2c. c 0 (b) Compute D3 and interpret the results in terms of derivatives. (c) What are the eigenvalues and eigenvectors of D?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "14. Show that every number is an eigenvalue for T f(x)=df/dx, but the transformation (cid:82) x T f(x)= f(t)dt has no eigenvalues (here −∞<x<∞). 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "15. On the space of 2 by 2 matrices, let T be the transformation that transposes every matrix. Find the eigenvalues and “eigenmatrices” for AT =λA."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "16. (a) Find an orthogonal Q so that Q−1AQ=Λ if     1 1 1 0 0 0     A=1 1 1 and Λ=0 0 0. 1 1 1 0 0 3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 337 Then find a second pair of orthonormal eigenvectors x , x forλ=0. 1 2 (b) Verify that P=x xT+x xT is the same for both pairs. 1 1 2 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "17. Prove that every unitary matrix A is diagonalizable, in two steps: (i) If A is unitary, andU is too, then so is T =U−1AU. (ii) An upper triangular T that is unitary must be diagonal. Thus T =Λ. Any unitary matrix A (distinct eigenvalues or not) has a complete set of orthonormal eigenvectors. All eigenvalues satisfy |λ|=1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "18. Find a normal matrix (NNH =NHN) that is not Hermitian, skew-Hermitian, unitary, or diagonal. Show that all permutation matrices are normal."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "19. SupposeT isa3by3uppertriangularmatrix,withentriest . Comparetheentriesof ij TTH and THT, and show that if they are equal, then T must be diagonal. All normal triangular matrices are diagonal."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "20. If N is normal, show that (cid:107)Nx(cid:107)=(cid:107)NHx(cid:107) for every vector x. Deduce that the ith row of N has the same length as the ith column. Note: If N is also upper triangular, this leads again to the conclusion that it must be diagonal."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "21. Provethatamatrixwithorthonormaleigenvectorsmustbenormal,asclaimedin5T: IfU−1NU =A, or N =UΛUH, then NNH =NHN."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "22. Find a unitaryU and triangular T so thatU−1AU =T, for   (cid:34) (cid:35) 0 1 0 5 −3   A= and A=0 0 0. 4 −2 1 0 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "23. If A has eigenvalues 0, 1, 2, what are the eigenvalues of A(A−I)(A−2I)?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "24. (a) Showby direct multiplication that everytriangular matrix T, say 3 by 3, satisfies its own characteristic equation: (T −λ I)(T −λ I)(T −λ I)=0. 1 2 3 (b) SubstitutingU−1AU forT,deducethefamousCayley-Hamiltontheorem: Every matrixsatisfiesitsowncharacteristicequation. For3by3thisis (A−λ I)(A− 1 λ I)(A−λ I)=0. 2 3 (cid:163) (cid:164)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "25. The characteristic polynomial of A = a b is λ2−(a+d)λ+(ad−bc). By direct c d substitution, verify Cayley-Hamilton: A2−(a+d)A+(ad−bc)I =0."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "26. If a = 1 above the main diagonal and a = 0 elsewhere, find the Jordan form (say ij ij 4 by 4) by finding all the eigenvectors."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "27. Show, by trying for an M and failing, that no two of the three Jordan forms in equa- tion (8) are similar: J (cid:54)=M−1J M, J (cid:54)=M−1J M, and J (cid:54)=M−1J M. 1 2 1 3 2 3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "28. Solve u(cid:48) =Ju by back-substitution, solving first for u (t): 2 (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) du 5 1 u 1 1 =Ju= with initial value u(0)= . dt 05 u 2 2 Noticete5t in the first component u (t). 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "29. Compute A10 and eA if A=MJM−1: (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) 14 9 3 −2 2 1 3 2 A= = . −16 −10 −4 3 0 2 4 3"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "30. Show that A and B are similar by finding M so that B=M−1AM: (cid:34) (cid:35) (cid:34) (cid:35) 1 0 0 1 (a) A= and B= . 1 0 0 1 (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 −1 (b) A= and B= . 1 1 −1 1 (cid:34) (cid:35) (cid:34) (cid:35) 1 2 4 3 (c) A= and B= . 3 4 2 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "31. Which of these matrices A to A are similar? Check their eigenvalues. 1 6 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 0 1 1 1 0 0 1 0 0 1 . 0 1 1 0 0 0 1 1 1 0 0 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "32. There are sixteen 2 by 2 matrices whose entries are 0s and 1s. Similar matrices go into the same family. How many families? How many matrices (total 16) in each family?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "33. (a) If x is in the nullspace of A, show that M−1x is in the nullspace of M−1AM. (b) The nullspaces of A and M−1AM have the same (vectors)(basis)(dimension)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "34. IfAandBhavetheexactlythesameeigenvaluesandeigenvectors,doesA=B? With nindependenteigenvectors,wedohaveA=B. FindA(cid:54)=Bwhenλ=0,0(repeated), but there is only one line of eigenvectors (x ,0). 1 Problems 35–39 are about the Jordan form."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "35. By direct multiplication, find J2 and J3 when (cid:34) (cid:35) c 1 J = . 0 c Guess the form of Jk. Set k =0 to find J0. Set k =−1 to find J−1."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 339"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "36. If J is the 5 by 5 Jordan block with λ = 0, find J2 and count its eigenvectors, and find its Jordan form (two blocks)."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "37. The text solved du/dt = Ju for a 3 by 3 Jordan block J. Add a fourth equation dw/dt =5w+x. Follow the pattern of solutions for z, y, x to find w."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "38. These Jordan matrices have eigenvalues 0, 0, 0, 0. They have two eigenvectors (find them). But the block sizes don’t match and J is not similar to K:     0 1 0 0 0 1 0 0      0 0 0 0   0 0 1 0  J =  and K = .  0 0 0 1   0 0 0 0  0 0 0 0 0 0 0 0 For any matrix M, compare JM with MK. If they are equal, show that M is not invertible. Then M−1JM =K is impossible."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "39. Prove in three steps that AT is always similar to A (we know that the λ’s are the same, the eigenvectors are the problem): (a) For A= one block, find M = permutation so that M−1JM =JT. i i i i i (b) For A= any J, build M from blocks so that M−1JM =JT. 0 0 0 (c) For any A=MJM−1: Show that AT is similar to JT and so to J and to A."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "40. Which pairs are similar? Choose a, b, c, d to prove that the other pairs aren’t: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) a b b a c d d c . c d d c a b b a"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "41. True or false, with a good reason: (a) An invertible matrix can’t be similar to a singular matrix. (b) A symmetric matrix can’t be similar to a nonsymmetric matrix. (c) A can’t be similar to −A unless A=0. (d) A−I can’t be similar to A+I."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "42. Prove that AB has the same eigenvalues as BA."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "43. If A is 6 by 4 and B is 4 by 6, AB and BA have different sizes. Nevertheless, (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) I −A AB 0 I A 0 0 = =G. 0 I B 0 0 I B BA (a) What sizes are the blocks of G? They are the same in each matrix. (b) This equation is M−1FM =G, so F and G have the same 10 eigenvalues. F has the eigenvalues of AB plus 4 zeros; G has the eigenvalues of BA plus 6 zeros. AB has the same eigenvalues as BA plus zeros."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "44. Why is each of these statements true? (a) If A is similar to B, then A2 is similar to B2. (b) A2 and B2 can be similar when A and B are not similar (tryλ=0,0). (cid:163) (cid:164) (cid:163) (cid:164) (c) 3 0 is similar to 3 1 . 0 4 0 4 (cid:163) (cid:164) (cid:163) (cid:164) (d) 3 0 is not similar to 3 1 . 0 3 0 3 (e) If we exchange rows 1 and 2 of A, and then exchange columns 1 and 2, the eigenvalues stay the same. Properties of Eigenvalues and Eigenvectors How are the properties of a matrix reflected in its eigenvalues and eigenvectors? This question is fundamental throughout Chapter 5. A table that organizes the key facts may be helpful. For each class of matrices, here are the special properties of the eigenvalues λ and eigenvectors x . i i Symmetric: AT =A realλ’s orthogonal xTx =0 i j Orthogonal: QT =Q−1 all |λ|=1 orthogonal xTx =0 i j Skew-symmetric: AT =−A imaginaryλ’s orthogonal xTx =0 i j Complex Hermitian: AT =A realλ’s orthogonal xTx =0 i j Positive definite: xTAx>0 allλ>0 orthogonal Similar matrix: B=M−1AM λ(B)=λ(A) x(B)=M−1x(A) Projection: P=P2 =PT λ=1;0 column space; nullspace Reflection: I−2uuT λ=−1;1,...,1 u;u⊥ Rank-1 matrix: uvT λ=vTu;0,...,0 u;v⊥ Inverse: A−1 1/λ(A) eigenvectors of A Shift: A+cI λ(A)+c eigenvectors of A Stable powers: An →0 all |λ|<1 Stable exponential: eAt →0 all Reλ<0 Markov: m >0, ∑n m =1 λ =1 steady state x>0 ij i=1 ij max Cyclic permutation: Pn =I λ =e2πik/n x =(1,λ ,...,λn−1) k k k k Diagonalizable: SΛS−1 diagonal of Λ columns of S are independent Symmetric: QΛQT diagonal of Λ (real) columns of Q are orthonormal Jordan: J =M−1AM diagonal of J each block gives 1 eigenvector Every matrix: A=UΣVT rank(A)=rank(Σ) eigenvectors of ATA, AAT inV,U"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 341 Review Exercises"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.1 Find the eigenvalues and eigenvectors, and the diagonalizing matrix S, for (cid:34) (cid:35) (cid:34) (cid:35) 1 0 7 2 A= and B= . 2 3 −15 −4"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.2 Find the determinants of A and A−1 if (cid:34) (cid:35) λ 2 A=S 1 S−1. 0 λ 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.3 If A has eigenvalues 0 and 1, corresponding to the eigenvectors (cid:34) (cid:35) (cid:34) (cid:35) 1 2 and , 2 −1 how can you tell in advance that A is symmetric? What are its trace and determi- nant? What is A?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.4 Inthepreviousproblem,whatwillbetheeigenvaluesandeigenvectorsofA2? What is the relation of A2 to A?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.5 Does there exist a matrix A such that the entire family A+cI is invertible for all complex numbers c? Find a real matrix with A+rI invertible for all real r."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 Solve for both initial values and then find eAt: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) du 3 1 1 0 = u if u(0)= and if u(0)= . dt 1 3 0 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.7 Would you prefer to have interest compounded quarterly at 40% per year, or annu- ally at 50%?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.8 True or false (with counterexample if false): (a) If B is formed from A by exchanging two rows, then B is similar to A. (b) If a triangular matrix is similar to a diagonal matrix, it is already diagonal. (c) Anytwoofthesestatementsimplythethird: AisHermitian,Aisunitary,A2=I. (d) If A and B are diagonalizable, so is AB."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.9 WhathappenstotheFibonaccisequenceifwegobackwardintime,andhowisF −k related to F ? The law F =F +F is still in force, so F =1. k k+2 k+1 k −1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.10 Find the general solution to du/dt =Au if   0 −1 0   A=1 0 −1. 0 1 0 Can you find a time T at which the solution u(T) is guaranteed to return to the initial value u(0)?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.11 If P is the matrix that projects Rn onto a subspace S, explain why every vector in S is an eigenvector, and so is every vector in S⊥. What are the eigenvai (Note the connection to P2 =P, which means thatλ2 =λ.)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.12 Show that every matrix of order >1 is the sum of two singular matrices."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.13 (a) Show that the matrix differential equation dX/dt = AX +XB has the solution X(t)=eAtX(0)eBt. (b) Prove that the solutions of dX/dt =AX−XA keep the same eigenvalues for all time."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.14 If the eigenvalues of A are 1 and 3 with eigenvectors (5,2) and (2,1), find the solutions to du/dt =Au and u =Au , starting from u=(9,4). k+1 k"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.15 Find the eigenvalues and eigenvectors of   0 −i 0   A=i 1 i. 0 −i 0 What property do you expect for the eigenvectors, and is it true?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.16 By trying to solve (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) a b a b 0 1 = =A c d c d 0 0 show that A has no square root. Change the diagonal entries of A to 4 and find a square root. (cid:104) (cid:105) 0 4"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.17 (a) Find the eigenvalues and eigenvectors of A= . 1 0 4 (b) Solve du/dt =Au starting from u(0)=(100,100). (c) If v(t) = income to stockbrokers and w(t) = income to client, and they help each other by dv/dt = 4w and dw/dt = 1v, what does the ratio v/w approach 4 ast →∞?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.18 True or false, with reason if true and counterexample if false: (a) For every matrix A, there is a solution to du/dt = Au starting from u(0) = (1,...,1). (b) Every invertible matrix can be diagonalized. (c) Every diagonalizable matrix can be inverted. (d) Exchanging the rows of a 2 by 2 matrix reverses the signs of its eigenvalues."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.6 SimilarityTransformations 343 (e) If eigenvectors x and y correspond to distinct eigenvalues, then xHy=0."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.19 If K is a skew-symmetric matrix, show that Q=(I−K)(I+K)−1 is an orthogonal (cid:163) (cid:164) matrix. Find Q if K = 0 2 . −2 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.20 If KH =−K (skew-Hermitian), the eigenvalues are imaginary and the eigenvectors are orthogonal. (a) How do you know that K−I is invertible? (b) How do you know that K =UΛUH for a unitaryU? (c) Why is eΛt unitary? (d) Why is eKt unitary?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.21 If M is the diagonal matrix with entries d, d2, d3, what is M−1AM? What are its eigenvalues in the following case?   1 1 1   A=1 1 1. 1 1 1"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.22 If A2 = −I, what are the eigenvalues of A? If A is a real n by n matrix show that n must be even, and give an example."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.23 If Ax=λ x and ATy=λ y (all real), show that xTy=0. 1 2"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.24 A variation on the Fourier matrix is the “sine matrix”:   sinθ sin2θ sin3θ 1   π S= √ sin2θ sin4θ sin6θ with θ= . 2 4 sin3θ sin6θ sin9θ Verify that ST = S−1. (The columns are the eigenvectors of the tridiagonal −1, 2, −1 matrix.)"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.25 (a) Find a nonzero matrix N such that N3 =0. (b) If Nx=λx, show thatλ must be zero. (c) Prove that N (called a “nilpotent” matrix) cannot be symmetric."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.26 (a) Find the matrix P = aaT/aTa that projects any vector onto the line through a=(2,1,2). (b) What is the only nonzero eigenvalue of P, and what is the corresponding eigen- vector? (c) Solve u =Pu , starting from u =(9,9,0). k+1 k 0"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.27 Suppose the first row of A is 7, 6 and its eigenvalues are i, −i. Find A."
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.28 (a) For which numbers c and d does A have real eigenvalues and orthogonal eigen- vectors?   1 2 0   A=2 d c. 0 5 3 (b) For which c and d can we find three orthonormal vectors that are combinations of the columns (don’t do it!)?"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.29 If the vectors x and x are in the columns of S, what are the eigenvalues and eigen- 1 2 vectors of (cid:34) (cid:35) (cid:34) (cid:35) 2 0 2 3 A=S S−1 and B=S S−1? 0 1 0 1 (cid:34) (cid:35) (cid:34) (cid:35) k .4 .3 a"
    },
    {
        "chapter": "EigenvaluesandEigenvectors",
        "question": "5.30 What is the limit as k →∞ (the Markov steady state) of ? .6 .7 b 6 Chapter Positive Definite Matrices"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.1 Minima, Maxima, and Saddle Points Up to now, we have hardly thought about the signs of the eigenvalues. We couldn’t ask whether λ was positive before it was known to be real. Chapter 5 established that everysymmetricmatrixhasrealeigenvalues. Nowwewillfindatestthatcanbeapplied directly to A, without computing its eigenvalues, which will guarantee that all those eigenvalues are positive. The test brings together three of the most basic ideas in the book—pivots, determinants, and eigenvalues. The signs of the eigenvalues are often crucial. For stability in differential equations, we needed negative eigenvalues so that eλt would decay. The new and highly important problem is to recognize a minimum point. This arises throughout science and engi- neering and every problem of optimization. The mathematical problem is to move the second derivative test F(cid:48)(cid:48) >0 into n dimensions. Here are two examples: F(x,y)=7+2(x+y)2−ysiny−x3 f(x,y)=2x2+4xy+y2. Does either F(x,y) or f(x,y) have a minimum at the point x=y=0? Remark 3. The zero-order terms F(0,0) = 7 and f(0,0) = 0 have no effect on the an- swer. They simply raise or lower the graphs of F and f. Remark 4. The linear terms give a necessary condition: To have any chance of a mini- mum, the first derivatives must vanish at x=y=0: ∂F ∂F =4(x+y)−3x2 =0 and =4(x+y)−ycosy−siny=0 ∂x ∂y ∂f ∂f =4x+4y=0 and =4x+2y=0. All zero. ∂x ∂y Thus (x,y) = (0,0) is a stationary point for both functions. The surface z = F(x,y) is tangent to the horizontal plane z = 7, and the surface z = f(x,y) is tangent to the plane z = 0. The question is whether the graphs go above those planes or not, as we move away from the tangency point x=y=0. Remark 5. The second derivatives at (0,0) are decisive: ∂2F ∂2f =4−6x=4 =4 ∂x2 ∂x2 ∂2F ∂2F ∂2f ∂2f = =4 = =4 ∂x∂y ∂y∂x ∂x∂y ∂y∂x ∂2F ∂2f =4+ysiny−2cosy=2 =2. ∂y2 ∂y2 These second derivatives 4, 4, 2 contain the answer. Since they are the same for F and f, they must contain the same answer. The two functions behave in exactly the same way near the origin. F has a minimum if and only if f has a minimum. I am going to show that those functions don’t! Remark 6. The higher-degree terms in F have no effect on the question of a local min- imum, but they can prevent it from being a global minimum. In our example the term −x3 must sooner or later pull F toward −∞. For f(x,y), with no higher terms, all the action is at (0,0). Everyquadraticform f =ax2+2bxy+cy2 hasastationarypointattheorigin,where ∂f/∂x =∂f/∂y = 0. A local minimum would also be a global minimum, The surface z = f(x,y) will then be shaped like a bowl, resting on the origin (Figure 6.1). If the stationary point of F is at x = α, y = β, the only change would be to use the second derivatives atα,β: Quadratic x2∂2F ∂2F y2∂2F f(x,y)= (α,β)+xy (α,β)+ (α,β). (1) part of F 2 ∂x2 ∂x∂y 2 ∂y2 This f(x,y) behaves near (0,0) in the same way that F(x,y) behaves near (α,β). (cid:163) (cid:164) (cid:163) (cid:164) Figure6.1: Abowlandasaddle: DefiniteA= 10 andindefiniteA= 01 . 01 10 The third derivatives are drawn into the problem when the second derivatives fail to give a definite decision. That happens when the quadratic part is singular. For a true minimum, f is allowed to vanish only at x = y = 0. When f(x,y) is strictly positive at all other points (the bowl goes up), it is called positive definite."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.1 Minima,Maxima,andSaddlePoints 347 Definite versus Indefinite: Bowl versus Saddle The problem comes down to this: For a function of two variables x and y, what is the correct replacement for the condition∂2F/∂x2 >0? With only one variable, the sign of the second derivative decides between a minimum or a maximum. Now we have three second derivatives: F , F = F , and F . These three numbers (like 4, 4, 2) must xx xy yx yy determine whether or not F (as well as f) has a minimum. What conditions on a, b, and c ensure that the quadratic f(x,y)=ax2+2bxy+cy2 is positive definite? One necessary condition is easy: (i) If ax2+2bxy+cy2 is positive definite, then necessarily a>0. We look at x = 1, y = 0, where ax2+2bxy+cy2 is equal to a. This must be positive. Translating back to F, that means that ∂2F/∂x2 > 0. The graph must go up in the x direction. Similarly, fix x=0 and look in the y direction where f(0,y)=cy2: (ii) If f(x,y) is positive definite, then necessarily c>0. Do these conditions a > 0 and c > 0 guarantee that f(x,y) is always positive? The answer is no. A large cross term 2bxy can pull the graph below zero. Example 1. f(x,y)=x2−10xy+y2. Here a=1 and c=1 are both positive. But f is not positive definite, because f(1,1)=−8. The conditions a>0 and c>0 ensure that f(x,y) is positive on the x and y axes. But this function is negative on the line x = y, because b=−10 overwhelms a and c. Example 2. In our original f the coefficient 2b = 4 was positive. Does this ensure a minimum? Again the answer is no; the sign of b is of no importance! Even though its second derivatives are positive, 2x2+4xy+y2 is not positive definite. Neither F nor f has a minimum at (0,0) because f(1,−1)=2−4+1=−1. It is the size of b, compared to a and c, that must be controlled. We now want a necessary and sufficient condition for positive definiteness. The simplest technique is to complete the square: (cid:181) (cid:182) (cid:181) (cid:182) Express f(x,y) b 2 b2 f =ax2+2bxy+cy2 =a x+ y + c− y2. (2) using squares a a The first term on the right is never negative, when the square is multiplied by a > 0. But this square can be zero, and the second term must then be positive. That term has coefficient (ac−b2)/a. The last requirement for positive definiteness is that this coefficient must be positive: (iii) If ax2+2bxy+cy2 stays positive, then necessarily ac>b2. Testforaminimum: Theconditionsa>0andac>b2 arejustright. Theyguarantee c>0. The right side of (2) is positive, and we have found a minimum: 6A ax2+2bxy+cy2 is positive definite if and only if a>0 and ac>b2. Any f(x,y) has a minimum at a point where∂F/∂x=∂F/∂y=0 with (cid:183) (cid:184)(cid:183) (cid:184) (cid:183) (cid:184) ∂F2 ∂F2 ∂F2 ∂F2 2 >0 and > . (3) ∂x2 ∂x2 ∂y2 ∂x∂y Testforamaximum: Since f hasamaximumwhenever−f hasaminimum,wejust reverse the signs of a, b, and c. This actually leaves ac > b2 unchanged: The quadratic form is negative definite if and only if a<0 and ac>b2. The same change applies for a maximum of F(x,y). Singular case ac = b2: The second term in equation (2) disappears to leave only the first square—which is either positive semidefinite, when a>0, or negative semidef- inite, when a<0. The prefix semi allows the possibility that f can equal zero, as it will atthepointx=b,y=−a. Thesurfacez= f(x,y)degeneratesfromabowlintoavalley. For f =(x+y)2, the valley runs along the line x+y=0. Saddle Point ac < b2: In one dimension, F(x) has a minimum or a maximum, or F(cid:48)(cid:48) =0. In two dimensions, a very important possibility still remains: The combination ac−b2 may be negative. This occurred in both examples, when b dominated a and c. It alsooccursif aandchaveoppositesigns. Thentwodirectionsgiveoppositeresults—in one direction f increases, in the other it decreases. It is useful to consider two special cases: Saddle points at (0,0) f =2xy and f =x2−y2 and ac−b2 =−1. 1 2 In the first, b = 1 dominates a = c = 0. In the second, a = 1 and c = −1 have opposite sign. The saddles 2xy and x2−y2 are practically the same; if we turn one through 45° we get the other. They are also hard to draw. These quadratic forms are indefinite, because they can take either sign. So we have a stationary point that is neither a maximum or a minimum. It is called a saddle point. Thesurfacez=x2−y2 goesdowninthedirectionoftheyaxis,wherethelegsfit(ifyou still ride a horse). In case you switched to a car, think of a road going over a mountain pass. The top of the pass is a minimum as you look along the range of mountains, but it is a maximum as you go along the road. Higher Dimensions: Linear Algebra CalculuswouldbeenoughtofindourconditionsF >0andF F >F2 foraminimum. xx xx yy xy Butlinearalgebraisreadytodomore,becausethesecondderivativesfitintoasymmetric matrix A. The terms ax2 and cy2 appear on the diagonal. The cross derivative 2bxy is"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. Thequadratic f =x2+4xy+2y2 hasasaddlepointattheorigin,despitethefactthat its coefficients are positive. Write f as a difference of two squares."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. Decide for or against the positive definiteness of these matrices, and write out the corresponding f =xTAx: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 3 1 −1 2 3 −1 2 (a) . (b) . (c) . (d) . 3 5 −1 1 3 5 2 −8 The determinant in (b) is zero; along what line is f(x,y)=0?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. If a 2 by 2 symmetric matrix passes the tests a > 0, ac > b2, solve the quadratic equation det(A−λI)=0 and show that both eigenvalues are positive."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "4. Decide between a minimum, maximum, or saddle point for the following functions. (a) F =−1+4(ex−x)−5xsiny+6y2 at the point x=y=0. (b) F =(x2−2x)cosy, with stationary point at x=1, y=π. (cid:163) (cid:164)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "5. (a) For which numbers b is the matrix A= 1 b positive definite? b 9 (b) Factor A=LDLT when b is in the range for positive definiteness. (c) Find the minimum value of 1(x2+2bxy+9y2)−y for b in this range. 2 (d) What is the minimum if b=3?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6. Suppose the positive coefficients a and c dominate b in the sense that a+c > 2b. Find an example that has ac<b2, so the matrix is not positive definite."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "7. (a) What 3 by 3 symmetric matrices A and A correspond to f and f ? 1 2 1 2 f =x2+x2+x2−2x x −2x x +2x x 1 1 2 3 1 2 1 3 2 3 f =x2+2x2+11x2−2x x −2x x −4x x . 2 1 2 3 1 2 1 3 2 3 (b) Show that f is a single perfect square and not positive definite. Where is f 1 1 equal to 0? (c) Factor A into LLT, Write f =xTA x as a sum of three squares. 2 2 2 (cid:163) (cid:164)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "8. If A= a b is positive definite, test A−1 =[p q ] for positive definiteness. b c q r"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.1 Minima,Maxima,andSaddlePoints 351"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "9. The quadratic f(x ,x ) = 3(x +2x )2+4x2 is positive. Find its matrix A, factor it 1 2 1 2 2 into LDLT, and connect the entries in D and L to 3, 2, 4 in f."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "10. If R=[p q ], write out R2 and check that it is positive definite unless R is singular. q r (cid:163) (cid:164)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "11. (a) If A= a b is Hermitian (complex b), find its pivots and determinant. b c (b) Complete the square for xHAx. Now xH =[x x ] can be complex 1 2 a|x |2+2Rebx x +c|x |2 =a|x +(b/a)x |2+ |x |2. 1 1 2 2 1 2 2 (c) Show that a>0 and ac>|b|2 ensure that A is positive definite. (cid:163) (cid:164) (cid:163) (cid:164) (d) Are the matrices 1 1+i and 3 4+i positive definite? 1−i 2 4−i 6"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "12. Decide whether F = x2y2−2x−2y has a minimum at the point x = y = 1 (after showing that the first derivatives are zero at that point)."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "13. Under what conditions on a, b, c is ax2+2bxy+cy2 >x2+y2 for all x, y? Problems 14–18 are about tests for positive definiteness."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "14. Which of A , A , A , A has two positive eigenvalues? Test a>0 and ac>b2, don’t 1 2 3 4 compute the eigenvalues. Find an x so that xTA x<0. 1 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 5 6 −1 −2 1 10 1 10 A = A = A = A = . 1 2 3 4 6 7 −2 −5 10 100 10 101"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "15. What is the quadratic f =ax2+2bxy+cy2 for each of these matrices? Complete the square to write f as a sum of one or two squares d ( )2+d ( )2. 1 2 (cid:34) (cid:35) (cid:34) (cid:35) 1 2 1 3 A= and A= . 2 9 3 9"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "16. Show that f(x,y) = x2+4xy+3y2 does not have a minimum at (0,0) even though it has positive coefficients. Write f as a difference of squares and find a point (x,y) where f is negative."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "17. (Important) If A has independent columns, then ATA is square and symmetric and invertible (Section 4.2). Rewrite xTATAx to show why it is positive except when x=0. Then ATA is positive definite."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "18. Test to see if ATA is positive definite in each case:   (cid:34) (cid:35) (cid:34) (cid:35) 1 1 1 2   1 1 2 A= , A=1 2, and A= . 0 3 1 2 1 2 1"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "19. Find the 3 by 3 matrix A and its pivots, rank, eigenvalues, and determinant:    x (cid:104) (cid:105) 1    x x x  A x =4(x −x +2x )2. 1 2 3 2 1 2 3 x 3"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "20. For F (x,y) = 1x4+x2y+y2 and F (x,y) = x3+xy−x, find the second derivative 1 4 2 matrices A and A : 1 2 (cid:34) (cid:35) ∂2F/∂x2 ∂2F/∂x∂y A= . ∂2F/∂y∂x ∂2F/∂y2 A is positive definite, so F is concave up (= convex). Find the minimum point of 1 1 F and the saddle point of F (look where first derivatives are zero). 1 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "21. The graph of z = x2+y2 is a bowl opening upward. The graph of z = x2−y2 is a saddle. The graph of z = −x2−y2 is a bowl opening downward. What is a test on F(x,y) to have a saddle at (0,0)?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "22. Which values of c give a bowl and which give a saddle point for the graph of z = 4x2+12xy+cy2? Describe this graph at the borderline value of c."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.2 Tests for Positive Definiteness Which symmetric matrices have the property that xTAx > 0 for all nonzero vectors x? There are four or five different ways to answer this question, and we hope to find all of them. The previous section began with some hints about the signs of eigenvalues. but that gave place to the tests on a, b, c: (cid:34) (cid:35) a b b= is positive definite when a>0 and ac−b2 >0. b c Fromthoseconditions,botheigenvaluesarepositive. Theirproductλ λ isdeterminant 1 2 ac−b2 > 0, so the eigenvalues are either both positive or both negative. They must be positive because their sum is the trace a+c>0. Lookingataandac−b2,itisevenpossibletospottheappearanceofthepivots. They turned up when we decomposed xTAx into a sum of squares: (cid:181) (cid:182) b 2 ac−b2 Sum of squares ax2+2bxy+cy2 =a x+ y + y2. (1) a a Those coefficients a and (ac−b2)/a are the pivots for a 2 by 2 matrix. For larger matrices the pivots still give a simple test for positive definiteness: xTAx stays positive when n independent squares are multiplied by positive pivots."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.2 TestsforPositiveDefiniteness 353 One more preliminary remark. The two parts of this hook were linked by the chapter on determinants. Therefore we ask what part determinants play. It is not enough to require that the determinant of A is positive. If a = c = −1 and b = 0. then detA = 1 but A = −I = negative definite. The determinant test is applied not only to A itself, giving ac−b2 >0, but also to the 1 by 1 submatrix a in the upper left-hand corner. The natural generalization will involve all n of the upper left submatrices of A:   (cid:34) (cid:35) a a a (cid:104) (cid:105) 11 12 13 a a   11 12 A = a , A = , A =a a a ,···, A =A. 1 11 2 3 21 22 23 n a a 21 22 a a a 31 32 33 Here is the main theorem on positive definiteness, and a reasonably detailed proof: 6B Each of the following tests is a necessary and sufficient condition for the real symmetric matrix A to be positive definite: (I) xTkx>0 for all nonzero real vectors x. (II) All the eigenvalues of A satisfyλ >0. i (III) All the upper left submatrices A have positive determinants. k (IV) All the pivots (without row exchanges) satisfy d >0. k Proof. Condition I defines a positive definite matrix. Our first step shows that each eigenvalue will be positive: If Ax=λx, then xTAx=xTλx=λ(cid:107)x(cid:107)2. A positive definite matrix has positive eigenvalues, since xTAx>0. Now we go in the other direction. If all λ > 0, we have to prove xTAx > 0 for i every vector x (not just the eigenvectors). Since symmetric matrices have a full set of orthonormal eigenvectors, any x is a combination c x +···+c x . Then 1 1 n n Ax=c Ax +···+c Ax =c λ x +···+c λ x . 1 1 n n 1 1 1 n n n Because of the orthogonality xTx =0, and the normalization xTx =1, i i i i (cid:161) (cid:162) xTAx= c xT+···+c xT (c λ x +···+c λ x ) 1 1 n n 1 1 1 n n n (2) =c2λ +···+c2λ . 1 1 n n If everyλ >0, then equation (2) shows that xTAx>0. Thus condition II implies condi- i tion I. If condition I holds, so does condition III: The determinant of A is the product of the eigenvalues. And if condition I holds, we already know that these eigenvalues are positive. But we also have to deal with every upper left submatrix A . The trick is to k look at all nonzero vectors whose last n−k components are zero: (cid:34) (cid:35)(cid:34) (cid:35) (cid:104) (cid:105) A ∗ x xTAx= xT 0 k k =xTA x >0. k ∗ ∗ 0 k k k Thus A is positive definite. Its eigenvalues (not the same λ !) must be positive. Its k 1 determinant is their product, so all upper left determinants are positive. If condition III holds, so does condition IV: According to Section 4.4, the kth pivot d is the ratio of detA to detA . If the determinants are all positive, so are the pivots. k k k−1 If condition IV holds, so does condition I: We are given positive pivots, and must deduce that xTAx>0. This is what we did in the 2 by 2 case, by completing the square. Thepivotswerethenumbersoutsidethesquares. Toseehowthathappensforsymmetric matrices of any size, we go back to elimination on a symmetric matrix: A=LDLT. Example 1. Positive pivots 2, 3, and 4: 2 3       2 −1 0 1 0 0 2 1 −1 0 2       A=−1 2 −1=−1 1 0 3 0 1 −2 =LDLT. 2 2 3 0 −1 2 0 −2 1 4 0 0 1 3 3 I want to split xTAx into xTLDLTx:        u 1 −1 0 u u−1v 2 2        If x=v, then LTx=0 1 −2 v=v−2w. 3 3 w 0 0 1 w w So xTAx is a sum of squares with the pivots 2, 3, and 4 as coefficients: 2 3 (cid:181) (cid:182) (cid:181) (cid:182) 2 2 1 3 2 4 xTAx=(LTx)TD(LTx)=2 u− v + v− w + (w)2. 2 2 3 3 Those positive pivots in D multiply perfect squares to make xTAx positive. Thus condi- tion IV implies condition I, and the proof is complete. Itisbeautifulthateliminationandcompletingthesquareareactuallythesame. Elim- ination removes x from all later equations. Similarly, the first square accounts for all 1 terms in xTAx involving x . The sum of squares has the pivots outside. The multipliers 1 (cid:96) are inside! You can see the numbers −1 and −2 inside the squares in the example. ij 2 3 Every diagonal entry a must be positive. As we know from the examples, however, ii it is far from sufficient to look only at the diagonal entries. The pivots d are not to be confused with the eigenvalues. For a typical positive i definite matrix, they are two completely different sets of positive numbers, In our 3 by 3 example, probably the determinant test is the easiest: Determinant test detA =2, detA =3, detA =detA=4. 1 2 3 The pivots are the ratios d = 2, d = 3, d = 4. Ordinarily the eigenvalue test is the 1 2 2 3 3 longest computation. For this A we know theλ’s are all positive: √ √ Eigenvalue test λ =2− 2, λ =2, λ =2+ 2. 1 2 3"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.2 TestsforPositiveDefiniteness 355 Even though it is the hardest to apply to a single matrix, eigenvalues can be the most useful test for theoretical purposes. Each test is enough by itself. Positive Definite Matrices and Least Squares I hope you will allow one more test for positive definiteness. It is already close. We connectedpositivedefinitematricestopivots(Chapter1),determinants(Chapter4),and eigenvalues (Chapter 5). Now we see them in the least-squares problems in Chapter 3, coming from the rectangular matrices of Chapter 2. The rectangular matrix will be R and the least-squares problem will be Rx=b. It has m equations with m≥n (square systems are included). The least-square choice x(cid:98)is the solution of RTRx(cid:98)= RTb. That matrix ARTR is not only symmetric but positive definite, as we now show—provided that the n columns of R are linearly independent: 6C The symmetric matrix A is positive definite if and only if (V) There is a matrix R with independent columns such that A=RTR. The key is to recognize xTAx as xTRTRx = (Rx)T(Rx). This squared length (cid:107)Rx(cid:107)2 is positive (unless x = 0), because R has independent columns. (If x is nonzero then Rx is nonzero.) Thus xTRTRx>0 and RTR is positive definite. It remains to find an R For which A=RTR. We have almost done this twice already: √ √ √ Elimination A=LDLT =(L D)( DLT). So take R= DLT. This Cholesky decomposition has the pivots split evenly between L and LT. √ √ √ Eigenvalues A=QΛQT =(Q Λ)( ΛQT). So take R= ΛQT. (3) √ A third possibility is R = Q ΛQT, the symmetric positive definite square root of A. Therearemanyotherchoices,squareorrectangular,andwecanseewhy. Ifyoumultiply anyRbyamatrixQwithorthonormalcolumns,then(QR)T(QR)=RTQTQR=RTIR= A. Therefore QR is another choice. Applications of positive definite matrices are developed in my earlier book Intro- duction to Applied Mathematics and also the new Applied Mathematics and Scientific Computing (see www.wellesleycambridge.com). We mention that Ax = λMx arises constantly in engineering analysis. If A and M are positive definite, this general- ized problem is parallel to the familiar Ax =λx, andλ>0. M is a mass matrix for the finite element method in Section 6.4. Semidefinite Matrices The tests for semidefiniteness will relax xTAx > 0, λ> 0, d > 0, and det > 0, to allow zeros to appear. The main point is to see the analogies with the positive definite case. 6D Each of the following tests is a necessary and sufficient condition for a symmetric matrix A to be positive semidefinite: (I(cid:48)) xTAx≥0 for all vectors x (this defines positive semidefinite). (II(cid:48)) All the eigenvalues of A satisfyλ ≥0. i (III(cid:48)) No principal submatrices have negative determinants. (IV(cid:48)) No pivots are negative. (V(cid:48)) There is a matrix R, possibly with dependent columns, such that A=RTR. ThediagonalizationA=QΛQT leadstoxTAx=xTQΛQTx=yTΛy. IfAhasrankr,there are r nonzeroλ’s and r perfect squares in yTΛy=λ y2+···+λy2. 1 1 r r Note. The novelty is that condition III(cid:48) applies to all the principal submatrices, not only those in the upper left-hand corner. Otherwise, we could not distinguish between two matrices whose upper left determinants were all zero: (cid:34) (cid:35) (cid:34) (cid:35) 0 0 0 0 is positive semidefinite, and is negative semidefinite. 0 1 0 −1 A row exchange comes with the same column exchange to maintain symmetry. Example 2.   2 −1 −1   A=−1 2 −1 is positive semidefinite, by all five tests: −1 −1 2 (I(cid:48)) xTAx=(x −x )2+(x −x )2+(x −x )2 ≥0 (zero if x =x =x ). 1 2 1 3 2 3 1 2 3 (II(cid:48)) The eigenvalues areλ =0,λ =λ =3 (a zero eigenvalue). 1 2 3 (III(cid:48)) detA=0 and smaller determinants are positive.       2 −1 −1 2 0 0 2 0 0       (IV(cid:48)) A=−1 2 −1→0 3 −3 →0 3 0 (missing pivot). 2 2 2 −1 −1 2 0 −3 3 0 0 0 2 2 (V(cid:48)) A=RTR with dependent columns in R:      2 −1 −1 1 −1 0 1 0 −1      −1 2 −1= 0 1 −1−1 1 0  (1,1,1) in the nullspace. −1 −1 2 −1 0 1 0 −1 1 Remark. Theconditionsforsemidefinitenesscouldalsobededucedfromtheorigincon- ditions I-V for definiteness by the following trick: Add a small multiple of the identity giving a positive definite matrix A+εI. Then let ε approach zero. Since the determi- nants and eigenvalues depend continuously onε, they will be positive until the very last moment. Atε=0 they must still be nonnegative."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.2 TestsforPositiveDefiniteness 357 My class often asks about unsymmetric positive definite matrices. I never use that term. One reasonable definition is that the symmetric part 1(A+AT) should be positive 2 definite. That guarantees that the real parts of the eigenvalues are positive. But it is not (cid:163) (cid:164) (cid:163) (cid:164) necessary: A= 1 4 hasλ>0 but 1(A+AT)= 1 2 is indefinite. 0 1 2 2 1 If Ax=λx, then xHAx=λxHx and xHAHx=λxHx. Adding, 1xH(A+AH)x=(Reλ)xHx>0, so that Reλ>0. 2 Ellipsoids in n Dimensions Throughout this book, geometry has helped the matrix algebra. A linear equation pro- duced a plane. The system Ax = b gives an intersection of planes. Least squares gives a perpendicular projection. The determinant is the volume of a box. Now, for a positive definite matrix and its xTAx, we finally get a figure that is curved. It is an ellipse in two dimensions, and an ellipsoid in n dimensions. The equation to consider is xTAx = 1. If A is the identity matrix, this simplifies to x2+x2+···+x2 = 1. This is the equation of the “unit sphere” in Rn. If A = 4I, the 1 2 n spheregetssmaller. Theequationchangesto4x2+···+4x2 =1. Insteadof(1,0,...,0), 1 n it goes through (1,0,...,0). The center is at the origin, because if x satisfies xTAx = 1, 2 so does the opposite vector −x. The important step is to go from the identity matrix to a diagonal matrix:   4   Ellipsoid For A= 1 , the equation is xTAx=4x2+x2+1x2 =1. 1 2 9 3 1 9 Since the entries are unequal (and positive!) the sphere changes to an ellipsoid. One solution is x = (1,0,0) along the first axis. Another is x = (0,1,0). The major 2 axishasthefarthestpointx=(0,0,3). Itislikeafootballorarugbyball,butnotquite— those are closer to x2+x2+ 1x2 = 1. The two equal coefficients make them circular in 1 2 2 3 the x -x plane, and much easier to throw! 1 2 Now comes the final step, to allow nonzeros away from the diagonal of A. (cid:163) (cid:164) Example 3. A = 5 4 and xTAx = 5u2+8uv+5v2 = 1. That ellipse is centered at 4 5 u = v = 0, but the axes are not so clear. The off-diagonal 4s leave the matrix positive definite, but they rotate the ellipse—its axes no longer line up with the coordinate axes (Figure 6.2). We will show that the axes of the ellipse point toward the eigenvector of A. Because A = AT, those eigenvectors and axes are orthogonal. The major axis of the ellipse corresponds to the smallest eigenvalue of A. To locate the ellipse we compute λ = 1 and λ = 9. The unit eigenvectors are √ √ 1 2 (1,−1)/ 2 and (1,1)/ 2. Those are at 45° angles with the u-v axes, and they are lined up with the axes of the ellipse. The way to see the ellipse properly is to rewrite v 1 1 1 Q = √ , √ 3 (cid:16) 2 2(cid:17) b u − 1 1 1 bP = √1 ,−√1 (cid:16) 2 2(cid:17) Figure6.2: TheellipsexTAx=5u2+8uv+5v2=1anditsprincipalaxes. xTAx=1: (cid:181) (cid:182) (cid:181) (cid:182) 2 2 u v u v New squares 5u2+8uv+v2 = √ −√ +9 √ +√ =1. (4) 2 2 2 2 λ= 1 and λ= 9 are outside the squares. The eigenvectors are inside. This is different from completing the square to 5(u+4v)2+9v2, with the pivots outside. √ 5 √5 The first square equals 1 at (1/ 2,−1/ 2) at the end of the major axis. The minor axis is one-third as long, since we need (1)2 to cancel the 9. 3 Any ellipsoid xTAx = 1 can be simplified in the same way. The key step is to diago- nalize A = QΛQT. We straightened the picture by rotating the axes. Algebraically, the change to y=QTx produces a sum of squares: xTAx=(xTQ)Λ(QTx)=yTΛy=λ y2+···+λ y2 =1. (5) 1 1 n n √ The major axis has y =1/ λ along the eigenvector with the smallest eigenvalue. 1 1 √ √ Theotheraxesarealongtheothereigenvectors. Theirlengthsare1/ λ ,...,1/ λ . 2 n Notice that the λ’s must be positive—the matrix must be positive definite—or these square roots are in trouble. An indefinite equation y2−9y2 = 1 describes a hyperbola 1 2 and not an ellipse. A hyperbola is a cross-section through a saddle, and an ellipse is a cross-section through a bowl. The change from x to y = QTx rotates the axes of the space, to match the axes of the ellipsoid. In the y variables we can see that it is an ellipsoid, because the equation becomes so manageable: 6E Suppose A=QΛQT withλ >0. Rotating y=QTx simplifies xTAx=1: i xTQΛQTx=1, yTΛy=1, and λ y2+···+λ y2 =1. 1 1 n n √ √ This is the equation of an ellipsoid. Its axes have lengths 1/ λ ,...,1/ λ 1 n from the center. In the original x-space they point along the eigenvectors of A."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.2 TestsforPositiveDefiniteness 359 The Law of Inertia For elimination and eigenvalues, matrices become simpler by elementary operations The essential thing is to know which properties of the matrix stay unchanged. When a multiple of one row is subtracted from another, the row space, nullspace. rant and determinant all remain the same. For eigenvalues, the basic operation was a similarity transformation A→S−1AS (or A→M−1AM). The eigenvalues are unchanged (and also the Jordan form). Now we ask the same question for symmetric matrices: What are the elementary operations and their invariants for xTAx? The basic operation on a quadratic form is to change variables. A new vector y is relatedtoxbysomenonsingularmatrix,x=Cy. ThequadraticformbecomesyTCTACy. This shows the fundamental operation on A: Congruence transformation A→CTAC for some nonsingularC. (6) The symmetry of A is preserved, since CTAC remains symmetric. The real question is, What other properties are shared by A and CTAC? The answer is given by Sylvester’s law of inertia. 6F CTAC has the same number of positive eigenvalues, negative eigenvalues, and zero eigenvalues as A. The signs of the eigenvalues (and not the eigenvalues themselves) are preserved by a congruence transformation. In the proof, we will suppose that A is nonsingular. Then CTAC is also nonsingular, and there are no zero eigenvalues to worry about. (Otherwise we can work with the nonsingular A+εI and A−εI, and at the end letε→0.) Proof. We want to borrow a trick from topology. SupposeC is linked to an orthogonal matrixQbyacontinuouschainofnonsingularmatricesC(t). Att =0andt =1,C(0)= C and C(1) = Q. Then the eigenvalues of C(t)TAC(t) will change gradually, as t goes from 0 to 1, from the eigenvalues ofCTAC to the eigenvalues of QTAQ. BecauseC(t) is never singular, none of these eigenvalues can touch zero (not to mention cross over it!). Therefore the number of eigenvalues to the right of zero, and the number to the left, is the same forCTAC as for QTAQ. And A has exactly the same eigenvalues as the similar matrix Q−1AQ=QTAQ. OnegoodchoiceforQistoapplyGram-SchmidttothecolumnsofC. ThenC=QR, andthechainofmatricesisC(t)=tQ+(1−t)QR. ThefamilyC(t)goesslowlythrough Gram-Schmidt, from QR to Q. It is invertible, because Q is invertible and the triangular factortI+(1−t)R has positive diagonal. That ends the proof. Example 4. Suppose A = I. Then CTAC =CTC is positive definite. Both I and CTC have n positive eigenvalues, confirming the law of inertia. (cid:163) (cid:164) Example 5. If A= 1 0 , thenCTAC has a negative determinant: 0 −1 detCTAC =(detCT)(detA)(detC)=−(detC)2 <0. ThenCTAC must have one positive and one negative eigenvalue, like A. Example 6. This application is the important one: 6G For any symmetric matrix A, the signs of the pivots agree with the signs of the eigenvalues. The eigenvalue matrix Λ and the pivot matrix D have the same number of positive entries, negative entries, and zero entries. We will assume that A allows the symmetric factorization A = LDLT (without row ex- changes). By the law of inertia, A has the same number of positive eigenvalues as D. But the eigenvalues of D are just its diagonal entries (the pivots). Thus the number of positive pivots matches the number of positive eigenvalues of A. That is both beautiful and practical. It is beautiful because it brings together (for symmetric matrices) two parts of this book that were previously separate: pivots and eigenvalues. It is also practical, because the pivots can locate the eigenvalues:     3 3 0 1 3 0 A has positive pivots     A=3 10 7 A−2I =3 8 7. A−2I has a negative pivot 0 7 8 0 7 6 Ahaspositiveeigenvalues,byourtest. Butweknowthatλ issmallerthan2,because min subtracting 2 dropped it below zero. The next step looks at A−I, to see if λ <1. (It min is, because A−I has a negative pivot.) That interval containingλ is cut in half at every step by checking the signs of the pivots. Thiswasalmostthefirstpracticalmethodofcomputingeigenvalues. Itwasdominant about 1960, after one important improvement—to make A tridiagonal first. Then the pivotsarecomputedin2nstepsinsteadof 1n3. Eliminationbecomesfast,andthesearch 6 for eigenvalues (by halving the intervals) becomes simple. The current favorite is the QR method in Chapter 7. The Generalized Eigenvalue Problem Physics, engineering, and statistics are usually kind enough to produce symmetric ma- trices in their eigenvalue problems. But sometimes Ax =λx is replaced by Ax =λMx. There are two matrices rather than one. An example is the motion of two unequal masses in a line of springs: d2v (cid:34) (cid:35) (cid:34) (cid:35) m +2v−w=0 1 dt2 m 1 0 d2u 2 −1 or + u=0. (7) d2w 0 m dt2 −1 2 2 m −v+2w=0 2 dt2 When the masses were equal, m = m = 1, this was the old system u(cid:48)(cid:48)+Au = 0. Now 1 2 it is Mu(cid:48)(cid:48)+Au=0, with a mass matrix M. The eigenvalue problem arises when we look"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.2 TestsforPositiveDefiniteness 361 for exponential solutions eiωtx: Mu(cid:48)(cid:48)+Au=0 becomes M(iω)2eiωtx+Aeiωtx=0. (8) Canceling eiωt, and writingλ forω2, this is an eigenvalue problem: (cid:34) (cid:35) (cid:34) (cid:35) 2 −1 m 0 1 Generalized problem Ax=λMx x=λ x. (9) −1 2 0 m 2 There is a solution when A−λM is singular. The special choice M =I brings back the usual det(A−λI)=0. We work out det(A−λM) with m =1 and m =2: 1 2 (cid:34) (cid:35) √ 2−λ −1 3± 3 det =2λ2−6λ+3=0 gives λ= . −1 2−2λ 2 √ For the eigenvector x ( 3−1,1), the two masses oscillate together—but the first mass 1√ only moves as far as 3−1 ≈ .73. In the fastest mode, the components of x = (1+ √ 2 3,−1) have opposite signs and the masses move in opposite directions. This time the smaller mass goes much further. The underlying theory is easier to explain if M is split into RTR. (M is assumed to be positive definite.) Then the substitution y=Rx changes Ax=λMx=λRTRx into AR−1y=λRTy. Writing C for R−1, and multiplying through by (RT)−1 =CT, this becomes a standard eigenvalue problem for the single symmetric matrixCTAC: Equivalent problem CTACy=λy. (10) The eigenvaluesλ are the same as for the original Ax=λMx. and the eigenvectors are j relatedbyy =Rx . ThepropertiesofCTAC leaddirectlytothcpropertiesofAx=λMx, j j when A=AT and M is positive definite:"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. The eigenvalues for Ax=λMx are real, becauseCTAC is symmetric."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. Theλ’s have the same signs as the eigenvalues of A, by the law of inertia."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. For what range of numbers a and b are the matrices A and B positive definite?     a 2 2 1 2 4     A=2 a 2 B=2 b 8. 2 2 a 4 8 7"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. Decide for or against the positive definiteness of       2 2 −1 −1 2 −1 −1 0 1 2       A=−1 2 −1, B=−1 2 1 , C =1 0 1 . −1 −1 2 −1 1 2 2 1 0"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. Construct an indefinite matrix with its largest entries on the main diagonal:   1 b −b   A= b 1 b  with |b|<1 can have detA<0. −b b 1"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "4. Show from the eigenvalues that if A is positive definite, so is A2 and so is A−1."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "5. IfAandBarepositivedefinite,thenA+Bispositivedefinite. Pivotsandeigenvalues are not convenient for A+B. Much better to prove xT(A+B)x>0. (cid:163) (cid:164)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6. From the pivots, eigenvalues, and eigenvectors of A= 5 4 , write A as RTR in three √ √ √ √ √ 4 5√ ways: (L D)( DLT), (Q Λ)( ΛQT), and (Q ΛQT)(Q ΛQT). √"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "7. If A=QΛQT is symmetric positive definite, then R=Q ΛQT is its symmetric pos- itive definite square root. Why does R have positive eigenvalues? Compute R and verify R2 =A for (cid:34) (cid:35) (cid:34) (cid:35) 10 6 10 −6 A= and A= . 6 10 −6 10"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "8. If A is symmetric positive definite andC is nonsingular, prove that B=CTAC is also symmetric positive definite."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "9. If A=RTR prove the generalized Schwarz inequality |xTAy|2 ≤(xTAx)(yTAy). (cid:163) (cid:164)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "10. The ellipse u2+4v2 =1 corresponds to A= 1 0 . Write the eigenvalues and eigen- 0 4 vectors, and sketch the ellipse. √"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "11. Reduce the equation 3u2 −2 2uv+2v2 = 1 to a sum of squares by finding the eigenvalues of the corresponding A, and sketch the ellipse."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.2 TestsforPositiveDefiniteness 363"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "12. In three dimensions,λ y2+λ y2+λ y2 =1 represents an ellipsoid when allλ >0. 1 1 2 2 3 3 i Describe all the different kinds of surfaces that appear in the positive semidefinite case when one or more of the eigenvalues is zero."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "13. Write down the five conditions for a 3 by 3 matrix to be negative definite (−A is positive definite) with special attention to condition III: How is det(−A) related to detA?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "14. Decidewhetherthefollowingmatricesarepositivedefinite,negativedefinite,semidef- inite, or indefinite:     1 2 0 0 1 2 3     2 6 −2 0  A=2 5 4, B= , C =−B, D=A−1. 0 −2 5 −2 3 4 9 0 0 −2 3 Is there a real solution to −x2−5y2−9z2−4xy−6xz−8yz=1?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "15. Suppose A is symmetric positive definite and Q is an orthogonal matrix. True or false: (a) QTAQ is a diagonal matrix. (b) QTAQ is symmetric positive definite. (c) QTAQ has the same eigenvalues as A. (d) e−A is symmetric positive definite."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "16. IfAispositivedefiniteanda isincreased,provefromcofactorsthatthedeterminant 11 is increased. Show by example that this can fail if A is indefinite."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "17. From A = RTR. show for positive definite matrices that detA ≤ a a ···a . (The 11 22 nn length squared of column j of R is a . Use determinant = volume.) jj"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "18. (Lyapunov test for stability of M) Suppose AM+MHA = −I with positive definite A. If Mx=λx show that ReA<0. (Hint: Multiply the first equation by xH and x.)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "19. Which 3 by 3 symmetric matrices A produce these functions f = xTAx? Why is the first matrix positive definite but not the second one? (a) f =2(x2+x2+x2−x x −x x ). 1 2 3 1 2 2 3 (b) f =2(x2+x2+x2−x x −x x −x x ). 1 2 3 1 2 1 3 2 3"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "20. Compute the three upper left determinants to establish positive definiteness. Verify that their ratios give the second and third pivots.   2 2 0   A=2 5 3. 0 3 8"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "21. A positive definite matrix cannot have a zero (or even worse, a negative number) on its diagonal. Show that this matrix fails to have xTAx>0:    4 1 1 x (cid:104) (cid:105) 1    x x x 1 0 2x  is not positive when (x ,x ,x )=( , , ). 1 2 3 2 1 2 3 1 2 5 x 3"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "22. A diagonal entry a of a symmetric matrix cannot be smaller than all λ’s. If it jj were,thenA−a I wouldhave eigenvaluesandwouldbepositivedefinite. But jj A−a I has a on the main diagonal. jj"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "23. Give a quick reason why each of these statements is true: (a) Every positive definite matrix is invertible. (b) The only positive definite projection matrix is P=I. (c) A diagonal matrix with positive diagonal entries is positive definite. (d) A symmetric matrix with a positive determinant might not be positive definite!"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "24. For which s andt do A and B have allλ>0 (and are therefore positive definite)?     s −4 −4 t 3 0     A=−4 s −4 and B=3 t 4. −4 −4 s 0 4 t"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "25. You may have seen the equation for an ellipse as (x)2+(y )2 = 1. What are a and a b b when the equation is written as λ x2+λ y2 = 1? The ellipse 9x2+16y2 = 1 has 1 2 half-axes with lengths a= , and b= ."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "26. Draw the tilted ellipse x2+xy+y2 =1 and find the half-lengths of its axes from the eigenvalues of the corresponding A. √ √"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "27. WithpositivepivotsinD, thefactorizationA=LDLT becomesL D DLT. (Square √ √ √ roots of the pivots give D = D D.) Then C = L D yields the Cholesky factor- ization A=CCT, which is “symmetrized LU”: (cid:34) (cid:35) (cid:34) (cid:35) 3 0 4 8 From C = find A. From A= findC. 1 2 8 25 √"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "28. IntheCholeskyfactorizationA=CCT,withC=L D,thesquarerootsofthepivots are on the diagonal ofC. FindC (lower triangular) for     9 0 0 1 1 1     A=0 1 2 and A=1 2 2. 0 2 8 1 2 7"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.2 TestsforPositiveDefiniteness 365"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "29. The symmetric factorization A=LDLT means that xTAx=xTLDLTx: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35)(cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) a b x 1 0 a 0 1 b/a x x y = x y . b c y b/a 1 0 (ac−b2)/a 0 1 y The left-hand side is ax2+2bxy+cy2. The right-hand side is a(x+ by)2+ y2. a The second pivot completes the square! Test with a=2, b=4, c=10. (cid:163) (cid:164)(cid:163) (cid:164)(cid:163) (cid:164)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "30. Without multiplying A= cosθ −sinθ 2 0 cosθ sinθ , find sinθ cosθ 0 5 −sinθ cosθ (a) the determinant of A. (b) the eigenvalues of A. (c) the eigenvectors of A. (d) a reason why A is symmetric positive definite."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "31. For the semidefinite matrices     2 −1 −1 1 1 1     A=−1 2 −1 (rank 2) and B=1 1 1 (rank 1), −1 −1 2 1 1 1 write xTAx as a sum of two squares and xTBx as one square."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "32. Apply any three tests to each of the matrices     1 1 1 2 1 2     A=1 1 1 and B=1 1 1, 1 1 0 2 1 2 to decide whether they are positive definite, positive semidefinite, or indefinite. (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "33. ForC = 2 0 and A= 1 1 , confirm thatCTAC has eigenvalues of the same signs 0 −1 1 1 as A. Construct a chain of nonsingular matrices C(t) linking C to an orthogonal Q. Why is it impossible to construct a nonsingular chain linking C to the identity matrix?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "34. If the pivots of a matrix are all greater than 1, are the eigenvalues all greater than 1? Test on the tridiagonal −1, 2, −1 matrices."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "35. Use the pivots of A−1I to decide whether A has an eigenvalue smaller than 1: 2 2  "
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2.5 3 0 1   A− I = 3 9.5 7 . 2 0 7 7.5"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "36. An algebraic proof of the law of inertia starts with the orthonormal eigenvectors x ,...,x ofAcorrespondingtoeigenvaluesλ >0. andtheorthonormaleigenvectors 1 p i y ,...,y ofCTAC corresponding to eigenvalues µ <0. 1 q i (a) To prove that the p+q vectors x ,...,x , Cy ,...,Cy are independent, assume 1 p 1 q that some combination gives zero: a x +···+a x =b Cy +···+b Cy (=z, say). 1 1 p p 1 1 q q Show that zTAz=λ a2+···+λ a2 ≥0 and zTAz=µ b2+···+µ b2 ≤0. 1 1 p p 1 1 q q (b) Deduce that the a’s and b’s are zero (proving linear independence). From that deduce p+q≤n. (c) The same argument for the n− p negative λ’s and the n−q positive µ’s gives n− p+n−q ≤ n. (We again assume no zero eigenvalues—which are handled separately). Show that p+q = n, so the number p of positive λ’s equals the number n−q of positive µ’s—which is the law of inertia."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "37. If C is nonsingular, show that A and CTAC have the same rank. Thus they have the same number of zero eigenvalues."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "38. Find by experiment the number of positive, negative, and zero eigenvalues of (cid:34) (cid:35) I B A= BT 0 when the block B (of order 1n) is nonsingular. 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "39. Do A andCTAC always satisfy the law of inertia whenC is not square?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "40. In equation (9) with m = 1 and m = 2, verify that the normal modes are M- 1 2 orthogonal: xTMx =0. 1 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "41. Find the eigenvalues and eigenvectors of Ax=λMx: (cid:34) (cid:35) (cid:34) (cid:35) 6 −3 λ 4 1 x= x. −3 6 18 1 4"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "42. If the symmetric matrices A and M are indefinite, Ax = λMx might not have real eigenvalues. Construct a 2 by 2 example."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "43. A group of nonsingular matrices includes AB and A−1 if it includes A and B. “Prod- uctsandinversesstayinthegroup.” Whichofthesesetsaregroups? Positivedefinite symmetric matrices A, orthogonal matrices Q, all exponentials etA of a fixed matrix A, matrices P with positive eigenvalues, matrices D with determinant 1. Invent a group containing only positive definite matrices."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.3 SingularValueDecomposition 367"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.3 Singular Value Decomposition A great matrix factorization has been saved for the end of the basic course. UΣVT joins with LU from elimination and QR from orthogonalization (Gauss and Gram-Schmidt). Nobody’s name is attached; A =UΣVT is known as the “SVD” or the singular value decomposition. We want to describe it, to prove it, and to discuss its applications— which are many and growing. TheSVDiscloselyassociatedwiththeeigenvalue-eigenvectorfactorizationQΛQT of apositivedefinitematrix. TheeigenvaluesareinthediagonalmatrixΛ. Theeigenvector matrix Q is orthogonal (QTQ = I) because eigenvectors of a symmetric matrix can be chosentobeorthonormal. Formostmatricesthatisnottrue,andforrectangularmatrices it is ridiculous (eigenvalues undefined). But now we allow the Q on the left and the QT on the right to be any two orthogonal matricesU andVT—not necessarily transposes of each other. Then every matrix will split into A=UΣVT. Thediagonal(butrectangular)matrixΣhaseigenvaluesfromATA,notfromA! Those positiveentries(alsocalledsigma)willbeσ ,...,σ. TheyarethesingularvaluesofA. 1 r They fill the first r places on the main diagonal of Σ—when A has rank r. The rest of Σ is zero. With rectangular matrices, the key is almost always to consider ATA and AAT. Singular Value Decomposition: Any m by n matrix A can be factored into A=UΣVT =(orthogonal)(diagonal)(orthogonal). The columns ofU (m by m) are eigenvectors of AAT, and the columns ofV (n by n) are eigenvectors of ATA. The r singular values on the diagonal of Σ (m by n) are the square roots of the nonzero eigenvalues of both AAT and ATA. Remark 1. For positive definite matrices, Σ is Λ and UΣVT is identical to QΛQT. For other symmetric matrices, any negative eigenvalues in Λ become positive in Σ. For complex matrices, Σ remains real butU andV become unitary (the complex version of orthogonal). We take complex conjugates inUHU =I andVHV =I and A=UΣVH. Remark 2. U andV give orthonormal bases for all four fundamental subspaces: first r columns ofU: column space of A last m−r columns ofU: left nullspace of A first r columns ofV: row space of A last n−r columns ofV: nullspace of A Remark 3. The SVD chooses those bases in an extremely special way. They are more than just orthonormal. When A multiplies a column v of V, it produces σ times a j j column ofU. That comes directly from AV =UΣ, looked at a column at a time. Remark 4. Eigenvectors of AAT and ATA must go into the columns ofU andV: AAT =(UΣVT)(VΣTUT)=UΣΣTUT and, similarly, ATA=VΣTΣVT. (1) U mustbetheeigenvectormatrixfor AAT. TheeigenvaluematrixinthemiddleisΣΣT— which is m by m withσ2,...,σ2 on the diagonal. 1 r From the ATA=VΣTΣVT, theV matrix must be the eigenvector matrix for ATA. The diagonal matrix ΣTΣ has the sameσ2,...,σ2, but it is n by n. 1 r Remark 5. Here is the reason that Av =σ u . Start with ATAv =σ2v : j j j j j j Multiply by A AATAv =σ2Av (2) j j j This says that Av is an eigenvector of AAT! We just moved parentheses to (AAT)(Av ). j j The length of this eigenvector Av isσ , because j j vTATAv =σ2vTv gives (cid:107)Av (cid:107)2 =σ2. j j j j j j So the unit eigenvector is Av /σ =u . In other words, AV =UΣ. j j j Example 1. This A has only one column: rank r =1. Then Σ has onlyσ =3: 1      −1 −1 2 2 3 (cid:104) (cid:105) 3 3 3      SVD A= 2 = 2 −1 2 0 1 =U Σ VT . 3 3 3 3×3 3×1 1×1 2 2 2 −1 0 3 3 3 ATA is 1 by 1, whereas AAT is 3 by 3. They both have eigenvalue 9 (whose square root is the 3 in Σ). The two zero eigenvalues of AAT leave some freedom for the eigenvectors in columns 2 and 3 ofU. We kept that matrix orthogonal. (cid:34) (cid:35) 2 −1 Example 2. Now A has rank 2, and AAT = withλ=3 and 1: −1 2   √ (cid:34) (cid:35) (cid:34) (cid:35)(cid:34)√ (cid:35) 1 −2 1 / 6 −1 1 0 1 −1 1 3 0 0   √ =UΣVT = √ −1 0 1 / 2 . 0 −1 1 2 1 1 0 1 0 √ 1 1 1 / 3 √ √ Notice 3 and 1. The columns of U are left singular vectors (unit eigenvectors of AAT). The columns ofV are right singular vectors (unit eigenvectors of ATA). Application of the SVD Wewillpickafewimportantapplications, afteremphasizingonekeypoint. TheSVDis terrific for numerically stable computations. becauseU andV are orthogonal matrices. Theyneverchangethelengthofavector. Since(cid:107)Ux(cid:107)2=xTUTUx=(cid:107)x(cid:107)2,multiplication byU cannot destroy the scaling."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.3 SingularValueDecomposition 369 Of course Σ could multiply by a large σ or (more commonly) divide by a small σ, and overflow the computer. But still Σ is as good as possible. It reveals exactly what is large and what is small. The ratioσ /σ is the condition number of an invertible n max min by n matrix. The availability of that information is another reason for the popularity of the SVD. We come back to this in the second application."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. Image processing Suppose a satellite takes a picture, and wants to send it to Earth. The picture may contain 1000 by 1000 “pixels”—a million little squares, each with a definite color. We can code the colors, and send back 1,000,000 numbers. It is better to find the essential information inside the 1000 by 1000 matrix, and send only that. Suppose we know the SVD. The key is in the singular values (in Σ). Typically, some σ’s are significant and others are extremely small. If we keep 20 and throw away 980, then we send only the corresponding 20 columns of U and V. The other 980 columns are multiplied in UΣVT by the small σ’s that are being ignored. We can do the matrix multiplication as columns times rows: A=UΣVT =u σ vT+u σ vT+···+u σvT. (3) 1 1 1 2 2 2 r r r Any matrix is the sum of r matrices of rank 1. If only 20 terms are kept, we send 20 times 2000 numbers instead of a million (25 to 1 compression). The pictures are really striking, as more and more singular values are included. At first you see nothing, and suddenly you recognize everything. The cost is in computing the SVD—this has become much more efficient, but it is expensive for a big matrix."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. The effective rank The rank of a matrix is the number of independent rows, and the number of independent columns. That can be hard to decide in computations! In exact arithmetic, counting the pivots is correct. Real arithmetic can be misleading—but discarding small pivots is not the answer. Consider the following: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) ε 2ε ε 1 ε 1 εis small and and . 1 2 0 0 ε 1+ε The first has rank 1, although roundoff error will probably produce a second pivot. Both pivots will be small; how many do we ignore? The second has one small pivot, but we cannotpretend thatits rowisinsignificant. The thirdhas twopivotsand its rankis 2, but its “effective rank” ought to be 1. We go to a more stable measure of rank. The first step is to use ATA or AAT, which are symmetric but share the same rank as A. Their eigenvalues—the singular values squared—are not misleading. Based on the accuracy of the data, we decide on a toler- ance like 10−6 and count the singular values above it—that is the effective rank. The examples above have effective rank 1 (whenεis very small)."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. Polar decomposition Every nonzero complex number z is a positive number r times a number eiθ on the unit circle: z = reiθ. That expresses z in “polar coordinates.” If we think of z as a 1 by 1 matrix, r corresponds to a positive definite matrix and eiθ corresponds to an orthogonal matrix. More exactly, since eiθ is complex and satisfies e−iθeiθ =1, it forms a 1 by 1 unitary matrix: UHU =I. We take the complex conjugate as well as the transpose, forUH. The SVD extends this “polar factorization” to matrices of any size: Every real square matrix can be factored into A =QS, where Q is orthogonal and S is symmetric positive semidefinite. If A is invertible then S is positive definite. For proof we just insertVTV =I into the middle of the SVD: A=UΣVT =(UVT)(VΣVT). (4) ThefactorS=VΣVT issymmetricandsemidefinite(becauseΣis). ThefactorQ=UVT isanorthogonalmatrix(becauseQTQ=VUTUVT=I). Inthecomplexcase,Sbecomes Hermitian instead of symmetric and Q becomes unitary instead of orthogonal. In the invertible case Σ is definite and so is S. Example 3. Polar decomposition: (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) 1 −2 0 −1 3 −1 A=QS = . 3 −1 1 0 −1 2 Example 4. Reverse polar decomposition: (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) 1 −2 2 1 0 −1 A=S(cid:48)Q = . 3 −1 1 3 1 0 The exercises show how, in the reverse order. S changes but Q remains the same. Both S and S(cid:48) are symmetric positive definite because this A is invertible. Application of A = QS: A major use of the polar decomposition is in continuum mechanics (and recently in robotics). In any deformation, it is important to separate stretching from rotation, and that is exactly what QS achieves. The orthogonal matrix Q is a rotation, and possibly a reflection. The material feels no strain. The symmetric matrix S has eigenvalues σ ,...,σ, which are the stretching factors (or compression 1 r factors). The diagonalization that displays those eigenvalues is the natural choice of axes—called principal axes: as in the ellipses of Section 6.2. It is S that requires work on the material, and stores up elastic energy. We note that S2 is ATA, which is symmetric positive definite when A is invertible. S is the symmetric positive definite square root of ATA, and Q is AS−1. In fact, A could be rectangular, as long as ATA is positive definite. (That is the condition we keep meeting,"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.3 SingularValueDecomposition 371 that A must have independent columns.) In the reverse order A = S(cid:48)Q, the matrix S(cid:48) is the symmetric positive definite square root of AAT."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "4. Least Squares For a rectangular system Ax=b. the least-squares solution comes from the normal equations ATAx(cid:98)= ATb. If A has dependent columns then ATA is not invertible and x(cid:98)is not determined. Any vector in the nullspace could be added to x(cid:98). We can now complete Chapter 3, by choosing a “best” (shortest) x(cid:98)for every Ax=b. Ax = b has two possible difficulties: Dependent rows or dependent columns. With dependent rows, Ax = b may have no solution. That happens when b is outside the column space of A. Instead of Ax = b. we solve ATAx(cid:98)= ATb. But if A has dependent columns, this x(cid:98)will not be unique. We have to choose a particular solution of ATAx(cid:98)= ATb, and we choose the shortest. The optimal solution of Ax=b is the minimum length solution of ATAx(cid:98)=ATb. That minimum length solution will be called x+. It is our preferred choice as the best solution to Ax = b (which had no solution), and also to ATAx(cid:98)= ATb (which had too many). We start with a diagonal example. Example 5. A is diagonal, with dependent rows and dependent columns:       x(cid:98) 1 σ 1 0 0 0   b 1  x(cid:98)    2 Ax(cid:98)= p is  0 σ 0 0 =b . 2 2 x(cid:98)  3 0 0 0 0 0 x(cid:98) 4 Thecolumnsallendwithzero. Inthecolumnspace,theclosestvectortob=(b ,b ,b ) 1 2 3 is p = (b ,b ,0). The best we can do with Ax = b is to solve the first two equations, 1 2 sincethethirdequationis0=b . Thaterrorcannotbereduced, buttheerrorsinthefirst 3 two equations will be zero. Then x(cid:98) =b /σ and x(cid:98) =b /σ . 1 1 1 2 2 2 Now we face the second difficulty. To make x(cid:98)as short as possible, we choose the totally arbitrary x(cid:98) and x(cid:98) to be zero. The minimum length solution is x+: 3 4       b /σ 1/σ 0 0 1 1 1 A+ is pseudoinverse  b /σ     0 1/σ 0 b 1  x+ = 2 2 = 2 b . (5) x+ =A+b is shortest  0   0 0 0 2 b 3 0 0 0 0 This equation finds x+, and it also displays the matrix that produces x+ from b. That matrix is the pseudoinverse A+ of our diagonal A. Based on this example, we know Σ+ and x+ for any diagonal matrix Σ:       σ 1/σ b /σ 1 1 1 1   ...     ...     . . .   Σ=  Σ+ =  Σ+b= .  σ   1/σ  b /σ  r r r r The matrix Σ is m by n, with r nonzero entries σ. Its pseudoinverse Σ+ is n by m, with i r nonzero entries 1/σ. All the blank spaces are zeros. Notice that (Σ+)+ is Σ again. i That is like (A−1)−1 =A, but here A is not invertible. Now we find x+ in the general case. We claim that the shortest solution x+ is always in the row space of A. Remember that any vector x(cid:98)can be split into a row space compo- nent x and a nullspace component: x(cid:98)=x +x . There are three important points about r r n that splitting:"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. The row space component also solves ATAx(cid:98) =ATb, because Ax =0. r n"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. The components are orthogonal, and they obey Pythagoras’s law: (cid:107)x(cid:98)(cid:107)2 =(cid:107)x (cid:107)2+(cid:107)x (cid:107)2, so x(cid:98)is shortest when x =0. r n n"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. All solutions of ATAx(cid:98)=ATb have the same x . That vector is x+. r The fundamental theorem of linear algebra was in Figure 3.4. Every p in the column space comes from one and only one vector x in the row space. All we are doing is to r choose that vector, x+ =x , as the best solution to Ax=b. r ThepseudoinverseinFigure6.3startswithbandcomesbacktox+. ItinvertsAwhere A is invertible—between row space and column space. The pseudoinverse knocks out theleftnullspacebysendingit tozero, anditknocksout thenullspacebychoosing x as r x+. We have not yet shown that there is a matrix A+ that always gives x+—but there is. It will be n by m, because it takes b and p in Rm back to x+ in Rn. We look at one more example before finding A+ in general. Example 6. Ax=b is −x +2x +2x =18, with a whole plane of solutions. 1 2 3 According to our theory, the shortest solution should be in the row space of A = [−1 2 2]. The multiple of that row that satisfies the equation is x+ = (−2,4,4). There are longer solutions like (−2,5,3), (−2,7,1), or (−6,3,3), but they all have nonzero components from the nullspace. The matrix that produces x+ from b=[18] is the pseu- doinverse A+. Whereas A was 1 by 3, this A+ is 3 by 1:     −1 −2 (cid:104) (cid:105) +  9    A+ = −1 2 2 = 2  and A+[18]= 4 . (6) 9 2 4 9"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. Compute ATA and its eigenvaluesσ2, 0 and unit eigenvectors v , v : 1 1 2 (cid:34) (cid:35) 1 4 A= . 2 8"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. (a) Compute AAT and its eigenvaluesσ2, 0 and unit eigenvectors u , u . 1 1 2 (b) Choose signs so that Av =σ u and verify the SVD: 1 1 1 (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) 1 4 σ T 1 = u u v v . 1 2 1 2 2 8 0 (c) Which four vectors give orthonormal bases for C(A), N(A), C(AT), N(AT)? Problems 3–5 ask for the SVD of matrices of rank 2."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. Find the SVD from the eigenvectors v , v of ATA and Av =σu : 1 2 i i i (cid:34) (cid:35) 1 1 Fibonacci matrix A= . 1 0"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "4. Use the SVD part of the MATLAB demo eigshow (or Java on the course page web.mit.edu/18.06) to find the same vectors v and v graphically. 1 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "5. Compute ATA and AAT, and their eigenvalues and unit eigenvectors, for (cid:34) (cid:35) 1 1 0 A= . 0 1 1 Multiply the three matricesUΣVT to recover A. Problems 6–13 bring out the underlying ideas of the SVD."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6. Suppose u ,...,u and v ,...,v are orthonormal bases for Rn. Construct the matrix 1 n 1 n A that transforms each v into u to give Av =u ,...,Av =u . j j 1 1 n n"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "7. Construct the matrix with rank 1 that has Av = 12u for v = 1(1,1,1,1) and u = 2 1(2,2,1). Its only singular value isσ = . 3 1"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "8. FindUΣVT if A has orthogonal columns w ,...,w of lengthsσ ,...,σ . 1 n 1 n"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "9. Explain howUΣVT expresses A as a sum of r rank-1 matrices in equation (3): A=σ u vT+···+σu vT. 1 1 1 r r r"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.3 SingularValueDecomposition 375"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "10. Suppose A is a 2 by 2 symmetric matrix with unit eigenvectors u and u . If its 1 2 eigenvalues areλ =3 andλ =−2, what areU, Σ, andVT? 1 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "11. SupposeAisinvertible(withσ >σ >0). ChangeAbyassmallamatrixaspossible 1 2 to produce a singular matrix A . Hint: U andV do not change: 0 (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) σ T 1 Find A from A= u u v v . 0 1 2 1 2 σ 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "12. (a) If A changes to 4A, what is the change in the SVD? (b) What is the SVD for AT and for A−1?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "13. Why doesn’t the SVD for A+I just use Σ+I?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "14. Find the SVD and the pseudoinverse 0+ of the m by n zero matrix."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "15. Find the SVD and the pseudoinverseVΣ+UT of (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) 0 1 0 1 1 A= 1 1 1 1 , B= , and C = . 1 0 0 0 0"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "16. If an m by n matrix Q has orthonormal columns, what is Q+?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "17. Diagonalize ATA to find its positive definite square root S =VΣ1/2VT and its polar decomposition A=QS: (cid:34) (cid:35) 1 10 6 A= √ . 10 0 8"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "18. What is the minimum-length least-squares solution x+ =A+b to the following?      1 0 0 C 0      Ax=1 0 0D=2. 1 1 1 E 2 You can compute A+, or find the general solution to ATAx(cid:98)= ATb and choose the solution that is in the row space of A. This problem fits the best planeC+Dt+Ez to b=0 and also b=2 att =z=0 (and b=2 att =z=1). (a) If A has independent columns, its left-inverse (ATA)−1AT is A+. (b) If A has independent rows, its right-inverse AT(AAT)−1 is A+. In both cases, verify that x+ =A+b is in the row space. and ATAx+ =ATb."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "19. Split A=UΣVT into its reverse polar decomposition QS(cid:48)."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "20. Is (AB)+ =B+A+ always true for pseudoinverses? I believe not."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "21. RemovingzerorowsofU leavesA=LU, wherether columnsorL spanthecolumn space of A and the r rows ofU span the row space. Then A+ has the explicit formula UT(U UT)−1(LTL)−1LT. Why is A+b in the row space with UT at the front? Why does ATAA+b = ATb, so that x+ =A+b satisfies the normal equation as it should?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "22. ExplainwhyAA+ andA+Aareprojectionmatrices(andthereforesymmetric). What fundamental subspaces do they project onto?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.4 Minimum Principles Inthissectionweescapeforthefirsttimefromlinearequations. Theunknownxwillnot be given as the solution to Ax = b or Ax =λx. Instead, the vector x will be determined by a minimum principle. Itisastonishinghowmanynaturallawscanbeexpressedasminimumprinciples. Just the fact that heavy liquids sink to the bottom is a consequence of minimizing their po- tentialenergy. Andwhenyousitonachairorlieonabed, thespringsadjustthemselves so that the energy is minimized. A straw in a glass of water looks bent because light reaches your eye as quickly as possible. Certainly there are more highbrow examples: Thefundamentalprincipleofstructuralengineeringistheminimizationoftotalenergy.1 We have to say immediately that these “energies” are nothing but positive definite quadratic functions. And the derivative of a quadratic is linear. We get back to the familiar linear equations, when we set the first derivatives to zero. Our first goal in this section is to find the minimum principle that is equivalent to Ax = b, and the minimization equivalent to Ax = λx. We will be doing in finite dimensions exactly what the theory of optimization does in a continuous problem, where “first derivatives = 0” gives a differential equation. In every problem, we are free to solve the linear equation or minimize the quadratic. Thefirststepisstraightforward: Wewanttofindthe“parabola”P(x)whoseminimum occurs when Ax=b. If A is just a scalar, that is easy to do: 1 dP The graph of P(x)= Ax2−bx has zero slope when =Ax−b=0. 2 dx This point x = A−1b will be a minimum if A is positive. Then the parabola P(x) opens upward (Figure 6.4). In more dimensions this parabola turns into a parabolic bowl (a paraboloid). To assure a minimum of P(x), not a maximum or a saddle point, A must be positive definite! 1Iamconvincedthatplantsandpeoplealsodevelopinaccordancewithminimumprinciples.Perhapscivilization is based on a law of least action. There must be new laws (and minimum principles) to be found in the social sciencesandlifesciences."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.4 MinimumPrinciples 377 6H If A is symmetric positive definite, then P(x) = 1xTAx−xTb reaches its 2 minimum at the point where Ax=b. At that point P =−1bTA−1b. min 2 Figure6.4: ThegraphofapositivequadraticP(x)isaparabolicbowl. Proof. Suppose Ax=b. For any vector y, we show that P(y)≥P(x): 1 1 P(y)−P(x)= yTAy−yTb− xTAx+xTb 2 2 1 1 = yTAy−yTAx+ xTAx (set b=Ax) (1) 2 2 1 = (y−x)TA(y−x). 2 This can’t be negative since A is positive definite—and it is zero only if y−x=0. At all other points P(y) is larger than P(x), so the minimum occurs at x. Example 1. Minimize P(x) = x2−x x +x2−b x −b x . The usual approach, by 1 1 2 2 1 1 2 2 calculus, is to set the partial derivatives to zero. This gives Ax=b: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) ∂P/∂x =2x −x −b =0 2 −1 x b 1 1 2 1 1 1 means = . (2) ∂P/∂x =−x +2x −b =0 −1 2 x b 2 1 2 2 2 2 Linearalgebrarecognizes this P(x)as 1xTAx−xTb, andknowsimmediately that Ax=b 2 gives the minimum. Substitute x=A−1b into P(x): 1 1 Minimum value P = (A−1b)TA(A−1b)−(A−1b)Tb=− bTA−1b. (3) min 2 2 In applications, 1xTAx is the internal energy and −xTb is the external work. The system 2 automatically goes to x=A−1b, where the total energy P(x) is a minimum. Minimizing with Constraints Many applications add extra equations Cx = d on top of the minimization problem. These equations are constraints. We minimize P(x) subject to the extra requirement Cx = d. Usually x can’t satisfy n equations Ax = b and also (cid:96) extra constraintsCx = d. We have too many equations and we need (cid:96) more unknowns. Those new unknowns y ,...,y are called Lagrange multipliers. They build the 1 (cid:96) constraint into a function L(x,y). This was the brilliant insight of Lagrange: 1 L(x,y)=P(x)+yT(Cx−d)= xTAx−xTb+xTCTy−yTd. 2 That term in L is chosen exactly so that ∂L/∂y = 0 brings back Cx = d. When we set the derivatives of L to zero, we have n+(cid:96) equations for n+(cid:96) unknowns x and y: Constrained ∂L/∂x=0: Ax+CTy=b (4) minimization ∂L/∂y=0: Cx =d The first equations involve the mysterious unknowns y. You might well ask what they represent. Those “dual unknowns” y tell how much the constrained minimum P C/min (which only allows x whenCx=d) exceeds the unconstrained P (allowing all x): min 1 Sensitivity of minimum P =P + yT(CA−1b−d)≥P . (5) C/min min min 2 Example 2. Suppose P(x ,x ) = 1x2+ 1x2. Its smallest value is certainly P = 0. 1 2 2 1 2 2 min This unconstrained problem has n = 2, A = I, and b = 0. So the minimizing equation Ax=b just gives x =0 and x =0. 1 2 Now add one constraint c x +c x = d. This puts x on a line in the x -x plane. 1 1 2 2 1 2 The old minimizer x = x = 0 is not on the line. The Lagrangian function L(x,y) = 1 2 1x2+1x2+y(c x +c x −d) has n+(cid:96)=2+1 partial derivatives: 2 1 2 2 1 1 2 2 ∂L/∂x =0 x +c y=0 1 1 1 ∂L/∂x =0 x +c y=0 (6) 2 2 2 ∂L/∂y=0 c x +c x =d. 1 1 2 2 Substituting x =−c y and x =−c y into the third equation gives −c2y−c2y=d. 1 1 2 2 1 2 −d c d c d 1 2 Solution y= x = x = . (7) 1 2 c2+c2 c2+c2 c2+c2 1 2 1 2 1 2 The constrained minimum of P= 1xTx is reached at that solution point: 2 1 1 1c2d2+c2d2 1 d2 P = x2+ x2 = 1 2 = . (8) C/min 2 1 2 2 2 (c2+c2)2 2c2+c2 1 2 1 2 This equals −1yd as predicted in equation (5), since b=0 and P =0. 2 min Figure 6.5 shows what problem the linear algebra has solved, if the constraint keeps x on a line 2x −x =5. We are looking for the closest point to (0,0) on this line. The 1 2 solution is x = (2,−1). We expect this shortest vector x to be perpendicular to the line, and we are right."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.4 MinimumPrinciples 379 Figure6.5: Minimizing 1(cid:107)x(cid:107)2forallxontheconstraintline2x −x =5. 2 1 2 Least Squares Again In minimization, our big application is least squares. The best x(cid:98)is the vector that mini- mizes the squared error E2 =(cid:107)Ax−b(cid:107)2. This is a quadratic and it fits our framework! I will highlight the parts that look new: Squared error E2 =(Ax−b)T(Ax−b)=xT AT Ax−2xT AT b+bT b. (9) Compare with 1xTAx−xTb at the start of the section, which led to Ax=b: 2 (cid:163) (cid:164) (cid:163) (cid:164) (cid:163) (cid:164) A changes to ATA b changes to ATb bTb is added . The constant bTb raises the whole graph—this has no effect on the best x(cid:98). The other two changes, A to ATA and b to ATb, give a new way to reach the least-squares equation (normal equation). The minimizing equation Ax=b changes into the Least-squares equation ATAx(cid:98)=ATb. (10) Optimization needs a whole book. We stop while it is pure linear algebra. The Rayleigh quotient Our second goal is to find a minimization problem equivalent to Ax=λx. That is not so easy. Thefunctiontominimizecannotbeaquadratic,oritsderivativewouldbelinear— and the eigenvalue problem is nonlinear (λ times x). The trick that succeeds is to divide one quadratic by another one: xTAx Rayleigh quotient Minimize R(x)= . xTx 6I Rayleigh’s Principle: The minimum value of the Rayleigh quotient is the smallest eigenvalueλ . R(x) reaches that minimum at the first eigenvector 1 x of A: 1 xTAx xTλ x Minimum where Ax =λx R(x )= 1 1 = 1 1 1 =λ . 1 1 1 1 xTx xTx 1 1 1 1 If we keep xTAx = 1, then R(x) is a minimum when xTx = (cid:107)x(cid:107)2 is as large as possible. We are looking for the point on the ellipsoid xTAx = 1 farthest from the origin—the vector x of greatest length. From our earlier description of the ellipsoid, its longest axis points along the first eigenvector. So R(x) is a minimum at x . 1 Algebraically,wecandiagonalizethesymmetricAbyanorthogonalmatrix: QTAQ= Λ. Then set x=Qy and the quotient becomes simple: (Qy)TA(Qy) yTΛy λ y2+···+λ y2 R(x)= = = 1 1 n n. (11) (Qy)T(Qy) yTy y2+···+y2 1 n The minimum of R isλ , at the point where y =1 and y =···=y =0: 1 1 2 n At all points λ (y2+y2+···+y2)≤(λ y2+λ y2+···+λ y2). 1 1 2 n 1 1 2 2 n n TheRayleighquotientinequation(11)isneverbelowλ andneveraboveλ (thelargest 1 n eigenvalue). Its minimum is at the eigenvector x and its maximum is at x : 1 n xTAx xTλ x Maximum where Ax =λ x R(x )= n n = n n n =λ . n n n n n xTx xTx n n n n One small yet important point: The Rayleigh quotient equals a , when the trial vector 11 is x=(1,0,...,0). So a (on the main diagonal) is betweenλ andλ . You can see this 11 1 n in Figure 6.6, where the horizontal distance to the ellipse (where a x2 = 1) is between 11 the shortest distance and the longest distance: 1 1 1 √ ≤ √ ≤ √ which is λ ≤a ≤λ . 1 11 n λ a λ n 11 1 The diagonal entries of any symmetric matrix are betweenλ andλ . We drew Figure 1 n"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.6 for a 2 by 2 positive definite matrix to see it clearly. Intertwining of the Eigenvalues The intermediate eigenvectors x ,...,x are saddle points of the Rayleigh quotient 2 n−1 (zero derivatives, but not minima or maxima). The difficulty with saddle points is that we have no idea whether R(x) is above or below them. That makes the intermediate eigenvaluesλ ,...,λ harder to estimate. 2 n−1 For this optional topic, the key is to find a constrained minimum or maximum. The constraints come from the basic property of symmetric matrices: x is perpendicular to j the other eigenvectors. 6J The minimum of R(x) subject to xTx = 0 is λ . The minimum of R(x) 1 2 subject to any other constraint xTv=0 is not aboveλ : 2 λ = min R(x) and λ ≥ min R(x). (12) 2 2 xTx =0 xTv=0 1"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. Consider the system Ax=b given by      2 −1 0 x 4 1      −1 2 −1x =0. 2 0 −1 2 x 4 3 Construct the corresponding quadratic P(x ,x ,x ), compute its partial derivatives 1 2 3 ∂P/∂x , and verify that they vanish exactly at the desired solution. i"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. Complete the square in P = 1xTAx−xTb = 1(x−A−1b)TA(x−A−1b)+constant. 2 2 This constant equals P because the term before it is never negative. (Why?) min"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. Findtheminimum,ifthereisoneofP = 1x2+xy+y2−3yandP = 1x2−3y. What 1 2 2 2 matrix A is associated with P ? 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "4. (Review) Another quadratic that certainly has its minimum at Ax=b is 1 1 1 Q(x)= (cid:107)Ax−b(cid:107)2 = xTATAx−xTATb+ bTb. 2 2 2 Comparing Q with P, and ignoring the constant 1bTb, what system of equations do 2 we get at the minimum of Q? What are these equations called in the theory of least squares?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.4 MinimumPrinciples 383"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "5. For any symmetric matrix A, compute the ratio R(x) for the special choice x = (1,...,1). How is the sum of all entries a related toλ andλ ? ij 1 n (cid:163) (cid:164)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6. With A= 2 −1 , find a choice of x that gives a smaller R(x) than the boundλ ≤2 −1 2 1 that comes from the diagonal entries. What is the minimum value of R(x)?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "7. IfBispositivedefinite,showfromtheRayleighquotientthatthesmallesteigenvalue of A+B is larger than the smallest eigenvalue of A."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "8. If λ and µ are the smallest eigenvalues of A and B, show that the smallest eigen- 1 1 value θ of A+B is at least as large as λ +µ . (Try the corresponding eigenvector 1 1 1 x in the Rayleigh quotients.) Note. Problems 7 and 8 are perhaps the most typical and most important results that come easily from Rayleigh’s principle, but only with great difficulty from the eigenvalue equations themselves."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "9. IfBispositivedefinite, showfromtheminimaxprinciple(12)thatthesecond small- est eigenvalue is increased by adding B:λ (A+B)>λ (A). 2 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "10. If you throw away two rows and columns of A, what inequalities do you expect between the smallest eigenvalue µof the new matrix and the originalλ’s?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "11. Find the minimum values of x2−x x +x2 x2−x x +x2 R(x)= 1 1 2 2 and R(x)= 1 1 2 2. x2+x2 2x2+x2 1 2 1 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "12. Prove from equation (11) that R(x) is never larger than the largest eigenvalueλ . n"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "13. The minimax principle forλ involves j-dimensional subspaces S : j j (cid:183) (cid:184) Equivalent to equation (15) λ =min maxR(x) . j S j xinS j (a) Ifλ is positive, infer that every S contains a vector x with R(x)>0. j j (b) Deduce that S contains a vector y=C−1x with yTcTACy/yTy>0. j (c) Conclude that the jth eigenvalue of CTAC, from its minimax principle, is also positive—proving again the law of inertia in Section 6.2."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "14. Showthatthesmallesteigenvalueλ ofAx=λMxisnotlargerthantheratioa /m 1 11 11 of the corner entries."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "15. Which particular subspace S in Problem 13 gives the minimum value λ ? In other 2 2 words, over which S is the maximum of R(x) equal toλ ? 2 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "16. (Recommended) From the zero submatrix decide the signs of the n eigenvalues:   0 · 0 1   · · 0 2 A= . 0 0 0 · 1 2 · n"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "17. (Constrained minimum) Suppose the unconstrained minimum x = A−1b happens to satisfy the constraintCx=d. Verify that equation (5) correctly gives P =P ; C/min min the correction term is zero."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.5 The Finite Element Method There were two main ideas in the preceding section on minimum principles: (i) Solving Ax=b is equivalent to minimizing P(x)= 1xTAx−xTb. 2 (ii) Solving Ax=λ x is equivalent to minimizing R(x)=xTAx/xTx. 1 Now we try to explain how these ideas can be applied. The story is a long one, because these principles have been known for more than a century. Inengineeringproblemslikeplatebending,orphysicsproblemsliketheground state (eigenfunction) of an atom, minimization was used to get a rough approximation to the true solution. The approximations had to be rough; the computers were human. The principles (i) and (ii) were there, but they could not be implemented. Obviously the computer was going to bring about a revolution. It was the method of finite differences that jumped ahead, because it is easy to “discretize” a differential equation. AlreadyinSection1.7, derivativeswerereplacedbydifferences. Thephysical region is covered by a mesh, and u(cid:48)(cid:48) = f(x) became u −2u +u = h2f . The j+1 j j−1 j 1950s brought new ways to solve systems Au= f that are very large and very sparse— algorithms and hardware are both much faster now. What we did not fully recognize was that even finite differences become incredibly complicated for real engineering problems, like the stresses on an airplane. The real difficulty is not to solve the equations, but to set them up. For an irregular region we piece the mesh together from triangles or quadrilaterals or tetrahedra. Then we need a systematic way to approximate the underlying physical laws. The computer has to help not only in the solution of Au= f and Ax=λx, but in its formulation. You can guess what happened. The old methods came back, with a new idea and a new name. The new name is the finite element method. The new ides uses more of the power of the computer—in constructing a discrete approximation, solving it, and displaying the results—than any other technique in scientific computation2. If the basic 2Pleaseforgivethisenthusiasm: Iknowthemethodmaynotbeimmortal."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.5 TheFiniteElementMethod 385 idea is simple, the applications can be complicated. For problems on this scale, the one undebatable point is their cost—I am afraid a billion dollars would be a conservative estimate of the expense so far. I hope some readers will be vigorous enough to master the finite element method and put it to good use. Trial Functions StartingfromtheclassicalRayleigh-Ritzprinciple,Iwillintroducethenewideaoffinite elements. The equation can be −u(cid:48)(cid:48) = f(x) with boundary conditions u(0) = u(1) ="
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "0. This problem is infinite-dimensional (the vector b is replaced by a function f, and the matrix A becomes −d2/dx2). We can write down the energy whose minimum is required, replacing inner products vTf by integrals of v(x)f(x): (cid:90) (cid:90) 1 1 1 1 Total energy P(v)= vTAv−vTf = v(x)(−v(cid:48)(cid:48)(x))dx− v(x)f(x)dx. (1) 2 2 0 0 P(v)istobeminimizedoverallfunctionsv(x)thatsatisfyv(0)=v(1)=0. Thefunction that gives the minimum will be the solution u(x). The differential equation has been converted to a minimum principle, and it only remains to integrate by parts: (cid:183) (cid:184) (cid:90) (cid:90) (cid:90) 1 1 1 1 v(−v(cid:48)(cid:48))dx= (v(cid:48))2dx−[vv(cid:48)]x=1 so P(v)= (v(cid:48)(x))2+v(x)f(x) dx. x=0 2 0 0 0 (cid:82) The term vv(cid:48) is zero at both limits, because v is. Now (v(cid:48)(x))2dx is positive like xTAx. We are guaranteed a minimum. Tocomputetheminimumexactlyisequivalenttosolvingthedifferentialequationex- actly. The Rayleigh-Ritz principle produces an n-dimensional problem by choosing only n trial functions V (x),...,V (x). From all combinations V = y V (x)+···+y V (x), 1 n 1 1 n n we look for the particular combination (call it U) that minimizes P(V). This is the key idea, to minimize over a subspace ofV’s instead of over all possible v(x). The function that gives the minimum isU(x). We hope and expect thatU(x) is near the correct u(x). SubstitutingV for v, the quadratic turns into (cid:90) (cid:90) 1 1(cid:161) (cid:162) 1(cid:161) (cid:162) P(V)= y V(cid:48)(x)+···+y V(cid:48)(x) 2 dx− y V (x)+···+y V (x) f(x)dx. (2) 2 1 1 n n 1 1 n n 0 0 The trial functions V are chosen in advance. That is the key step! The unknowns y ,...,y go into a vector y. Then P(V) = 1yTAy−yTb is recognized as one of the 1 n 2 (cid:82) quadratics we are accustomed to. The matrix entries A are V(cid:48)V(cid:48)dx = coefficient of ij i j (cid:82) y y . Thecomponentsb are V fdx. Wecancertainlyfindtheminimumof 1yTAy−yTb i j j j 2 by solving Ay=b. Therefore the Rayleigh-Ritz method has three steps:"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. Choose the trial functionsV ,...,V . 1 n"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. Compute the coefficients A and b . ij j"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. Solve Ay=b to findU(x)=y V (x)+···+y V (x). 1 1 n n Everything depends on step 1. Unless the functions V (x) are extremely simple, the j other steps will be virtually impossible. And unless some combination of theV is close j to the true solution u(x), those steps will be useless. To combine both computability and accuracy, the key idea that makes finite elements successful is the use of piecewise polynomials as the trial functionsV(x). Linear Finite Elements The simplest and most widely used finite element is piecewise linear. Place nodes at the interior points x = h,x = 2h,...,x = nh, just as for finite differences. Then V is 1 2 n j the “hat function” that equals 1 at the node x , and zero at all the other nodes (Figure j"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.7a). It is concentrated in a small interval around its node, and it is zero everywhere else(includingx=0andx=1). Anycombinationy V +···+y V musthavethevalue 1 1 n n y at node j (the otherV’s are zero there), so its graph is easy to draw (Figure 6.7b). j V4(x) V(x) = y1V1 +··· +y5V5 1 y4 y1 0 x4 = 4h 1 0 1 Figure6.7: Hatfunctionsandtheirlinearcombinations. (cid:82) Step2computesthecoefficientsA = V(cid:48)V(cid:48)dxinthe“stiffnessmatrix”A. Theslope ij i j V(cid:48) equals 1/h in the small interval to the left of x , and −1/h in the interval to the right. j j If these “double intervals” do not overlap, the product V(cid:48)V(cid:48) is zero and A = 0. Each i j ij hat function overlaps itself and only two neighbors: (cid:181) (cid:182) (cid:181) (cid:182) (cid:90) (cid:90) 2 (cid:90) 2 1 1 2 Diagonal i= j A = V(cid:48)V(cid:48)dx= dx+ − dx= . ii i i h h h (cid:181) (cid:182)(cid:181) (cid:182) (cid:90) (cid:90) 1 −1 −1 Off-diagonal i= j±1 A = V(cid:48)V(cid:48)dx= dx= . ij i j h h h Then the stiffness matrix is actually tridiagonal:   2 −1   −1 2 −1  1  Stiffness matrix A=  −1 2 −1 . h   −1 2 −1 −1 2"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. Use three hat functions, with h = 1, to solve −u(cid:48)(cid:48) = 2 with u(0) = u(1) = 0. Verify 4 that the approximationU matches u=x−x2 at the nodes."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. Solve −u(cid:48)(cid:48) = x with u(0) = u(1) = 0. Then solve approximately with two hat func- tions and h= 1. Where is the largest error? 3"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. Suppose −u(cid:48)(cid:48) =2, with the boundary condition u(1)=0 changed to u(cid:48)(1)=0. This “natural” condition on u(cid:48) need not be imposed on the trial functionsV. With h = 1, 3 there is an extra half-hat V , which goes from 0 to 1 between x = 2 and x = 1. (cid:82) 3 (cid:82) 3 Compute A = (V(cid:48))2dx and f = 2V dx. Solve Ay = f for the finite element 33 3 3 3 solution y V +y V +y V . 1 1 2 2 3 3"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "4. Solve−u(cid:48)(cid:48) =2withasinglehatfunction,butplaceitsnodeatx= 1 insteadofx= 1. 4 2 (Sketch this function V .) With boundary conditions u(0) = u(1) = 0, compare the 1 finite element approximation with the true u=x−x2."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "5. Galerkin’s method starts with the differential equation (say −u(cid:48)(cid:48) = f(x)) instead of the energy P. The trial solution is still u = y V +y V +···+y V , and the y’s are 1 1 2 2 n n chosen to make the difference between −u(cid:48)(cid:48) and f orthogonal to everyV : j (cid:90) (cid:90) Galerkin (−y V(cid:48)(cid:48)−y V(cid:48)(cid:48)−···−y V(cid:48)(cid:48))V dx= f(x)V (x)dx. 1 1 2 2 n n j j"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6.5 TheFiniteElementMethod 389 integrate the left side by parts to reach Ay= f, proving that Galerkin gives the same A and f as Rayleigh-Ritz for symmetric problems."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "6. A basic identity for quadratics shows y=A−1b as minimizing: 1 1 1 P(y)= yTAy−yTb= (y−A−1b)TA(y−A−1b)− bTA−1b. 2 2 2 The minimum over a subspace of trial functions is at the y nearest to A−1b. (That makes the first term on the right as small as possible; it is the key to convergence of U to u.) If A=I and b=(1,0,0), which multiple ofV =(1,1,1) gives the smallest value of P(y)= 1yTy−y ? 2 1 (cid:82)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "7. For a single hat function V(x) centered at x = 1, compute A = (V(cid:48))2dx and M = (cid:82) 2 V2dx. Inthe1by1eigenvalueproblem, isλ=A/M largerorsmallerthanthetrue eigenvalueλ=π2?"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "8. For the hat functionsV andV centered at x=h= 1 and x=2h= 2, compute the 2 1 (cid:82) 2 3 3 by 2 mass matrix M = VV dx, and solve the eigenvalue problem Ax=λMx. ij i j (cid:82)"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "9. What is the mass matrix M = VV dx for n hat functions with h= 1 ? ij i j n+1 7 Chapter Computations with Matrices"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "7.1 Introduction One aim of this book is to explain the useful parts of matrix theory. In comparison with older texts in abstract linear algebra, the underlying theory has not been radically changed. One of the best things about the subject is that the theory is really essential for the applications. What is different is the change in emphasis which comes with a new point of view. Elimination becomes more than just a way to find a basis for the row space, and the Gram-Schmidt process is not just a proof that every subspace has an orthonormal basis. Instead, we really need these algorithms. And we need a convenient description, A=LU or A=QR, of what they do. Thischapterwilltakeafewmorestepsinthesamedirection. Isupposethesestepsare governedbycomputationalnecessity,ratherthanbyelegance,andIdon’tknowwhether to apologize for that; it makes them sound very superficial, and that is wrong. They deal with the oldest and most fundamental problems of the subject, Ax=b and Ax=λx, but they are continually changing and improving. In numerical analysis there is a survival of the fittest, and we want to describe some ideas that have survived so far. They fall into three groups:"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "1. Techniques for Solving Ax = b. Elimination is a perfect algorithm, except when the particular problem has special properties—as almost every problem has. Sec- tion 7.4 will concentrate on the property of sparseness, when most of the entries in A are zero. We develop iterative rather than direct methods for solving Ax = b. An iter- ative method is “self-correcting,” and never reaches the exact answer. The object is to get close more quickly than elimination. In some problems, that can be done; in many others, elimination is safer and faster if it takes advantage of the zeros. The competition is far from over, and we will identify the spectral radius that controls the speed of con- vergence to x=A−1b."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "2. Techniques for Solving Ax =λx. The eigenvalue problem is one of the out-"
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "7.2 MatrixNormandConditionNumber 391 standingsuccessesofnumericalanalysis. Itisclearlydefined,itsimportanceisobvious, but until recently no one knew how to solve it. Dozens of algorithms have been sug- gested, and everything depends on the size and the properties of A (and on the number of eigenvalues that are wanted). You can ask LAPACK for an eigenvalue subroutine, withoutknowingitscontents, butitisbettertoknow. Wehavechosentwoorthreeideas that have superseded almost all of their predecessors: the QR algorithm, the family of “power methods,” and the preprocessing of a symmetric matrix to make it tridiagonal. The first two methods are iterative, and the last is direct. It does its job in a finite number of steps, but it does not end up with the eigenvalues themselves. This produces a much simpler matrix to use in the iterative steps."
    },
    {
        "chapter": "PositiveDefiniteMatrices",
        "question": "3. The Condition Number of a Matrix. Section 7.2 attempts to measure the “sensitivity” of a problem: If A and b are slightly changed, how great is the effect on x=A−1b? Before starting on that question, we need a way to measure A and the change ∆A. The length of a vector is already defined, and now we need the norm of a matrix. Then the condition number, and the sensitivity of A will follow from multiplying the norms of A and A−1. The matrices in this chapter are square."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.2 Matrix Norm and Condition Number An error and a blunder are very different things. An error is a small mistake, probably unavoidable even by a perfect mathematician or a perfect computer. A blunder is much more serious, and larger by at least an order of magnitude. When the computer rounds oft a number after 16 bits, that is an error, But when a problem is so excruciatingly sensitive that this roundoff error completely changes the solution, then almost certainly someone has committed a blunder. Our goal in this section is to analyze the effect of errors, so that blunders can be avoided. We are actually continuing a discussion that began in Chapter 1 with (cid:34) (cid:35) (cid:34) (cid:35) 1 1 0.0001 1 A= and B= . 1 1.0001 1 1 WeclaimedthatBiswell-conditioned,andnotparticularlysensitivetoroundoff—except that if Gaussian elimination is applied in a stupid way, the matrix becomes completely vulnerable. Itisablundertoaccept.0001asthefirstpivot,andwemustinsistonalarger and safer choice by exchanging the rows of B. When “partial pivoting” is built into the elimination algorithm, the computer automatically looks for the largest pivot. Then the natural resistance to roundoff error is no longer compromised. How do we measure this natural resistance, and decide whether a matrix is well- conditioned or ill-conditioned? If there is a small change in b or in A, how large a change does that produce in the solution x? We begin with a change in the right-hand side, from b to b+δb. This error might come from experimental data or from roundoff. We may suppose that δb is small, but its direction is outside our control. The solution is changed from x to x+δx: Error equation A(x+δx)=b+δb, so, by subtraction A(δx)=δb. (1) An error δb leads to δx = A−1δb. There will be a large change in the solution x when A−1 is large—A is nearly singular. The change in x is especially large whenδb points in the direction that is amplified most by A−1. Suppose A is symmetric and its eigenvalues are positive: 0 < λ ≤ ··· ≤ λ . Any 1 n vector δb is a combination of the corresponding unit eigenvectors x ,...,x . The worst 1 n errorδx, coming from A−1, is in the direction of the first eigenvector x : 1 δb Worst error If δb=εx , then δx= . (2) 1 λ 1 The error (cid:107)δb(cid:107) is amplified by 1/λ , which is the largest eigenvalue of A−1. This 1 amplification is greatest whenλ is near zero, and A is nearly singular. 1 Measuringsensitivityentirelybyλ hasaseriousdrawback. Supposewemultiplyall 1 the entries of A by 1000; then λ will be multiplied by 1000 and the matrix will look 1 much less singular. This offends our sense of fair play; such a simple rescaling cannot make an ill-conditioned matrix well. It is true thatδx will be 1000 times smaller, but so will the solution x = A−1b. The relative error (cid:107)δx(cid:107)/(cid:107)x(cid:107) will be the same. Dividing by (cid:107)x(cid:107) normalizes the problem against a trivial change of scale. At the same time there is a normalization for δb; our problem is to compare the relative change (cid:107)δb(cid:107)/(cid:107)b(cid:107) with the relative error (cid:107)δx(cid:107)/(cid:107)x(cid:107). The worst case is when (cid:107)δx(cid:107) is large—with δb in the direction of the eigenvector x —and when (cid:107)x(cid:107) is small. The true solution x should be as small as possible compared 1 tothetrueb. ThismeansthattheoriginalproblemAx=bshouldbeattheotherextreme, in the direction of the last eigenvector x : if b=x , then x=A−1b=b/λ . n n n It is this combination, b = x and δb =εx , that makes the relative error as large as n 1 possible. These are the extreme cases in the following inequalities: 7A For a positive definite matrix, the solution x = A−1b and the error δx = A−1δb always satisfy (cid:107)b(cid:107) (cid:107)δb(cid:107) (cid:107)δx(cid:107) λ (cid:107)δb(cid:107) max (cid:107)x(cid:107)≥ and (cid:107)δx(cid:107)≤ and ≤ . (3) λ λ (cid:107)x(cid:107) λ (cid:107)b(cid:107) max min min The ratio c =λ /λ is the condition number of a positive definite matrix max min A. Example 1. The eigenvalues of A are approximatelyλ =10−4/2 andλ =2: 1 2 (cid:34) (cid:35) 1 1 A= has condition number about c=4·104. 1 1.0001"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.2 MatrixNormandConditionNumber 393 We must expect a violent change in the solution from ordinary changes in the data. Chapter 1 compared the equations Ax=b and Ax(cid:48) =b(cid:48): u + v = 2 u + v = 2 u + 1.0001v = 2 u + 1.0001v = 2.0001. The right-hand sides are changed only by (cid:107)δb(cid:107) = .0001 = 10−4. At the same time, the solution goes from u=2, v=0 to u=v=1. This is a relative error of √ (cid:107)δx(cid:107) (cid:107)(−1,1)(cid:107) 2 (cid:107)δb(cid:107) = = , which equals 2·104 . (cid:107)x(cid:107) (cid:107)(2,0)(cid:107) 2 (cid:107)b(cid:107) Without having made any special choice of the perturbation, there was a relatively large change in the solution. Our x and δb make 45° angles with the worst cases, which accounts for the missing 2 between 2·104 and the extreme possibility c=4·104. If A=I or even if A=I/10, its condition number is c=λ /λ =1. By compari- max min son, the determinant is a terrible measure of ill-conditioning. It depends not only on the scaling but also on the order n; if A = I/10, then the determinant of A is 10−n. In fact, this “nearly singular” matrix is as well-conditioned as possible. Example 2. The n by n finite difference matrix A hasλ ≈4 andλ ≈π2/n2: max min   2 −1   −1 2 −1    A= −1 2 · .    · · −1 −1 2 Theconditionnumberisapproximatelyc(A)= 1n2,andthistimethedependenceonthe 2 order n is genuine. The better we approximate −u(cid:48)(cid:48) = f, by increasing the number of unknowns, the harder it is to compute the approximation. At a certain crossover point, an increase in n will actually produce a poorer answer. Fortunately for the engineer, this crossover occurs where the accuracy is already prettygood. Workinginsingleprecision,atypicalcomputermightmakeroundofferrors of order 10−9. With n = 100 unknowns and c = 5000, the error is amplified at most to be of order 10−5—which is still more accurate than any ordinary measurements. But there will be trouble with 10,000 unknowns, or with a 1, −4, 6, −4, 1 approximation to d4u/dx4 = f(x)—for which the condition number grows as n4.1 Unsymmetric Matrices Our analysis so far has applied to symmetric matrices with positive eigenvalues. We could easily drop the positivity assumption, and use absolute values |λ|. But to go 1The usual rule of thumb, experimentally verified, is that the computer can lose logc decimal places to the roundofferrorsinGaussianelimination. beyondsymmetry,aswecertainlywanttodo,therewillhavetobeamajorchange. This is easy to see for the very unsymmetric matrices (cid:34) (cid:35) (cid:34) (cid:35) 1 100 1 −100 A= and A−1 = . (4) 0 1 0 1 The eigenvalues all equal one, but the proper condition number is not λ /λ = 1. max min The relative change in x is not bounded by the relative change in b. Compare (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 0 100 100 100 x= when b= ; x(cid:48) = when b(cid:48) = . 1 1 0 0 A 1% change in b has produced a hundredfold change in x; the amplification factor is"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1002. Since c represents an upper bound, the condition number must be at least 10,000. The difficulty here is that a large off-diagonal entry in A means an equally large entry in A−1. Expecting A−1 to get smaller as A gets bigger is often wrong. For a proper definition of the condition number, we look back at equation (3). We weretryingtomakexsmallandb=Axlarge. WhenAisnotsymmetric,themaximumof (cid:107)Ax(cid:107)/(cid:107)x(cid:107) may be found at a vector x that is not one of the eigenvectors. This maximum is an excellent measure of the size of A. It is the norm of A. 7B The norm of A is the number (cid:107)A(cid:107) defined by (cid:107)Ax(cid:107) (cid:107)A(cid:107)=max . (5) x(cid:54)=0 (cid:107)x(cid:107) In other words, (cid:107)A(cid:107) bounds the “amplifying power” of the matrix: (cid:107)Ax(cid:107)≤(cid:107)A(cid:107)(cid:107)x(cid:107) for all vectors x. (6) ThematricesAandA−1 inequation(4)havenormssomewherebetween100and101. They can be calculated exactly, but first we want to complete the connection between norms and condition numbers. Because b=Ax andδx=A−1δb, equation (6) gives (cid:107)b(cid:107)≤(cid:107)A(cid:107)(cid:107)x(cid:107) and (cid:107)δx(cid:107)≤(cid:107)A−1(cid:107)(cid:107)δb(cid:107). (7) Thisisthereplacementforequation(3),whenAisnotsymmetric. Inthesymmetriccase, (cid:107)A(cid:107) is the same asλ , and (cid:107)A−1(cid:107) is the same as 1/λ . The correct replacement for max min λ /λ is the product (cid:107)A(cid:107)(cid:107)A−1(cid:107)—which is the condition number. max min 7C The condition number of A is c=(cid:107)A(cid:107)(cid:107)A−1(cid:107). The relative error satisfies (cid:107)δx(cid:107) (cid:107)δb(cid:107) δx fromδb ≤c directly from equation (7). (8) (cid:107)x(cid:107) (cid:107)b(cid:107) If we perturb the matrix A instead of the right-hand side b, then (cid:107)δx(cid:107) (cid:107)δA(cid:107) δx fromδA ≤c from equation (10) below. (9) (cid:107)x+δx(cid:107) (cid:107)A(cid:107)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1. For an orthogonal matrix Q, show that (cid:107)Q(cid:107) = 1 and also c(Q) = 1. Orthogonal matrices (and their multiplesαQ) are the only perfectly conditioned matrices."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "2. Which“famous”inequalitygives(cid:107)(A+B)x(cid:107)≤(cid:107)Ax(cid:107)+(cid:107)Bx(cid:107),andwhydoesitfollow from equation (5) that (cid:107)A+B(cid:107)≤(cid:107)A(cid:107)+(cid:107)B(cid:107)?"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "3. Explainwhy(cid:107)ABx(cid:107)≤(cid:107)A(cid:107)(cid:107)B(cid:107)(cid:107)x(cid:107),anddeducefromequation(5)that(cid:107)AB(cid:107)≤(cid:107)A(cid:107)(cid:107)B(cid:107). Show that this also implies c(AB)≤c(A)c(B)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.2 MatrixNormandConditionNumber 397 (cid:163) (cid:164)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "4. ForthepositivedefiniteA= 2 −1 ,compute(cid:107)A−1(cid:107)=1/λ ,(cid:107)A(cid:107)=λ ,andc(A)= −1 2 1 2 λ /λ . Find a right-hand side b and a perturbation δb so that the error is the worst 2 1 possible, (cid:107)δx(cid:107)/(cid:107)x(cid:107)=c(cid:107)δb(cid:107)/(cid:107)b(cid:107)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "5. Show that ifλ is any eigenvalue of A, Ax=λx, then |λ|≤(cid:107)A(cid:107)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "6. The matrices in equation (4) have norms between 100 and 101. Why?"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7. Comparing the eigenvalues of ATA and AAT, prove that (cid:107)A(cid:107)=(cid:107)AT(cid:107)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "8. For a positive definite A, the Cholesky decomposition is A = LDLT = RTR, where √ R = DLT. Show directly from equation (12) that the condition number of c(R) is the square root of c(A). Elimination without row exchanges cannot hurt a positive definite matrix, since c(A)=c(RT)c(R)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "9. Showthatmax|λ|isnotatruenorm,byfinding2by2counterexamplestoλ (A+ max B)≤λ (A)+λ (B) andλ (AB)≤λ (A)λ (B). max max max max max (cid:163) (cid:164)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "10. Show that the eigenvalues of B= 0 A are ±σ, the singular values of A. Hint: Try AT 0 i B2."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "11. (a) Do A and A−1 have the same condition number c? (b) In parallel with the upper bound (8) on the error, prove a lower bound: (cid:107)δx(cid:107) 1(cid:107)δb(cid:107) ≥ . (Consider A−1b=x instead of Ax=b.) (cid:107)x(cid:107) c (cid:107)b(cid:107)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "12. Find the norms λ and condition numbers λ /λ of these positive definite max max min matrices: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 100 0 2 1 3 1 . 0 2 1 2 1 1"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "13. Find the norms and condition numbers from the square roots of λ (ATA) and max λ (ATA): min (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) −2 0 1 1 1 1 . 0 2 0 0 −1 1"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "14. Prove that the condition number (cid:107)A(cid:107)(cid:107)A−1(cid:107) is at least 1."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "15. WhyisI theonlysymmetricpositivedefinitematrixthathasλ =λ =1? Then max min the only matrices with (cid:107)A(cid:107) = 1 and (cid:107)A−1(cid:107) = 1 must have ATA = I. They are matrices."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "16. Orthogonal matrices have norm (cid:107)Q(cid:107)=1. If A=QR, show that (cid:107)A(cid:107)≤(cid:107)R(cid:107) and also (cid:107)R(cid:107)≤(cid:107)A(cid:107). Then (cid:107)A(cid:107)=(cid:107)Q(cid:107)(cid:107)R(cid:107). Find anexampleof A=LU with(cid:107)A(cid:107)<(cid:107)L(cid:107)(cid:107)U(cid:107)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "17. (Suggested by Moler and Van Loan) Compute b−Ay and b−Az when (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) .217 .780 .563 .341 .999 b= A= y= z= . .254 .913 .659 −.087 −1.0 Is y closer than z to solving Ax = b? Answer in two ways: Compare the residual b−Ay to b−Az. Then compare y and z to the true x=(1,−1), Sometimes we want a small residual, sometimes a smallδx. √ Problems 18–20 are about vector norms other than the usual (cid:107)x(cid:107)= x·x."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "18. The“(cid:96)1 norm”is(cid:107)x(cid:107) =|x| +···+|x| . The“(cid:96)∞ norm”is(cid:107)x(cid:107) =max|x |. Compute 1 1 n ∞ i (cid:107)x(cid:107), (cid:107)x(cid:107) and (cid:107)x(cid:107) for the vectors 1 ∞ x=(1,1,1,1,1) and x=(.1,.7,.3,.4,.5)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "19. Prove that (cid:107)x(cid:107) ≤ (cid:107)x(cid:107) ≤ (cid:107)x(cid:107) . Show from the Schwarz inequality that the ratios ∞ 1 √ (cid:107)x(cid:107)/(cid:107)x(cid:107) and (cid:107)x(cid:107) /(cid:107)x(cid:107) are never larger than n. Which vector (x ,...,x ) gives ∞ √ 1 1 n ratios equal to n?"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "20. All vector norms must satisfy the triangle inequality. Prove that (cid:107)x+y(cid:107) ≤(cid:107)x(cid:107) +(cid:107)y(cid:107) and (cid:107)x+y(cid:107) ≤(cid:107)x(cid:107) +(cid:107)y(cid:107) . ∞ ∞ ∞ 1 1 1"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "21. Compute the exact inverse of the Hilbert matrix A by elimination. Then compute A−1 again by rounding all numbers to three figures:   1 1 1 2 3   In MATLAB : A=hilb(3)=1 1 1 . 2 3 4 1 1 1 3 4 5"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "22. ForthesameA,computeb=Axforx=(1,1,1)andx=(0,6,−3.6). Asmallchange ∆b produces a large change ∆x."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "23. Compute λ and λ for the 8 by 8 Hilbert matrix a = 1/(i+ j−1). If Ax = b max min ij with (cid:107)b(cid:107) = 1, how large can (cid:107)x(cid:107) be? If b has roundoff error less than 10−16, how large an error can this cause in x?"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "24. If you know L,U, Q, and R, is it faster to solve LUx=b or QRx=b?"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "25. Choosing the largest available pivot in each column (partial pivoting), factor each A into PA=LU:   (cid:34) (cid:35) 1 0 1 1 0   A= and A=2 2 0. 2 2 0 2 0"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.3 ComputationofEigenvalues 399 (cid:163) (cid:164)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "26. Find the LU factorization of A = ε 1 . On your computer, solve by elimination 1 1 whenε=10−3,10−6,10−9,10−12,10−15: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) ε 1 x 1+ε 1 = . 1 1 x 2 2 The true x is (1,1). Make a table to show the error for each ε. Exchange the two equations and solve again—the errors should almost disappear."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.3 Computation of Eigenvalues There is no one best way to find the eigenvalues of a matrix. But there are certainly some terrible ways which should never be tried, and also some ideas that do deserve a permanentplace. Webeginbydescribingoneveryroughandreadyapproach,thepower method, whose convergence properties are easy to understand. We added a graphic animation(withsound)tothecoursepageweb.mit.edu/18.06,toshowthepowermethod in action. We move steadily toward a more sophisticated algorithm, which starts by making a symmetricmatrixtridiagonalandendsbymakingitvirtuallydiagonal. Thatsecondstep is done by repeating Gram-Schmidt, so it is known as the QR method. The ordinary power method operates on the principle of a difference equation. It starts with an initial guess u and then successively forms u = Au , u = Au , and 0 1 0 2 1 in general u = Au . Each step is a matrix-vector multiplication. After k steps it k+1 k produces u = Aku , although the matrix Ak will never appear. The essential thing is k 0 that multiplication by A should be easy—if the matrix is large, it had better be sparse— because convergence to the eigenvector is often very slow. Assuming A has a full set of eigenvectors x ,...,x , the vector u will be given by the usual formula: 1 n k Eigenvectors weighted byλk u =c λkx +···+c λkx . k 1 1 1 n n n Supposethelargesteigenvalueλ isallbyitself;thereisnoothereigenvalueofthesame n magnitude, and |λ |≤···≤|λ |<|λ |. Then as long as the initial guess u contained 1 n−1 n 0 some component of the eigenvector x , so that c (cid:54)= 0, this component will gradually n n dominate in u : k (cid:181) (cid:182) (cid:181) (cid:182) k k u λ λ k 1 n−1 =c x +···+c x +c x . (1) 1 1 n−1 n−1 n n λk λ λ n n n The vectors u point more and more accurately toward the direction of x . Their conver- k n gence factor is the ratio r =|λ |/|λ |. It is just like convergence to a steady state, for n−1 n a Markov matrix, except nowλ may not equal 1. The scaling factorλk in equation (1) n n prevents u from growing very large or very small, in case |λ |>1 or |λ |<1. k n n Oftenwecanjustdivideeachu byitsfirstcomponentα beforetakingthenextstep. k k With this simple scaling, the power method u = Au /α converges to a multiple of k+1 k k x . The scaling factorsα will approachλ . n k n (cid:104) (cid:105) (cid:163) (cid:164) (cid:163) (cid:164) Example 1. The u approach the eigenvector 2/3 = .667 when A = .9 .2 is the k 1/3 .333 .1 .8 matrix of population shifts in Section 1.3: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 .9 .83 .781 .747 u = , u = , u = , u = , u = . 0 1 2 3 4 0 .1 .17 .219 .253 If r = |λ |/|λ | is close to 1, then convergence is very slow. In many applications n−1 n r > .9, which means that more than 20 iterations are needed to achieve one more digit. (Theexamplehadr=.7,anditwasstillslow.) Ifr=1,whichmeans|λ |=|λ |,then n−1 n convergencewillprobablynotoccuratall. Thathappens(intheappletwithsound)fora complex conjugate pairλ =λ . There are several ways to get around this limitation, n−1 n and we shall describe three of them:"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1. The block power method works with several vectors at once, in place of u . If we k multiply p orthonormal vectors by A, and then apply Gram-Schmidt to orthogonal- izethemagain—thatisasinglestepofthemethod—theconvergenceratiobecomes r(cid:48)=|λ |/|λ |. Wewillobtainapproximationsto pdifferenteigenvaluesandtheir n−p n elgenvectors."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "2. The inverse power method operates with A−1 instead of A. A single step is v = k+1 A−1v ,whichmeansthatwesolvethelinearsystemAv =v (andsavethefactors k k+1 k L and U!). Now we converge to the smallest eigenvalue λ and its eigenvector x , 1 1 provided |λ | < |λ |. Often it is λ that is wanted in the applications, and then 1 2 1 inverse iteration is an automatic choice."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "3. The shifted inverse power method is best of all. Replace A by A−αI. Each eigen- valueisshiftedbyα,andtheconvergencefactorfortheinversemethodwillchange to r(cid:48)(cid:48) =|λ −α|/|λ −α|. Ifαis a good approximation toλ , r(cid:48)(cid:48) will be very small 1 2 1 and the convergence is enormously accelerated. Each step of the method solves (A−αI)w =w : k+1 k c x c x c x 1 1 2 2 n n w = + +···+ . k (λ −α)k (λ −α)k (λ −α)k 1 2 n When α is close to λ , the first term dominates after only one or two steps. If 1 λ has already been computed by another algorithm (such as QR), then α is this 1 computed value. One standard procedure is to factor A−αI into LU and to solve Ux =(1,1,...,1) by back-substitution. 1 Ifλ isnotalreadyapproximated,theshiftedinversepowermethodhastogenerateits 1 ownchoiceofα. Wecanvaryα=α ateverystepifwewantto,so(A−α I)w =w . k k k+1 k"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.3 ComputationofEigenvalues 401 When A is symmetric, a very accurate choice is the Rayleigh quotient: wTAw shift by α =R(w )= k k . k k wTw k k Thisquotient R(x)hasaminimumatthetrueeigenvectorx . Itsgraphislikethebottom 1 of a parabola, so the error λ −α is roughly the square of the error in the eigenvector. 1 k The convergence factors |λ −α |/|λ −α | are themselves converging to zero. Then 1 k 2 k these Rayleigh quotient shifts give cubic convergence ofα toλ .2 k 1 Tridiagonal and Hessenberg Forms The power method is reasonable only for a matrix that is large and sparse. When too many entries are nonzero, this method is a mistake. Therefore we ask whether there is any simple way to create zeros. That is the goal of the following paragraphs. It should be said that after computing a similar matrix Q−1AQ with more zeros than A, we do not intend to go back to the power method. There are much more powerful variants, and the best of them seems to be the QR algorithm. (The shifted inverse power method has its place at the very end, in finding the eigenvector.) The first step is to pro- ducequicklyasmanyzerosaspossible,usinganorthogonalmatrixQ. IfAissymmetric, thensoisQ−1AQ. NoentrycanbecomedangerouslylargebecauseQpreserveslengths. TogofromAtoQ−1AQ,therearetwomainpossibilities: Wecanproduceonezeroat everystep(asinelimination), orwecanworkwithawholecolumnatonce. Forasingle zero, it is easy to use a plane rotation as illustrated in equation (7), found near the end of this section, that has cosθ and sinθ in a 2 by 2 block. Then we could cycle through all the entries below the diagonal, choosing at each step a rotationθ that will produce a zero; this is Jacobi’s method. It fails to diagonalize A after a finite number of rotations, since the zeros from early steps will be destroyed when later zeros are created. To preserve the zeros and stop, we have to settle for less than a triangular form. The Hessenberg form accepts one nonzero diagonal below the main diagonal. If a Hessenberg matrix is symmetric, it only has three nonzero diagonals. A series of rotations in the right planes will produce the required zeros. Householder found a new way to accomplish exactly the same thing. A Householder transformation is a reflection matrix determined by one vector v: vvT Householder matrix H =I−2 . (cid:107)v(cid:107)2 Often v is normalized to become a unit vector u=v/(cid:107)v(cid:107), and then H becomes I−2uuT. In either case H is both symmetric and orthogonal: HTH =(I−2uuT)(I−2uuT)=I−4uuT+4uuTuuT =I. 2Linearconvergencemeansthateverystepmultipliestheerrorbyafixedfactorr<1. Quadraticconvergence meansthattheerrorissquaredateverystep,asinNewton’smethodx −x =−f(x )/f(cid:48)(x )forsolving f(x)= k+1 k k k"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "0. Cubicconvergencetakes10−1to10−3to10−9. ThusH =HT =H−1. Householder’splanwastoproducezeroswiththesematrices,and its success depends on the following identity Hx=−σz: 7E Suppose z is the column vector (1,0,...,0), σ = (cid:107)x(cid:107), and v = x+σz. Then Hx=−σz=(−σ,0,...,0). The vector Hx ends in zeros as desired. The proof is to compute Hx and reach −σz: 2vvTx 2(x+σz)Tx Hx=x− =x−(x+σz) (cid:107)v(cid:107)2 (x+σz)T(x+σz) (2) =x−(x+σz) (because xTx=σ2) =−σz. This identity can be used right away, on the first column of A. The final Q−1AQ is allowed one nonzero diagonal below the main diagonal (Hessenberg form). Therefore only the entries strictly below the diagonal will be involved:       a 1 −σ 21       a 31 0  0  x= . , z=., Hx= . . (3)  . .  . .  . .  a 0 0 n1 At this point Householder’s matrix H is only of order n−1, so it is embedded into the lower right-hand corner of a full-size matrixU : 1     1 0 0 0 0 a ∗ ∗ ∗ ∗ 11     0  −σ ∗ ∗ ∗ ∗     U =0 H =U−1, and U−1AU = 0 ∗ ∗ ∗ ∗. 1   1 1 1   0   0 ∗ ∗ ∗ ∗ 0 0 ∗ ∗ ∗ ∗ The first stage is complete, and U−1AU has the required first column. At the second 1 1 stage, x consists of the last n−2 entries in the second column (three bold stars). Then H is of order n−2. When it is embedded inU , it produces 2 2     1 0 0 0 0 ∗ ∗ ∗ ∗ ∗     0 1 0 0 0 ∗ ∗ ∗ ∗ ∗     U =0 0 =U−1, U−1(U−1AU )U =0 ∗ ∗ ∗ ∗. 2   2 2 1 1 2   0 0 H  0 0 ∗ ∗ ∗ 2 0 0 0 0 ∗ ∗ ∗ U will take care of the third column. For a 5 by 5 matrix, the Hessenberg form is 3 achieved (it has six zeros). In general Q is the product of all the matricesU U ···U , 1 2 n−2 and the number of operations required to compute it is of order n3."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.3 ComputationofEigenvalues 403 Example 2. (to change a =a to zero) 13 31   (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1   0 1 0 −1 A=0 1 1, x= , v= , H = . 1 1 −1 0 1 1 0 Embedding H into Q, the result Q−1AQ is tridiagonal:     1 0 0 1 −1 0     Q=0 0 −1, Q−1AQ=−1 0 1. 0 −1 0 0 1 1 Q−1AQ is a matrix that is ready to reveal its eigenvalues—the QR algorithm is ready to begin—but we digress for a moment to mention two other applications of these same Householder matrices H."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1. The Gram-Schmidt factorization A = QR. Remember that R is to be upper trian- gular. We no longer have to accept an extra nonzero diagonal below the main one, since no matrices are multiplying on the right to spoil the zeros. The first step in constructing Q is to work with the whole first column of A:     a 1 11     a 21 0 vvT x=  . . .  , z= . . . , v=x+(cid:107)x(cid:107)z, H 1 =I−2 (cid:107)v(cid:107)2. a 0 n1 The first column of H A equals −(cid:107)x(cid:107)z. It is zero below the main diagonal, and it 1 is the first column of R. The second step works with the second column of H A, 1 from the pivot on down, and produces an H H A which is zero below that pivot. 2 1 (The whole algorithm is like elimination, but slightly slower.) The result of n−1 steps is an upper triangular R, but the matrix that records the steps is not a lower triangularL. InsteaditistheproductQ=H H ···H ,whichcanbestoredinthis 1 2 n−1 factored form (keep only the v’s) and never computed explicitly. That completes Gram-Schmidt."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "2. The singular value decompositionUTAV =Σ. The diagonal matrix Σ has the same shape as A, and its entries (the singular values) are the square roots of the eigenval- uesofATA. SinceHouseholdertransformationscanonlypreparefortheeigenvalue problem, we cannot expect them to produce Σ. Instead, they stably produce a bidi- agonal matrix, with zeros everywhere except along the main diagonal and the one above. The first step toward the SVD is exactly as in QR above: x is the first column of A, and H x is zero below the pivot. The next step is to multiply on the right by an H(1) 1 which will produce zeros as indicated along the first row:     ∗ ∗ ∗ ∗ ∗ ∗ 0 0     A→H A=0 ∗ ∗ ∗→H AH(1) =0 ∗ ∗ ∗. (4) 1 1 0 ∗ ∗ ∗ 0 ∗ ∗ ∗ Then two final Householder transformations quickly achieve the bidiagonal form:     ∗ ∗ 0 0 ∗ ∗ 0 0     H H AH(1) =0 ∗ ∗ ∗ and H H AH(1)H(2) =0 ∗ ∗ 0. 2 1 2 1 0 0 ∗ ∗ 0 0 ∗ ∗ The QR Algorithm for Computing Eigenvalues The algorithm is almost magically simple. It starts with A , factors it by Gram-Schmidt 0 into Q R , and then reverses the factors: A = R Q . This new matrix A is similar to 0 0 1 0 0 1 theoriginalonebecauseQ−1A Q =Q−1(Q R )Q =A . Sotheprocesscontinueswith 0 0 0 0 0 0 0 1 no change in the eigenvalues: All A are similar A =Q R and then A =R Q . (5) k k k k k+1 k k This equation describes the unshifted QR algorithm, and almost always A approaches a k triangular form, Its diagonal entries approach its eigenvalues, which are also the eigen- values of A . If there was already some processing to obtain a tridiagonal form, then A 0 0 is connected to the absolutely original A by Q−1AQ=A . 0 As it stands, the QR algorithm is good but not very good. To make it special, it needs two refinements: We must allow shifts to A −α I, and we must ensure that the QR k k factorization at each step is very quick."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1. The Shifted Algorithm. If the number α is close to an eigenvalue, the step in k equation (5) should be shifted immediately byα (which changes Q and R ): k k k A =α I =Q R and then A =R Q +α I. (6) k k k k k+1 k k k This matrix A is similar to A (always the same eigenvalues): k+1 k Q−1A Q =Q−1(Q R +α I)Q =A . k k k k k k k k k+1 What happens in practice is that the (n,n) entry of A —the one in the lower right-hand k corner—is the first to approach an eigenvalue. That entry is the simplest and most pop- ular choice for the shift α . Normally this produces quadratic convergence, and in the k symmetric case even cubic convergence, to the smallest eigenvalue. After three or four"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.3 ComputationofEigenvalues 405 steps of the shifted algorithm, the matrix A looks like this: k   ∗ ∗ ∗ ∗    ∗ ∗ ∗ ∗  A = , with ε(cid:191)1. k  0 ∗ ∗ ∗  0 0 ε λ(cid:48) 1 We accept the computed λ(cid:48) as a very close approximation to the true λ . To find the 1 1 next eigenvalue, the QR algorithm continues with the smaller matrix (3 by 3, in the illustration) in the upper left-hand corner. Its subdiagonal elements will be somewhat reduced by the first QR steps, and another two steps are sufficient to findλ . This gives 2 a systematic procedure for finding all the eigenvalues. In fact, the QR method is now completely described. It only remains to catch up on the eigenvectors—that is a single inverse power step—and to use the zeros that Householder created."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1. For the matrix A = 2 −1 with eigenvalues λ = 1 and λ = 3, apply the power −1 2 1 (cid:163) (cid:164)2 method u = Au three times to the initial guess u = 1 . What is the limiting k+1 k 0 0 vector u ? ∞ (cid:163) (cid:164)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "2. For the same A and the initial guess u = 3 , compare three inverse power steps to 0 4 one shifted step withα=uTAu /uTu : 0 0 0 0 (cid:34) (cid:35) 1 2 1 u =A−1u = u or u=(A−αI)−1u . k+1 k k 0 3 1 2 The limiting vector u is now a multiple of the other eigenvector (1,1). ∞"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "3. Explain why |λ /λ | controls the convergence of the usual power method. Con- n n−1 struct a matrix A for which this method does not converge. (cid:163) (cid:164)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "4. The Markov matrix A = .9 .3 has λ = 1 and .6, and the power method u = Aku (cid:163) (cid:164) .1 .7 k 0 converges to .75 . Find the eigenvectors of A−1. What does the inverse power .25 method u =A−ku converge to (after you multiply by .6k)? −k 0"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "5. Show that for any two different vectors of the same length, (cid:107)x(cid:107) = (cid:107)y(cid:107), the House- holder transformation with v=x−y gives Hx=y and Hy=x."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "6. Computeσ=(cid:107)x(cid:107), v=x+σz, and H =I−2vvT/vTv, Verify Hx=−σz: (cid:34) (cid:35) (cid:34) (cid:35) 3 1 x= and z= . 4 0"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7. Using Problem 6, find the tridiagonal HAH−1 that is similar to   1 3 4   A=3 1 0 4 0 0"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.4 IterativeMethodsforAx=b 407 (cid:163) (cid:164)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "8. ShowthatstartingfromA = 2 −1 ,theunshiftedQRalgorithmproducesonlythe 0 (cid:163) −1 2(cid:164) modest improvement A = 1 14 −3 . 1 5 −3 6"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "9. Apply to the following matrix A a single QR step with the shift α= a —which in 22 this case means without shift, since a = 0. Show that the off-diagonal entries go 22 from sinθto −sin3θ, which is cubic convergence. (cid:34) (cid:35) cosθ sinθ A= . sinθ 0 (cid:163) (cid:164)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "10. Check that the tridiagonal A= 0 1 is left unchanged by the QR algorithm. It is one 1 0 of the (rare) counterexamples to convergence (so we shift)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "11. Show by induction that, without shifts, (Q Q ···Q )(R ···R R ) is exactly the QR 0 1 k k 1 0 factorization of A . This identity connects QR to the power method and leads to k+1 an explanation of its convergence. If |λ |>|λ |>···>|λ |, these eigenvalues will 1 2 n gradually appear on the main diagonal."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "12. Choose sinθand cosθin the rotation P to triangularize A, and find R: (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) cosθ −sinθ 1 −1 ∗ ∗ P A= = =R. 21 sinθ cosθ 3 5 0 ∗"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "13. Choose sinθ and cosθ to make P AP−1 triangular (same A). What are the eigen- 21 21 values?"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "14. When A is multiplied by P (plane rotation), which entries are changed? When P A ij ij is multiplied on the right by P−1, which entries are changed now? ij"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "15. How many multiplications and how many additions are used to compute PA? (A careful organization of all the rotations gives 2n3 multiplications and additions, the 3 same as for QR by reflectors and twice as many as for LU.)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "16. (Turning a robot hand) A robot produces any 3 by 3 rotation A from plane rotations around the x, y, and z axes. If P P P A = I, the three robot turns are in A = 32 31 21 P−1P−1P−1. The three angles are Euler angles. Choose the firstθso that 21 31 32     cosθ −sinθ 0 −1 2 2  1  P A=sinθ cosθ 0  2 −1 2  is zero in the (2,1) position. 21 2 0 0 1 2 2 −1"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.4 Iterative Methods for Ax=b In contrast to eigenvalues, for which there was no choice, we do not absolutely need an iterative method to solve Ax = b. Gaussian elimination will reach the solution x in a finite number of steps (n3/3 for a full matrix, less than that for the large matrices we actually meet), Often that number is reasonable. When it is enormous, we may have to settle for an approximate x that can be obtained more quickly—and it is no use to go part way through elimination and then stop. Our goal is to describe methods that start from any initial guess x , and produce an 0 improved approximation x from the previous x . We can stop when we want to. k+1 k An iterative method is easy to invent, by splitting the matrix A. If A = S−T, then the equation Ax=b is the same as Sx=Tx+b. Therefore we can try Iteration from x to x Sx =Tx +b. (1) k k+1 k+1 k There is no guarantee that this method is any good. A successful splitting S−T satisfies two different requirements:"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1. The new vector x should be easy to compute. Therefore S should be a simple k+1 (and invertible!) matrix; it may be diagonal or triangular."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "2. The sequence x should converge to the true solution x. If we subtract the iteration k inequation(1)from thetrueequation Sx=Tx+b, theresultisa formulainvolving only the errors e =x−x : k k Error equation Se =Te . (2) k+1 k This is just a difference equation. It starts with the initial error e , and after k steps 0 it produces the new error e = (S−1T)ke . The question of convergence is exactly k 0 the same as the question of stability: x →x exactly when e →0. k k 7F The iterative method in equation (1) is convergent if and only if every eigenvalue of S−1T satisfies |λ| < 1. Its rate of convergence depends on the maximum size of |λ|: Spectral radius “rho” ρ(S−1T)=max|λ|. (3) i i Remember that a typical solution to e =S−1Te is a combination of eigenvectors: k+1 k Error after k steps e =c λkx +···+c λkx . (4) k 1 1 1 n n n Thelargest|λ|willeventuallybedominant,sothespectralradiusρ=|λ |willgovern i max the rate at which e converges to zero. We certainly needρ<1. k Requirements 1 and 2 above are conflicting. We could achieve immediate conver- gence with S=A and T =0; the first and only step of the iteration would be Ax =b. In 1 that case the error matrix S−1T is zero, its eigenvalues and spectral radius are zero, and the rate of convergence (usually defined as −logρ) is infinite. But Ax =b may be hard 1 to solve; that was the reason for a splitting. A simple choice of S can often succeed, and we start with three possibilities:"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.4 IterativeMethodsforAx=b 409"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1. S= diagonal part of A (Jacobi’s method)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "2. S= triangular pail of A (Gauss-Seidel method)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "3. S= combination of 1 and 2 (successive overrelaxation or SOR). S is also called a preconditioner, and its choice is crucial in numerical analysis. Example 1 (Jacobi). Here S is the diagonal part of A: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 2 −1 2 0 1 0 1 A= , S= ,T = ,S−1T = 2 . −1 2 2 1 0 1 0 2 If the components of x are v and w, the Jacobi step Sx =Tx +b is k+1 k (cid:34) (cid:35) (cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35) 2v =w +b v 0 1 v b /2 k+1 k 1 or = 2 + 1 . 2w =v +b , w 1 0 w b /2 k+1 k 2 k+1 2 k 2 The decisive matrix S−1T has eigenvalues ±1, which means that the error is cut in half 2 (one more binary digit becomes correct) at every step. In this example, which is much too small to be typical, the convergence is fast. For a larger matrix A, there is a very practical difficulty. The Jacobi iteration re- quires us to keep all components of x until the calculation of x is complete. A k k+1 much more natural idea, which requires only half as much storage, is to start using each component of the new x as soon as it is computed; x takes the place of x a com- k+1 k+1 k ponentatatime. Thenx canbedestroyedasfastasx iscreated,Thefirstcomponent k k+1 remains as before: New x a (x ) =(−a x −a x −···−a x ) +b . 1 11 1 k+1 12 2 13 3 1n n k 1 The next step operates immediately with this new value of x , to find (x ) : 1 2 k+1 New x a (x ) =−a (x ) +(−a x −···−a x ) +b . 2 22 2 k+1 21 1 k+1 23 3 2n n k 2 And the last equation in the iteration step will use new values exclusively: New x a (x ) =(−a x −a x −···−a x ) +b . n nn n k+1 n1 1 n2 2 nn−1 n−1 k+1 n This is called the Gauss-Seidel method, even though it was apparently unknown to Gauss and not recommended by Seidel. That is a surprising bit of history, because it is not a bad method. When the terms in x are moved to the left-hand side, S is seen as k+1 the lower triangular part of A. On the right-hand side, T is strictly upper triangular. Example 2 (Gauss-Seidel). Here S−1T has smaller eigenvalues: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 2 −1 2 0 0 1 0 1 A= , S= , T = , S−1T = 2 . −1 2 −1 2 0 0 0 1 4 A single Gauss-Seidel step takes the components v and w into k k (cid:34) (cid:35) (cid:34) (cid:35) 2v =w +b 2 0 0 1 k+1 k 1 or x = x +b. k+1 k 2w =v +b , −1 2 0 0 k+1 k 2 The eigenvalues of S−1T are 1 and 0. The error is divided by 4 every time, so a sin- 4 gle Gauss-Seidel step is worth two Jacobi steps. Since both methods require the same number of operations—we just use the new value instead of the old, and actually save on storage—the Gauss-Seidel method is better. Thisruleholdsinmanyapplications,eventhoughthereareexamplesinwhichJacobi converges and Gauss-Seidel fails (or conversely). The symmetric case is straightfor- ward: When all a >0, Gauss-Seidel converges if and only if A is positive definite. ii It was discovered during the years of hand computation (probably by accident) that convergence is faster if we go beyond the Gauss-Seidel correction x −x . Roughly k+1 k speaking, those approximations stay on the same side of the solution x. An overrelax- ation factor ω moves us closer to the solution. With ω= 1, we recover Gauss-Seidel; with ω > 1, the method is known as successive overrelaxation (SOR). The optimal choice ofωnever exceeds 2. It is often in the neighborhood of 1.9. To describe overrelaxation, let D, L, and U be the parts of A on, below, and above the diagonal, respectively. (This splitting has nothing to do with the A = LDU of elim- ination. In fact we now have A = L+D+U.) The Jacobi method has S = D on the left-hand side and T = −L−U on the right-hand side. Gauss-Seidel chose S = D+L and T =−U. To accelerate the convergence, we move to Overrelaxation [D+ωL]x =[(1−ω)D−ωU]x +ωb. (5) k+1 k Regardless of ω, the matrix on the left is lower triangular and the one on the right is upper triangular. Therefore x can still replace x , component by component, as soon k+1 k as it is computed. A typical step is a (x ) =a (x ) +ω[(−a x −···−a x ) +(−a x −···−a x ) +b ]. ii i k+1 ii i k i1 1 ii−1 i−1 k+1 ii i in n k i Iftheoldguessx happenedtocoincidewiththetruesolutionx,thenthenewguessx k k+1 would stay the same, and the quantity in brackets would vanish. (cid:163) (cid:164) Example 3 (SOR). For the same A= 2 −1 , each overrelaxation step is −1 2 (cid:34) (cid:35) (cid:34) (cid:35) 2 0 2(1−ω) ω x = x +ωb. k+1 k −ω 2 0 2(1−ω) Ifwedividebyω,thesetwomatricesaretheSandT inthesplittingAS−T;theiteration is back to Sx =Tx +b. The crucial matrix L=S−1T is k+1 k (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) −1 2 0 2(1−ω) ω 1−ω 1ω L= = 2 . −ω 2 0 2(1−ω) 1ω(1−ω) 1−ω+1ω2 2 4"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.4 IterativeMethodsforAx=b 411 TheoptimalωmakesthelargesteigenvalueofL(itsspectralradius)assmallaspossible. The whole point of overrelaxation is to discover this optimal ω. The product of the eigenvalues equals detL=detT/detS: λ λ =detL=(1−ω)2. 1 2 Always detS = detD because L lies below the diagonal, and detT = det(1−ω)D be- causeU lies above the diagonal. Their product is detL = (1−ω)n. (This explains why we never go as far as ω= 2. The product of the eigenvalues would be too large, and the iteration could not converge.) We also get a clue to the behavior of the eigenvalues: At the optimal ω the two eigenvalues are equal. They must both equal ω−1 so their product will match detL. This value of ω is easy to compute, because the sum of the eigenvalues always agrees with the sum of the diagonal entries (the trace of L): 1 Optimalω λ +λ =(ω −1)+(ω −1)=2−2ω + ω2 . (6) 1 2 opt opt opt 4 opt √ This quadratic equation gives ω = 4(2− 3) ≈ 1.07. The two equal eigenvalues are opt approximately ω−1 = 1.07, which is a major reduction from the Gauss-Seidel value λ = 1 at ω= 1. In this example, the right choice of ω has again doubled the rate of 4 convergence, because (1)2 ≈ .07. If ω is further increased, the eigenvalues become a 4 complex conjugate pair—both have |λ|=ω−1, which is now increasing withω. The discovery that such an improvement could be produced so easily, almost as if by magic, was the starting point for 20 years of enormous activity in numerical analysis. The first problem was solved in Young’s 1950 thesis—a simple formula for the optimal ω. The key step was to connect the eigenvalues λ of L to the eigenvalues µ of the original Jacobi matrix D−1(−L−U). That connection is expressed by Formula forω (λ+ω−1)2 =λω2µ2. (7) This is valid for a wide class of finite difference matrices, and if we takeω=1 (Gauss- Seidel)ityieldsλ2=λµ2. Thereforeλ=0andλ=µ2 asinExample2,whereµ=±1 2 and λ = 0, λ = 1. All the matrices in Young’s class have eigenvalues µ that occur in 4 plus-minus pairs, and the corresponding λ are 0 and µ2. So Gauss-Seidel doubles the Jacobi rate of convergence. The important problem is to choose ω so that λ will be minimized. Fortunately, max Young’s equation (7) is exactly our 2 by 2 example! The best ωmakes the two roots λ both equal toω−1: (cid:112) 2(1− 1−µ2) (ω−1)+(ω−1)=2−2ω+µ2ω2, or ω= . µ2 Foralargematrix,thispatternwillberepeatedforanumberofdifferentpairs±µ—and i we can only make a single choice of ω. The largest µ gives the largest value of ω and of λ =ω−1. Since our goal is to make λ as small as possible, that extremal pair max specifies the best choiceω : opt (cid:112) 2(1− 1−µ2 ) Optimalω ω = max and λ =ω −1. (8) opt max opt µ2 max 7G The splittings of the −1, 2, −1 matrix of order n yield these eigenvalues of B: π Jacobi (S= 0, 2, 0 matrix): S−1T has |λ| =cos max n+1 (cid:181) (cid:182) 2 π Gauss-Seidel (S = −1, 2, 0 matrix): S−1T has |λ| = cos max n+1 (cid:44) (cid:181) (cid:182) (cid:181) (cid:182) 2 2 π π SOR (with the bestω): |λ| = cos 1+sin . max n+1 n+1 This can only be appreciated by an example. Suppose A is of order 21, which is very moderate. Then h = 1 , cosπh = .99, and the Jacobi method is slow; cos2πh = .98 22 means that even Gauss-Seidel will require a great many iterations. But since sinπh = √ .02=.14, the optimal overrelaxation method will have the convergence factor .86 λ = =.75, with ω =1+λ =1.75. max opt max"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1.14 The error is reduced by 25% at every step, and a single SOR step is the equivalent of 30 Jacobi steps: (.99)30 =.75. That is a striking result from such a simple idea. Its real applications are not in one- dimensional problems like −u = f. A tridiagonal system Ax =b is already easy. It is xx for partial differential equations that overrelaxation (and other ideas) will be important. Changing to −u −u = f leads to the “five-point scheme.” The entries −1, 2, −1 in xx yy the x direction combine with −1, 2, −1 in the y direction to give a main diagonal of +4 and four off-diagonal entries of −1. The matrix A does not have a small bandwidth! There is no way to number the N2 mesh points in a square so that each point stays close to all four of its neighbors. That is the true curse of dimensionality, and parallel computers will partly relieve it. If the ordering goes a row at a time, every point must wait a whole row for the neigh- bor above it to turn up. The “five-point matrix” has bandwidth N: This matrix has had more attention, and been attacked in more different ways, than any other linear equa- tion Ax = b. The trend now is back to direct methods, based on an idea of Golub and Hockney; certain special matrices will fall apart when they are dropped the right way. (It is comparable to the Fast Fourier Transform.) Before that came the iterative methods of alternating direction, in which the splitting separated the tridiagonal matrix in the x direction from the one in the y direction, A recent choice is S = L U , in which small 0 0"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1. This matrix has eigenvalues 2− 2, 2, and 2+ 2:   2 −1 0   A=−1 2 −1. 0 −1 2 Find the Jacobi matrix D−1(−L−U) and the Gauss-Seidel matrix (D+L)−1(−U) and their eigenvalues, and the numbersω andλ for SOR. opt max"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "2. For this n by n matrix, describe the Jacobi matrix J =D−1(−L−U):   2 −1   −1 · ·  A= .  · · −1 −1 2 Show that the vector x = (sinπh,sin2πh,...,sinnπh) is an eigenvector of J with 1 eigenvalueλ =cosπh=cosπ/(n+1). 1"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "3. In Problem 2, show that x =(sinkπh,sin2kπh,...,sinnkπh) is an eigenvector of A. k Multiply x by A to find the corresponding eigenvalue α . Verify that in the 3 by 3 k √ √ k case these eigenvalues are 2− 2, 2, 2+ 2. Note. The eigenvalues of the Jacobi matrix J = 1(−L−U) = I− 1A are λ = 1− 2 2 k 1α =coskπh. They occur in plus-minus pairs andλ is cosπh. 2 k max Problems 4–5 require Gershgorin’s “circle theorem”: Every eigenvalue of A lies in at least one of the circles C ,...,C , where C has its center at the diagonal entry a . Its 1 n i ii radius r =∑ |a | is equal to the absolute sum along the rest of the row. i i(cid:54)=j ij Proof. Suppose x is the largest component of x. Then Ax=λx leads to i |x | (λ−a )x = ∑a x , or |λ−a |≤ ∑|a | j ≤ ∑|a |=r . ii i ij j ii ij ij i |x | j(cid:54)=i j(cid:54)=i i j(cid:54)=i"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "4. The matrix   3 1 1   A=0 4 1 2 2 5 is called diagonally dominant because every |a | > r . Show that zero cannot lie in ii i any of the circles, and conclude that A is nonsingular."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "5. Write the Jacobi matrix J for the diagonally dominant A of Problem 4, and find the three Gershgorin circles for J. Show that all the radii satisfy r < 1, and that the i Jacobi iteration converges."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "6. The true solution to Ax = b is slightly different from the elimination solution to LUx =b; A−LU misses zero because of roundoff. One strategyis to do everything 0 indoubleprecision,butabetterandfasterwayisiterativerefinement: Computeonly onevectorr=b−Ax indoubleprecision,solveLUy=r,andaddthecorrectionyto 0 x . Problem: Multiplyx =x +ybyLU,writetheresultasasplittingSx =Tx +b, 0 1 0 1 0 and explainwhy T is extremelysmall. This single step brings us almost exactlyto x."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7. For a general 2 by 2 matrix (cid:34) (cid:35) a b A= , c d find the Jacobi iteration matrix S−1T = −D−1(L+U) and its eigenvalues µ. Find i alsotheGauss-Seidelmatrix−(D+L)−1U anditseigenvaluesλ,anddecidewhether i λ =µ2 . max max"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "8. Change Ax=b to x=(I−A)x+b. What are S and T for this splitting? What matrix S−1T controls the convergence of x =(1−A)x +b? k+1 k"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "9. If λ is an eigenvalue of A, then is an eigenvalue of B = I−A. The real eigen- values of B have absolute value less than 1 if the real eigenvalues of A lie between and . (cid:163) (cid:164)"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "10. Show why the iteration x =(I−A)x +b does not converge for A= 2 −1 . k+1 k −1 2"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "7.4 IterativeMethodsforAx=b 415"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "11. Why is the norm of Bk never larger than (cid:107)B(cid:107)k? Then (cid:107)B(cid:107) < 1 guarantees that the powers Bk approach zero (convergence). This is no surprise, since |λ| is below max (cid:107)B(cid:107)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "12. If A is singular, then all splittings A = S−T must fail. From Ax = 0, show that S−1Tx=x. So this matrix B=S−1T hasλ=1 and fails."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "13. Change the 2s to 3s and find the eigenvalues of S−1T for both methods: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 3 0 0 1 3 0 0 1 (J) x = x +b (GS) x = x +b. k+1 k k+1 k 0 3 1 0 −1 3 0 0 Does |λ| for Gauss-Seidel equal |λ|2 for Jacobi? max max"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "14. Write a computer code (MATLAB or other) for Gauss-Seidel. You can define S and T from A, or set up the iteration loop directly from the entries a . Test it on the −1, ij 2, −1 matrices A of order 10, 20, 50, with b=(1,0,...,0)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "15. The SOR splitting matrix S is the same as for Gauss-Seidel except that the diagonal isdividedbyω. WriteaprogramforSORonannbynmatrix. Applyitwithω=1,"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "1.4, 1.8, 2.2 when A is the −1, 2, −1 matrix of order 10."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "16. When A = AT, the Arnoldi-Lanczos method finds orthonormal q’s so that Aq = j b q +a q +b q (withq =0). MultiplybyqT tofindaformulafora . The j−1 j−1 j j j j+1 0 j j equation says that AQ=QT where T is a matrix."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "17. What bound on |λ| does Gershgorin give For these matrices (see Problem 4)? max What are the three Gershgorin circles that contain all the eigenvalues?     .3 .3 .2 2 −1 0     A=.3 .2 .4 A=−1 2 −1. .2 .4 .1 0 −1 2 The key point for large matrices is that matrix-vector multiplication is much faster than matrix-matrix multiplication. A crucial construction starts with a vec- tor b and computes Ab,A2b,... (but never A2!). The first N vectors span the Nth Krylov subspace. They are the columns of the Krylov matrix K : N (cid:104) (cid:105) K = b Ab A2b ··· AN−1b . N The Arnoldi-Lanczos iteration orthogonalizes the columns of K , and the conjugate N gradient iteration solves Ax=b when A is symmetric positive definite. Arnoldi Iteration Conjugate Gradient Iteration q =b/(cid:107)b(cid:107) x =0, r =b, p =r 1 0 0 0 0 for n=1 to N−1 for n=1 to N v=Aq α =(rT r )/(pT Ap ) step length x to x n n n−1 n−1 n−1 n−1 n−1 n for j =1 to n x =x +α p approximate solution n n−1 n n−1 h =qTv r =r −α Ap new residual b−Ax jn j n n−1 n n−1 n v=v−h q β =(rTr )/(rT r ) improvement this step jn j n n n n−1 n−1 h =(cid:107)v(cid:107) p =r +β p next search direction n+1,n n n n n−1 q =v/h Note: Only 1 matrix vector multiplication Aq and Ap n+1 n+1,n"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "18. In Arnoldi, show that q is orthogonal to q . The Arnoldi method is Gram-Schmidt 2 1 orthogonalization applied to the Krylov matrix: K = Q R . The eigenvalues of N N N QTAQ are often very close to those of A, even for N (cid:191)n. The Lanczos iteration is N N Arnoldi for symmetric matrices (all coded in ARPACK)."
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "19. In conjugate gradients, show that r is orthogonal to r (orthogonal residuals), and 1 0 pTAp = 0 (search directions are A-orthogonal). The iteration solves Ax = b by 0 minimizing the error eTAe in the Krylov subspace. It is a fantastic algorithm. 8 Chapter Linear Programming and Game Theory"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "8.1 Linear Inequalities Algebra is about equations, and analysis is often about inequalities. The line between them has always seemed clear. But I have realized that this chapter is a counterexam- ple: linear programming is about inequalities, but it is unquestionably a part of linear algebra. It is also extremely useful—business decisions are more likely to involve linear programming than determinants or eigenvalues. There are three ways to approach the underlying mathematics: intuitively through the geometry, computationally through the simplex method, or algebraically through duality. These approaches are developed in Sections 8.1, 8.2, and 8.3. Then Section"
    },
    {
        "chapter": "ComputationswithMatrices",
        "question": "8.4 is about problems (like marriage) in which the solution is an integer. Section 8.5 discussespokerandothermatrixgames. TheMITstudentsinBringingDowntheHouse counted high cards to win at blackjack (Las Vegas follows fixed rules, and a true matrix game involves random strategies). Section 8.3 has something new in this fourth edition. The simplex method is now in a lively competition with a completely different way to do the computations, called an interior point method. The excitement began when Karmarkar claimed that his version was 50 times faster than the simplex method. (His algorithm, outlined in 8.2, was one of the first to be patented—something we then believed impossible, and not really desirable.) That claim brought a burst of research into methods that approach the solution from the “interior” where all inequalities are strict: x ≥ 0 becomes x > 0. The result is now a great way to get help from the dual problem in solving the primal problem. One key to this chapter is to see the geometric meaning of linear inequalities. An inequality divides n-dimensional space into a halfspace in which the inequality is satis- fied, and a halfspace in which it is not. A typical example is x+2y ≥ 4. The boundary between the two halfspaces is the line x+2y=4, where the inequality is “tight.” Figure"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.1 would look almost the same in three dimensions. The boundary becomes a plane like x+2y+z = 4, and above it is the halfspace x+2y+z ≥ 4. In n dimensions, the “plane” has dimension n−1. y (1,2) b x+2y ≥ 4 b x x+2y = 4 x+2y = 0 Figure8.1: Equationsgivelinesandplanes. Inequalitiesgivehalfspaces. Another constraint is fundamental to linear programming: x and y are required to be nonnegative. This pair of inequalities x ≥ 0 and y ≥ 0 produces two more halfspaces. Figure 8.2 is bounded by the coordinate axes: x ≥ 0 admits all points to the right of x=0, and y≥0 is the halfspace above y=0. The Feasible Set and the Cost Function The important step is to impose all three inequalities at once. They combine to give the shaded region in Figure 8.2. This feasible set is the intersection of the three halfspaces x+2y ≥ 4, x ≥ 0, and y ≥ 0. A feasible set is composed of the solutions to a family of linear inequalities like Ax ≥ b (the intersection of m halfspaces). When we also require that every component of x is nonnegative (the vector inequality x≥0), this adds n more halfspaces. The more constraints we impose, the smaller the feasible set. It can easily happen that a feasible set is bounded or even empty. If we switch our example to the halfspace x+2y≤4, keeping x≥0 and y≥0, we get the small triangle OAB. By combining both inequalities x+2y≥4 and x+2y≤4, the set shrinks to a line where x+2y=4. If we add a contradictory constraint like x+2y≤−2, the feasible set is empty. The algebra of linear inequalities (or feasible sets) is one part of our subject. But linear programming has another essential ingredient: It looks for the feasible point that maximizes or minimizes a certain cost function like 2x+3y. The problem in linear programming is to find the point that lies in the feasible set and minimizes the cost. The problem is illustrated by the geometry of Figure 8,2. The family of costs 2x+3y"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.1 LinearInequalities 419 gives a family of parallel lines. The minimum cost comes when the first line intersects the feasible set. That intersection occurs at B, where x∗ = 0 and y∗ = 2; the minimum cost is 2x∗+3y∗ =6. The vector (0,2) is feasible because it lies in the feasible set, it is optimal because it minimizes the cost function, and the minimum cost 6 is the value of the program. We denote optimal vectors by an asterisk. y cost 2x+3y = 6 feasible set x+2y ≥ 4 2x+3y = 0 x ≥ 0 y ≥ 0 B A x O Figure8.2: Thefeasiblesetwithflatsides,andthecosts2x+3y,touchingatB. The optimal vector occurs at a corner of the feasible set. This is guaranteed by the geometry, because the lines that give the cost function (or the planes, when we get to more unknowns) move steadily up until they intersect the feasible set. The first contact must occur along its boundary! The “simplex method” will go from one corner of the feasible set to the next until it finds the corner with lowest cost. In contrast, “interior point methods” approach that optimal solution from inside the feasible set. Note. With a different cost function, the intersection might not be just a single point. If the cost happened to be x+2y, the whole edge between B and A would be optimal. The minimum cost is x∗+2y∗, which equals 4 for all these optimal vectors. On our feasible set, the maximum problem would have no solution! The cost can go arbitrarily high and the maximum cost is infinite. Every linear programming problem falls into one of three possible categories:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. The feasible set is empty."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. The cost function is unbounded on the feasible set."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. The cost reaches its minimum (or maximum) on the feasible set: the good case. The empty and unbounded cases should be very uncommon for a genuine problem in economics or engineering. We expect a solution. Slack Variables There is a simple way to change the inequality x+2y≥4 to an equation. Just introduce the difference as a slack variable w = x+2y−4. This is our equation! The old con- straint x+2y ≥ 4 is converted into w ≥ 0, which matches perfectly the other inequality constraints x ≥ 0, y ≥ 0. Then we have only equations and simple nonnegativity con- straintsonx,y,w. Thevariableswthat“takeuptheslack”arenowincludedinthevector unknown x: Primal problem Minimize cx subject to Ax=b and x≥0. The row vector c contains the costs; in our example, c = [2 3 0]. The condition x ≥ 0 puts the problem into the nonnegative part of Rn. Those inequalities cut back on the solutions to Ax=b. Elimination is in danger, and a completely new idea is needed. The Diet Problem and Its Dual Our example with cost 2x+3y can be put into words. It illustrates the “diet problem” in linear programming, with two sources of protein—say steak and peanut butter. Each pound of peanut butter gives a unit of protein, and each steak gives two units. At least four units are required in the diet. Therefore a diet containing x pounds of peanut butter andysteaksisconstrainedbyx+2y≥4,aswellasbyx≥0andy≥0. (Wecannothave negativesteak or peanutbutter.) This is thefeasible set, andme p1001cm isto minimize thecost. Ifapoundofpeanutbuttercosts$2andasteakis$3. thenthecostofthewhole diet is 2x+3y. Fortunately, the optimal diet is two steaks: x∗ =0 and y∗ =2. Every linear program, including this one, has a dual. If the original prohe1v a min- imization, its dual is a maximization. The minimum in the given “primal problem” equals the maximum in its dual. This is the key to linear programming, and it will be explainedinSection8.3. Herewestaywiththedietproblemandtrytointerpretitsdual. In place of the shopper, who buys enough protein at minimal cost, the dual problem is faced by a druggist. Protein pills compete with steak and peanut butter. Immediately we meet the two ingredients of a typical linear program: The druggist maximizes the pill price p, but that price is subject to linear constraints. Synthetic protein must not cost more than the protein in peanut butter ($2 a unit) or the protein in steak ($3 for two units). The price must be nonnegative or the druggist will not sell. Since four units of protein are required, the income to the druggist will be 4p: Dual problem Maximize 4p, subject to p≤2, 2p≤3, and p≥0. In this example the dual is easier to solve than the primal; it has only one unknown p. The constraint 2p ≤ 3 is the tight one that is really active, and the maximum price of synthetic protein is p = $1.50. The maximum revenue is 4p = $6, and the shopper ends up paying the same for natural and synthetic protein. That is the duality theorem: maximum equals minimum."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.1 LinearInequalities 421 Typical Applications Thenextsectionwillconcentrateonsolvinglinearprograms. Thisisthetimetodescribe twopracticalsituationsinwhichweminimizeormaximizealinearcostfunctionsubject to linear constraints."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. Production Planning. Suppose General Motors makes a profit of $200 on each Chevrolet, $300 on each Buick, and $500 on each Cadillac. These get 20, 17, and 14 milespergallon,respectively,andCongressinsiststhattheaveragecarmustget18. The plant can assemble a Chevrolet in 1 minute, a Buick in 2 minutes, and a Cadillac in 3 minutes. What is the maximum profit in 8 hours (480 minutes)? Problem Maximize the profit 200x+300y+500z subject to 20x+17y+14z≥18(x+y+z), x+2y+3z≤480, x,y,z≥0."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. Sketch the feasible set with constraints x+2y ≥ 6, 2x+y ≥ 6, x ≥ 0, y ≥ 0. What points lie at the three “corners” of this set?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. (Recommended) On the preceding feasible set, what is the minimum value of the cost function x+y? Draw the line x+y= constant that first touches the feasible set. What points minimize the cost functions 3x+y and x−y?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. Show that the feasible set constrained by 2x+5y≤3, −3x+8y≤−5, x≥0, y≥0, is empty."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. Show that the following problem is feasible but unbounded, so it has no optimal solution: Maximize x+y, subject to x≥0, y≥0, −3x+2y≤−1, x−y≤2."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. Add a single inequality constraint to x ≥ 0, y ≥ 0 such that the feasible set contains only one point."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6. What shape is the feasible set x ≥ 0, y ≥ 0, z ≥ 0, x+y+z = 1, and what is the maximum of x+2y+3z?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. Solve the portfolio problem at the end of the preceding section."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8. InthefeasiblesetfortheGeneralMotorsproblem,thenonnegativityx,y,z≥0leaves aneighthofthree-dimensionalspace(thepositiveoctant). Howisthiscutbythetwo planes from the constraints, and what shape is the feasible set? How do its corners showthat, withonlythesetwoconstraints, therewillbeonlytwokindsofcarsinthe optimal solution?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. (Transportationproblem)SupposeTexas,California,andAlaskaeachproduceamil- lionbarrelsofoil;800,000barrelsareneededinChicagoatadistanceof1000,2000, and 3000 miles from the three producers, respectively; and 2,200,000 barrels are needed in New England 1500, 3000, and 3700 miles away. If shipments cost one unit for each barrel-mile, what linear program with five equality constraints must be solved to minimize the shipping cost?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.2 The Simplex Method This section is about linear programming with n unknowns x ≥ 0 and m constraints Ax≥b. Intheprevioussectionwehadtwovariables,andoneconstraintx+2y≥4. The full problem is not hard to explain, and not easy to solve. The best approach is to put the problem into matrix form. We are given A, b, and c:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. an m by n matrix A."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. a column vector b with m components, and"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. a row vector c (cost vector) with n components. Tobe“feasible,” thevectorx mustsatisfyx≥0andAx≥b. Theoptimalvectorx∗ isthe feasible vector of least cost—and the cost is cx=c x +···+c x . 1 1 n n Minimum problem Minimize the cost cx, subject to x≥0 and Ax≥b. The condition x ≥ 0 restricts x to the positive quadrant in n-dimensional space. In R2 it is a quarter of the plane; it is an eighth of R3. A random vector has one chance in 2n of being nonnegative. Ax ≥ b produces m additional halfspaces, and the feasible"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.2 TheSimplexMethod 423 vectors meet all of the m+n conditions. In other words, x lies in the intersection of m+n halfspaces. This feasible set has flat sides; it may be unbounded. and it may be empty. The cost function cx brings to the problem a family of parallel planes. One plane cx = 0 goes through the origin. The planes cx = constant give all possible costs. As the cost varies, these planes sweep out the whole n-dimensional space. The optimal x∗ (lowest cost) occurs at the point where the planes first touch the feasible set. Our aim is to compute x∗. We could do it (in principle) by finding all the corners of the feasible set, and computing their costs. In practice this is impossible. There could be billions of corners, and we cannot compute them all. Instead we turn to the simplex method, one of the most celebrated ideas in computational mathematics. It was developed by Dantzig as a systematic way to solve linear programs, and either by luck or genius it is an astonishing success. The steps of the simplex method are summarized later, and first we try to explain them. The Geometry: Movement Along Edges Ithinkitisthegeometricexplanationthatgivesthemethodaway. PhaseIsimplylocates one corner of the feasible set. The heart of the method goes from corner to corner along the edges of the feasible set. At a typical corner there are n edges to choose from. Some edges lead away from the optimal but unknown x∗, and others lead gradually toward it. Dantzig chose an edge that leads to a new corner with a lower cost. There is no possibility of returning to anything more expensive. Eventually a special corner is reached, from which all edges go the wrong way: The cost has been minimized. That corner is the optimal vector x∗, and the method stops. Thenextproblemistoturntheideasofcorner andedgeintolinearalgebra. Acorner is the meeting point of n different planes. Each plane is given by one equation—just as three planes (front wall, side wall, and floor) produce a corner in three dimensions. Each corner of the feasible set comes from turning n of the n+m inequalities Ax ≥ b and x≥0 into equations, and finding the intersection of these n planes. One possibility is to choose the n equations x = 0,...,x = 0, and end up at the 1 n origin. Like all the other possible choices, this intersection point will only be a genuine cornerifitalsosatisfiesthemremaininginequalityconstraints. Otherwiseitisnoteven in the feasible set, and is a complete fake. Our example with n=2 variables and m=2 constraints has six intersections, illustrated in Figure 8.3. Three of them are actually corners P, Q, R of the feasible set. They are the vectors (0,6), (2,2), and (6,0), One of them must be the optimal vector (unless the minimum cost is −∞). The other three, including the origin, are fakes. In general there are (n+m)!/n!m! possible intersections. That counts the number of ways to choose n plane equations out of n+m. The size of that binomial coefficient makes computing all corners totally impractical for large m and n. It is the task of Phase y x= 0 feasible set P b 2x+y = 6 b b Q x+2y = 6 y = 0 b b b x R Figure8.3: ThecornersP,Q,R,andtheedgesofthefeasibleset. I either to find one genuine corner or to establish that the feasible set is empty. We continue on the assumption that a corner has been found. Suppose one of the n intersecting planes is removed. The points that satisfy the remaining n−1 equations form an edge that comes out of the corner. This edge is the intersection of the n−1 planes. To stay in the feasible set, only one direction is allowed along each edge. But we do have a choice of n different edges, and Phase II must make that choice. To describe this phase, rewrite Ax ≥ b in a form completely parallel to the n simple constraints x ≥ 0. This is the role of the slack variables w = Ax−b. The constraints j Ax ≥ b are translated into w ≥ 0,...,w ≥ 0, with one slack variable for every row of 1 m A. The equation w=Ax−b, or Ax−w=b, goes into matrix form: (cid:34) (cid:35) (cid:104) (cid:105) x Slack variables give m equations A −I =b. w Thefeasiblesetisgovernedbythesemequationsandthen+msimpleinequalitiesx≥0, w≥0. We now have equality constraints and nonnegativity. The simplex method notices no difference between x and w, so we simplify: (cid:34) (cid:35) (cid:104) (cid:105) (cid:104) (cid:105) x A −I is renamed A is renamed x c 0 is renamed c. w The equality constraints are now Ax = b. The n+m inequalities become just x ≥ 0. The only trace left of the slack variable w is in the fact that the new matrix A is m by n+m, and the new x has n+m components. We keep this much of the original notation leaving m and n unchanged as a reminder of what happened. The problem has become: Minimize cx, subject to x≥0 and Ax=b."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.2 TheSimplexMethod 425 Example 1. The problem in Figure 8.3 has constraints x+2y≥6, 2x+y≥6, and cost x+y. The new system has four unknowns (x, y, and two slack variables): (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) 1 2 −1 0 6 A= b= c= 1 1 0 0 . 2 1 0 −1 6 The Simplex Algorithm Withequalityconstraints,thesimplexmethodcanbegin. Acornerisnowapointwhere n components of the new vector x (the old x and w) are zero. These n components of x arethefreevariablesinAx=b. Theremainingmcomponentsarethebasicvariablesor pivot variables. Setting the n free variables to zero, the m equations Ax = b determine the m basic variables. This “basic solution” x will be a genuine corner if its m nonzero components are positive. Then x belongs to the feasible set. 8A The corners of the feasible set are the basic feasible solutions of Ax=b. A solution is basic when n of its m+n components are zero, and it is feasible when it satisfies x≥0. Phase I of the simplex method finds one basic feasible solution. Phase II moves step by step to the optimal x∗. The corner point P in Figure 8.3 is the intersection of x=0 with 2x+y−6=0.   0 (cid:34) (cid:35) (cid:34) (cid:35) Corner (0,6,6,0)   1 2 −1 0 6 6 Basic (two zeros) Ax=  = =b. 2 1 0 −1 6 6 Feasible (positive nonzeros) 0 Which corner do we go to next? We want to move along an edge to an adjacent corner. Since the two corners are neighbors, m−1 basic variables will remain basic. Only one of the 6s will become free (zero). At the same time, one variable will move up from zero to become basic. The other m−1 basic components (in this case, the other 6) will change but stay positive. The choice of edge (see Example 2 below) decides which variable leaves the basis and which one enters. The basic variables are computed by solving Ax=b. The free components of x are set to zero. Example 2. An entering variable and a leaving variable move us to a new corner. x +x +6x +2x =8 1 3 4 5 Minimize 7x −x −3x subject to 3 4 5 x +x +3x =9. 2 3 5 Start from the corner at which x = 8 and x = 9 are the basic variables. At that corner 1 2 x = x = x = 0. This is feasible, but the zero cost may not be minimal. It would 3 4 5 be foolish to make x positive, because its cost coefficient is +7 and we are trying to 3 lower the cost. We choose x because it has the most negative cost coefficient −3. The 5 entering variable will be x . 5 With x entering the basis, x or x must leave. In the first equation, increase x and 5 1 2 5 decrease x while keeping x +2x = 8. Then x will be down to zero when x reaches 1 1 5 1 5"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. The second equation keeps x +3x = 9. Here x can only increase as far as 3. To 2 5 5 go further would make x negative, so the leaving variable is x . The new corner has 2 2 x=(2,0,0,0,3). The cost is down to −9. Quick Way In Ax = b, the right sides divided by the coefficients of the entering variable are 8 and 9. The smallest ratio 9 tells which variable hits zero first, and must 2 3 3 leave. We consider only positive ratios, because if the coefficient of x were −3, then 5 increasing x would actually increase x . (At x = 10 the second equation would give 5 2 5 x =39.) The ratio 9 says that the second variable leaves. It also gives x =3. 2 3 5 If all coefficients of x had been negative, this would be an unbounded case: we can 5 make x arbitrarily large, and bring the cost down toward −∞. 5 The current step ends at the new corner x = (2,0,0,0,3). The next step will only be easy if the basic variables x and x stand by themselves (as x and x originally did). 1 5 1 2 Therefore, we “pivot” by substituting x = 1(9−x −x ) into the cost function and the 5 3 2 3 first equation. The new problem, starting from the new corner, is: Minimize the cost 7x −x −(9−x −x )=x +8x −x −9 3 4 2 3 2 3 4 x −2x +1x +6x =2 with constraints 1 3 2 3 3 4 1x +1x +x =3. 3 2 3 3 5 The next step is now easy. The only negative coefficient −1 in the cost makes x the 4 entering variable. The ratios of 2 and 3, the right sides divided by the x column, make 6 0 4 x the leaving variable. The new corner is x∗ = (0,0,0,1,3). The new cost −91 is the 1 3 3 minimum. In a large problem, a departing variable might reenter the basis later on. But the cost keeps going down—except in a degenerate case—so the m basic variables can’t be the sameasbefore. Nocorneriseverrevisited! Thesimplexmethodmustendattheoptimal corner (or at −∞ if the cost turns out to be unbounded). What is remarkable is the speed at which x∗ is found. Summary The cost coefficients 7, −1, −3 at the first corner and 1, 8, −1 at the second corner decided the entering variables. (These numbers go into r, the crucial vec- tor defined below. When they are all positive we stop.) The ratios decided the leaving variables. Remark on Degeneracy A corner is degenerate if more than the usual n com- ponents of x are zero. More than n planes pass through the corner, so a basic variable happens to vanish. The ratios that determine the leaving variable will include zeros, and the basis might change without actually moving from the corner. In theory, we could stay at a corner and cycle forever in the choice of basis."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.2 TheSimplexMethod 427 Fortunately, cycling does not occur. It is so rare that commercial codes ignore it. Unfortunately, degeneracy is extremely common in applications—if you print the cost after each simplex step you see it repeat several times before the simplex method finds a good edge. Then the cost decreases again. The Tableau Each simplex step involves decisions followed by row operations—the entering and leaving variables have to be chosen, and they have to be made to come and go. One way to organize the step is to fit A, b, c into a large matrix, or tableau: (cid:34) (cid:35) A b Tableau is m+1 by m+n+1 T = . c 0 At the start, the basic variables may be mixed with the free variables. Renumbering if necessary,supposethatx ,...,x arethebasic(nonzero)variablesatthecurrentcorner. 1 m The first m columns of A form a square matrix B (the basis matrix for that corner). The last n columns give an m by n matrix N. The cost vector c splits into [c c ], and the B N unknown x into (x ,x ). B N At the corner, the free variables are x =0. There, Ax=b turns into Bx =b: N B (cid:34) (cid:35) B N b Tableau at corner T = x =0 x =B−1b cost=c B−1b. N B B c c 0 B N The basic variables will stand alone when elimination multiplies by B−1: (cid:34) (cid:35) I B−1N B−1b Reduced tableau T(cid:48) = . c c 0 B N To reach the fully reduced row echelon form R = rref(T), subtract c times the top B block row from the bottom row: (cid:34) (cid:35) I B−1N B−1b Fully reduced R= . 0 c −c B−1N −c B−1b N B B Let me review the meaning of each entry in this tableau, and also call attention to Ex- ample 3 (following, with numbers). Here is the algebra: Constraints x +B−1Nx =B−1b Corner x =B−1b, x =0. (1) B N B N The cost c x +c x has been turned into B B N N Cost cx=(c −c B−1N)x +c B−1b Cost at this corner=c B−1b. (2) N B N B B Everyimportant quantity appears in the fully reduced tableau R. We can decide whether the corner is optimal by looking at r =c −c B−1N in the middle of the bottom row. If N B any entry in r is negative, the cost can still be reduced. We can make rx negative, N at the start of equation (2), by increasing a component of x . That will be our next step. N But if r ≥ 0, the best corner has been found. This is the stopping test, or optimality condition: 8B The corner is optimal when r = c −c B−1N ≥ 0. Its cost is c B−1b. N B B Negative components of r correspond to edges on which the cost goes down. The entering variable x corresponds to the most negative component of r. i The components of r are the reduced costs—the cost in c to use a free variable N minus what it saves. Computing r is called pricing out the variables. If the direct cost (inc )islessthanthesaving(fromreducingbasicvariables),thenr <0,anditwillpay N i to increase that free variable. Suppose the most negative reduced cost is r . Then the ith component of x is the i N entering variable, which increases from zero to a positive value α at the next corner (the end of the edge). As x is increased, other components of x may decrease (to maintain Ax=b). The x i k that reaches zero first becomes the leaving variable—it changes from basic to free. We reach the next corner when a component of x drops to zero. B That new corner is feasible because we still have x ≥0. It is basic because we again have n zero components. The ith component of x went from zero to α. The kth com- N ponent of x dropped to zero (the other components of x remain positive). The leaving B B x that drops to zero is the one that gives the minimum ratio in equation (3): k 8C Suppose x is the entering variable and u is column i of N: i (B−1b) (B−1b) j k At new corner x =α=smallest ratio = . (3) i (B−1u) (B−1u) j k This minimum is taken only over positive components of B−1u. The kth col- umn of the old B leaves the basis (x becomes 0) and the new column u enters. k B−1u is the column of B−1N in the reduced tableau R, above the most negative entry in the bottom row r, If B−1u ≤ 0, the next corner is infinitely far away and the minimal cost is −∞ (this doesn’t happen here). Our example will go from the corner P to Q, and begin again at Q. Example 3. The original cost function x+y and constraints Ax=b=(6,6) give   (cid:34) (cid:35) 1 2 −1 0 6 A b   = 2 1 0 −1 6 . c 0 1 1 0 0 0 At the corner P in Figure 8.3, x=0 intersects 2x+y=6. To be organized, we exchange"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.2 TheSimplexMethod 429 columns 1 and 3 to put basic variables before free variables:   −1 2 1 0 6   Tableau at P T = 0 1 2 −1 6 . 0 1 1 0 0 Then,eliminationmultipliesthefirstrowby−1,togiveaunitpivot,andusesthesecond row to produce zeros in the second column:   1 0 3 −2 6   Fully reduced at P R= 0 1 2 −1 6 . 0 0 −1 1 −6 Look first at r = [−1 1] in the bottom row. It has a negative entry in column 3, so the third variable will enter the basis. The current corner P and its cost +6 are not optimal. The column above that negative entry is B−1u = (3,2); its ratios with the last column are 6 and 6. Since the first ratio is smaller, the first unknown w (and the first column of 3 2 the tableau) is pushed out of the basis. We move along the feasible set from corner P to corner Q in Figure 8.3. The new tableau exchanges columns 1 and 3, and pivoting by elimination gives     1 0 1 −2 2 3 0 1 −2 6  3 3       2 1 0 −1 6 → 0 1 −2 1 2 .  3 3  −1 0 0 1 −6 0 0 1 1 −4 3 3 In that new tableau at Q, r =[1 1] is positive. The stopping test is passed. The corner 3 3 x=y=2 and its cost +4 are optimal. The Organization of a Simplex Step The geometry of the simplex method is now expressed in algebra—“corners” are “basic feasible solutions.” The vector r and the ratio α are decisive. Their calculation is the heart of the simplex method, and it can be organized in three different ways:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. In a tableau, as above."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. By updating B−1 when column u taken from N replaces column k of B."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. By computing B=LU, and updating these LU factors instead of B−1. This list is really a brief history of the simplex method, In some ways, the most fascinating stage was the first—the tableau—which dominated the subject for so many years. For most of us it brought an aura of mystery to linear programming, chiefly becauseitmanagedtoavoidmatrixnotationalmostcompletely(bytheskillfuldeviceof writingoutallmatricesinfull!). Forcomputationalpurposes(exceptforsmallproblems in textbooks), the day of the tableau is over. To see why, remember that after the most negative coefficient in r indicates which column u will enter the basis, none of the other columns above r will be used. It was a waste of time to compute them. In a larger problem, hundreds of columns would be computed time and time again, just waiting for their turn to enter the basis. It makes the theory clear to do the eliminations so completely and reach R. But in practice this cannot be justified. It is quicker, and in the end simpler, to see what calculations are really necessary. Each simplex step exchanges a column of N for a column of B. Those columns are decided by r and α. This step begins with the current basis matrix B and the current solution x =B−1b. B A Step of the Simplex Method"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. Compute the row vectorλ=c B−1 and the reduced costs r =c −λN. B N"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. If r ≥ 0, stop: the current solution is optimal. Otherwise, if r is the most i negative component, choose u= column i of N to enter the basis."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. Compute the ratios of B−1b to B−1u, admitting only positive components of B−1u. (If B−1u < 0, the minimal cost is −∞.) When the smallest ratio occurs at component k, the kth column of the current B will leave."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. Update B, B−1, or LU, and the solution x =B−1b. Return to step 1. B This is sometimes called the revised simplex method to distinguish it from the oper- ations on a tableau. It is really the simplex method itself, boiled down. This discussion is finished once we decide how to compute steps 1, 3, and 4: λ=c B−1, v=B−1u, and x =B−1b. (4) B B The most popular way is to work directly with B−1, calculating it explicitly at the first corner. Atsucceedingcorners,thepivotingstepissimple. Whencolumnkoftheidentity matrixisreplacedbyu,columnkofB−1 isreplacedbyv=B−1u. Torecovertheidentity matrix, elimination will multiply the old B−1 by     −1 1 v 1 −v /v 1 1 k      · ·   · ·      E−1 = v  = 1/v  (5) k k      · ·   · ·  v 1 −v /v 1 n n k Many simplex codes use the product form of the inverse, which saves these simple matrices E−1 instead of directly updating B−1. When needed, they are applied to b and c . At regular intervals (maybe every 40 simplex steps), B−1 is recomputed and the E−1 B are erased. Equation (5) is checked in Problem 9 at the end of this section."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. Minimize x +x −x , subject to 1 2 3 2x −4x +x +x =4 1 2 3 4 3x +5x +x +x =2. 1 2 3 5 Whichofx ,x ,x shouldenterthebasis,andwhichofx ,x shouldleave? Compute 1 2 3 4 5 the new pair of basic variables, and find the cost at the new corner."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. After the preceding simplex step, prepare for and decide on the next step."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.2 TheSimplexMethod 433"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. In Example 3, suppose the cost is 3x+y. With rearrangement, the cost vector is c=(0,1,3,0). Show that r ≥0 and, therefore, that corner P is optimal."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. Suppose the cost function in Example 3 is x−y, so that after rearrangement c = (0,−1,1,0) at the corner P. Compute r and decide which column u should enter the basis. Then compute B−1u and show from its sign that you will never meet another corner. We are climbing the y-axis in Figure 8.3, and x−y goes to −∞."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. Again in Example 3, change the cost to x+3y. Verify that the simplex method takes you from P to Q to R, and that the corner R is optimal."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6. Phase I finds a basic feasible solution to Ax = b (a corner). After changing signs to make b ≥ 0, consider the auxiliary problem of minimizing w +w +···+w , 1 2 m subject to x ≥ 0, w ≥ 0, Ax+w = b. Whenever Ax = b has a nonnegative solution, the minimum cost in this problem will be zero—with w∗ =0. (a) Show that, for this new problem, the corner x = 0, w = b is both basic and fea- sible. Therefore its Phase I is already set, and the simplex method can proceed to find the optimal pair x∗, w∗. If w∗ = 0, then x∗ is the required corner in the original problem. (b) With A = [1 1] and b = [3], write out the auxiliary problem, its Phase I vector x=0,w=b,anditsoptimalvector. Findthecornerofthefeasiblesetx −x =3, 1 2 x ≥x ≥0, and draw a picture of this set. 1 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. If we wanted to maximize instead of minimize the cost (with Ax = b and x ≥ 0), what would be the stopping test on r, and what rules would choose the column of N to make basic and the column of B to make free?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8. Minimize 2x +x , subject to x +x ≥4, x +3x ≥12, x −x ≥0, x≥0. 1 2 1 2 1 2 1 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. Verify the inverse in equation (5), and show that BE has Bv = u in its kth column. Then BE is the correct basis matrix for the next stop, E−1B−1 is its inverse, and E−1 updates the basis matrix correctly."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "10. Suppose we want to minimize cx=x −x , subject to 1 2 2x −4x +x =6 1 2 3 (all x ,x ,x ,x ≥0). 1 2 3 4 3x +6x +x =12 1 2 4 Starting from x =(0,0,6,12), should x or x be increased from its current value of 1 2 zero? How far can it be increased until the equations force x or x down to zero? At 3 4 that point, what is the new x?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11. For the matrix P = I−AT(AAT)−1A, show that if x is in the nullspace of A, then Px=x. The nullspace stays unchanged under this projection."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "12. (a) MinimizethecostcTx=5x +4x +8x ontheplanex +x +x =3, bytesting 1 2 3 1 2 3 the vertices P, Q, R, where the triangle is cut off by the requirement x≥0. (b) Project c = (5,4,8) onto the nullspace of A = [1 1 1], and find the maximum step s that keeps e−sPc nonnegative."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.3 The Dual Problem EliminationcansolveAx=b,butthefourfundamentalsubspacesshowedthatadifferent anddeeperunderstandingispossible. Itisexactlythesameforlinearprogramming. The mechanics of the simplex method will solve a linear program, but duality is really at the center of the underlying theory. Introducing the dual problem is an elegant idea, and at the same time fundamental for the applications. We shall explain as much as we understand. The theory begins with the given primal problem: Primal (P) Minimize cx, subject to x≥0 and Ax≥b. The dual problem starts from the same A, b, and c, and reverses everything. In the primal, c is in the cost function and b is in the constraint, In the dual, b and c are switched, The dual unknown y is a row vector with m components, and the feasible set has yA≤c instead of Ax≥b. In short, the dual of a minimum problem is a maximum problem. Now y≥0: Dual (D) Maximize yb, subject to y≥0 and yA≤c. Thedualofthisproblemistheoriginalminimumproblem. Thereiscompletesymmetry between the primal and dual problems. The simplex method applies equally well to a maximization—anyway, both problems get solved at once. I have to give you some interpretation of all these reversals. They conceal a competi- tion between the minimizer and the maximizer. In the diet problem, the minimizer has n foods (peanut butter and steak, in Section 8.1). They enter the diet in the (nonnegative) amounts x ,...,x . The constraints represent m required vitamins, in place of the one 1 n earlier constraint of sufficient protein. The entry a measures the ith vitamin in the jth ij food, and the ith row of Ax≥b forces the diet to include at least b of that vitamin. If c i i is the cost of the jth food, then c x +···+c x =cx is the cost of the diet. That cost is 1 1 n n to be minimized. In the dual, the druggist is selling vitamin pills at prices y ≥ 0. Since food j i contains vitamins in the amounts a , the druggist’s price for the vitamin equivalent ij cannot exceed the grocer’s price c . That is the jth constraint in yA≤c. Working within j this constraint on vitamin prices, the druggist can sell the required amount b of each i vitamin for a total income of y b +···+y b =yb—to be maximized. 1 1 m m The feasible sets for the primal and dual problems look completely different. The first is a subset of Rn, marked out by x ≥ 0 and Ax ≥ b. The second is a subset of Rm,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.3 TheDualProblem 435 determined by y ≥ 0 and AT and c. The whole theory of linear programming hinges on the relation between primal and dual. Here is the fundamental result: 8D Duality Theorem When both problems have feasible vectors, they have optimal x∗ and y∗. The minimum cost cx∗ equals the maximum income y∗b. If optimal vectors do not exist, there are two possibilities: Either both feasible sets are empty, or one is empty and the other problem is unbounded (the maximum is +∞ or the minimum is −∞). The duality theorem settles the competition between the grocer and the druggist. The result is always a tie. We will find a similar “minimax theorem” in game theory. The customerhasnoeconomicreasontoprefervitaminsoverfood, eventhoughthedruggist guarantees to match the grocer on every food—and even undercuts on expensive foods (like peanut butter). We will show that expensive foods are kept out of the optimal diet, so the outcome can be (and is) a tie. This may seem like a total stalemate, but I hope you will not be fooled. The optimal vectors contain the crucial information. In the primal problem, x∗ tells the purchaser whattobuy. Inthedual,y∗ fixesthenaturalprices(shadowprices)atwhichtheeconomy should run. Insofar as our linear model reflects the true economy. x∗ and y∗ represent the essential decisions to be made. We want to prove that c∗x = y∗b. It may seem obvious that the druggist can raise the vitamin prices y∗ to meet the grocer, hut only one thing is truly clear: Since each food can be replaced by its vitamin equivalent, with no increase in cost, all adequate food diets must cost at least as much as vitamins. This is only a one-sided inequality, druggist’s price ≤ grocer’s price. It is called weak duality, and it is easy to prove for any linear program and its dual: 8E If x and y are feasible in the primal and dual problems, then yb≤cx. Proof. Since the vectors are feasible, they satisfy Ax ≥ b and yA ≤ c. Because feasi- bility also includes x ≥ 0 and y ≥ 0, we can take inner products without spoiling those inequalities (multiplying by negative numbers would reverse them): yAx≥yb and yAx≤cx. (1) Since the left-hand sides are identical, we have weak duality yb≤cx. This one-sided inequality prohibits the possibility that both problems are unbounded. If yb is arbitrarily large, a feasible x would contradict yb ≤ cx. Similarly, if cx can go down to −∞, the dual cannot admit a feasible y. Equally important, any vectors that achieve yb = cx must be optimal. At that point the grocer’s price equals the druggist’s price. We recognize an optimal food diet and optimal vitamin prices by the fact that the consumer has nothing to choose: 8F If the vectors x and y are feasible and cx=yb, then x and y are optimal. Since no feasible y can make yb larger than cx, our y that achieves this value is opti- mal. Similarly, any x that achieves the cost cx=yb must be an optimal x∗. We give an example with two foods and two vitamins. Note how AT appears when we write out the dual, since yA≤c for row vectors means ATyT ≤cT for columns. Primal Minimize x +4x Dual Maximize 6y +7y 1 2 1 2 subject to x ≥0, x ≥0 subject to y ≥0, y ≥0 1 2 1 2 2x +x ≥6 2y +5y ≤1 1 2 1 2 5x +3x ≥7. y +3x ≤4. 1 2 1 2 Solution x =3 and x =0 are feasible, with cost x +4x =3. In the dual, y = 1 and 1 2 1 2 1 2 y =0 give the same value 6y +7y =3. These vectors must be optimal. 2 1 2 Please look closely to see what actually happens at the moment when yb=cx. Some oftheinequalityconstraintsaretight,meaningthatequalityholds. Otherconstraintsare loose, and the key rule makes economic sense: (i) The diet has x∗ =0 when food j is priced above its vitamin equivalent. j (ii) The price is y∗ =0 when vitamin i is oversupplied in the diet x∗. i In the example, x = 0 because the second food is too expensive. Its price exceeds the 2 druggist’s price, since y +3y ≤ 4 is a strict inequality 1 +0 < 4. Similarly, the diet 1 2 2 required seven units of the second vitamin, but actually supplied 5x +3x =15. So we 1 2 found y = 0, and that vitamin is a free good. You can see how the duality has become 2 complete. These optimality conditions are easy to understand in matrix terms. From equation (1) we want y∗Ax∗ =y∗b at the optimum. Feasibility requires Ax∗ ≥b, and we look for any components in which equality fails. This corresponds to a vitamin that is oversup- plied, so its price is y∗ =0. i At the same time, we have y∗A ≤ c. All strict inequalities (expensive foods) corre- spond to x∗ = 0 (omission from the diet). That is the key to y∗Ax∗ = cx∗, which we j need. These are the complementary slackness conditions of linear programming, and the Kuhn-Tucker conditions of nonlinear programming: 8G The optimal vectors x∗ and y∗ satisfy complementary slackness: If (Ax∗) >b then y∗ =0 If (y∗A) >c then x∗ =0. (2) i i i j j j Let me repeat the proof. Any feasible vectors x and y satisfy weak duality: yb≤y(Ax)=(yA)x≤cx. (3) We need equality, and there is only one way in which y∗b can equal y∗(Ax∗). Any time b <(Ax∗) , the factor y∗ that multiplies these components must be zero. i i i"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.3 TheDualProblem 437 Similarly,feasibilitygivesyAx≤cx. Wegetequalityonlywhenthesecondslackness condition is fulfilled. If there is an overpricing (y∗A) <c , it must be canceled through j j multiplication by x∗ = 0. This leaves us with y∗b = cx∗ in equation (3). This equality j guarantees the optimality of x∗ and y∗. The Proof of Duality The one-sided inequality yb ≤ cx was easy to prove; it gave a quick test for optimal vectors (they turn it into an equality); and now it has given the slackness conditions in equation (2). The only thing it has not done is to show that y∗b=cx∗ is really possible. Until those optimal vectors are actually produced, the duality theorem is not complete. Toproducey∗ wereturntothesimplexmethod—whichhasalreadycomputedx∗. Our problemisto showthat themethod stoppedin theright placefor thedual problem(even though it was constructed to solve the primal). Recall that the m inequalities Ax ≥ b were changed to equations by introducing the slack variables w=Ax−b: (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) x x Primal feasibility A −I =b and ≥0. (4) w w Every simplex step picked m columns of the long matrix [A −I] to be basic, and shifted them (theoretically) to the front. This produced [B N]. The same shift reordered the long cost vector [c 0] into [c c ]. The stopping condition, which brought the simplex B N method to an end, was r =c −c B−1N ≥0. N B This condition r ≥ 0 was finally met, since the number of corners is finite. At that moment the cost was as low as possible: (cid:34) (cid:35) (cid:104) (cid:105) B−1b Minimum cost cx∗ = c c =c B−1b. (5) B N B 0 If we can choose y∗ = c B−1 in the dual, we certainly have y∗b = cx∗. The minimum B and maximum will be equal. We have to show that this y∗ satisfies the dual constraints yA≤c and y≥0: (cid:104) (cid:105) (cid:104) (cid:105) Dual feasibility y A −I ≤ c 0 . (6) Whenthesimplexmethodreshufflesthelongmatrixandvectortoputthebasicvariables first, this rearranges the constraints in equation (6) into (cid:104) (cid:105) (cid:104) (cid:105) y B N ≤ c c . (7) B N For y∗ =c B−1, the first half is an equality and the second half is c B−1N ≤c . This is B B N the stopping condition r ≥ 0 that we know to be satisfied! Therefore our y∗ is feasible, and the duality theorem is proved. By locating the critical m by m matrix B, which is nonsingular as long as degeneracy is forbidden, the simplex method has produced the optimal y∗ as well as x∗. Shadow Prices In calculus, everybody knows the condition for a maximum or a minimum: The first derivatives are zero. But this is completely changed by constraints. The simplest exam- ple is the line y = x. Its derivative is never zero, calculus looks useless, and the largest y is certain to occur at the end of the interval. That is exactly the situation in linear pro- gramming! There are more variables, and an interval is replaced by a feasible set, but still the maximum is always found at a corner of the feasible set (with only m nonzero components). The problem in linear programming is to locate that cornet For this, calculus is not completely helpless. Far from it, because “Lagrange multipliers” will bring back zero derivatives at the maximum and minimum. The dual variables y are exactly the La- grange multipliers. And they answer the key question: How does the minimum cost cx∗ =y∗b change, if we change b or c? Thisisaquestioninsensitivityanalysis. Itallowsustosqueezeextrainformationout of the dual problem. For an economist or an executive, these questions about marginal cost are the most important. If we allow large changes in b or c, the solution behaves in a very jumpy way. As the price of eggs increases, there will be a point at which they disappear from the diet. The variable x will jump from basic to free. To follow it properly, we would have egg to introduce “parametric” programming. But if the changes are small, the corner that was optimal remains optimal. The choice of basic variables does not change; B and N stay the same. Geometrically, we shifted the feasible set a little (by changing b), and we tilted the planes that come up to meet it (by changing c). When these changes are small, contact occurs at the same (slightly moved) corner. At the end of the simplex method, when the right basic variables are known, the corresponding m columns of A makeup the basis matrix B. At that corner, a shift of size ∆b changes the minimum cost by y∗∆b. The dual solution y∗ gives the rate of change of minimum cost (its derivative) with respect to changes in b. The components of y∗ are the shadow prices. If the requirement for a vitamin goes up by ∆, and the druggist’s price is y∗, then the diet cost (from druggist or grocer) will go up by y∗∆. In the case 1 1 that y∗ is zero, that vitamin is a free good and the small change has no effect. The diet 1 already contained more than b . 1 We now ask a different question. Suppose we insist that the diet contain some small edible amount of egg. The condition x ≥ 0 is changed to x ≥δ. How does this egg egg change the cost? If eggs were in the diet x∗, there is no change. But if x∗ =0, it will cost extra to add egg in the amountδ. The increase will not be the full price c δ, since we can cut down on egg other foods. The reduced cost of eggs is their own price, minus the price we are paying for the equivalent in cheaper foods. To compute it we return to equation (2) of Section"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.3 TheDualProblem 439"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.2: cost=(c −c B−1N)x +c B−1b=rx +c B−1b. N B N B N B Ifeggisthefirstfreevariable,thenincreasingthefirstcomponentofx toδwillincrease N thecostbyr δ. Therealcostofeggisr . Thisisthechangeindietcostasthezerolower 1 1 bound (nonnegativity constraint) moves upwards. We know that r ≥ 0, and economics tells us the same thing: The reduced cost of eggs cannot be negative or they would have entered the diet. Interior Point Methods The simplex method moves along edges of the feasible set, eventually reaching the opti- mal corner x∗. Interior point methods start inside the feasible set (where the constraints are all inequalities). These methods hope to move more directly to x∗ (and also find y∗). When they are very close to the answer, they stop. One way to stay inside is to put a barrier at the boundary. Add an extra cost in the form of a logarithm that blows up when any variable x or any slack variable w=Ax−b touches zero. The numberθis a small parameter to be chosen: (cid:195) (cid:33) n m Barrier problem P(θ) Minimize cx−θ ∑lnx +∑lnw . (8) i i 1 1 This cost is nonlinear (but linear programming is already nonlinear, from inequalities). The notation is simpler if the long vector (x,w) is renamed x and [A −I] is renamed A. The primal constraints are now x ≥ 0 and Ax = b. The sum of lnx in the barrier now i goes to m+n. The dual constraints are yA ≤ c. (We don’t need y ≥ 0 when we have Ax = b in the primal.) The slack variable is s = c−yA, with s ≥ 0. What are the Kuhn-Tucker conditionsforxandytobetheoptimalx∗ andy∗? Alongwiththeconstraintswerequire duality: cx∗ =y∗b. Including the barrier gives an approximate problem P(θ). For its Kuhn-Tucker op- timality conditions, the derivative of lnx gives 1/x . If we create a diagonal matrix X i i from those positive numbers x , and use e=[1 ··· 1] for the row vector of n+m ones, i then optimality in P(θ) is as follows: Primal (column vectors) Ax=b with x≥0 (9a) Dual (row vectors) yA+θeX−1 =c (9b) As θ → 0, we expect those optimal x and y to approach x∗ and y∗ for the original no- barrier problem, and θeX−1 will stay nonnegative. The plan is to solve equations (9a– 9b) with smaller and smaller barriers, given by the size ofθ. In reality, those nonlinear equations are approximately solved by Newton’s method (which means they are linearized). The nonlinear term is s = θeX−1. To avoid 1/x , i rewrite that as sX =θe. Creating the diagonal matrix S from s, this is eSX =θe. If we change e, y, c, and s to column vectors, and transpose, optimality now has three parts: Primal Ax=b, x≥0. (10a) Dual ATy+s=c. (10b) Nonlinear XSe−θe=0. (10c) Newton’s method takes a step ∆x, ∆y, ∆s from the current x, y, s. (Those solve equa- tions (10a) and (10b), but not (10c).) By ignoring the second-order term ∆X∆Se, the corrections come from linear equations! A∆x=0. (11a) Newton step AT∆y+∆s=0. (11b) S∆x+X∆s=θe−XSe. (11c) Robert Freund’s notes for his MIT class pin down the (quadratic) convergence rate and the computational complexity of this algorithm. Regardless of the dimensions m and n, the duality gap sx is generally below 10−8 after 20–80 Newton steps. This algorithm is used almost “as is” in commercial interior-point software, and for a large class of nonlinear optimization problems as well. The Theory of Inequalities There is more than one way to study duality. We quickly proved yb ≤ cx, and then used the simplex method to get equality. This was a constructive proof; x∗ and y∗ were actuallycomputed. Nowwelookbrieflyatadifferentapproach,whichomitsthesimplex algorithm and looks more directly at the geometry. I think the key ideas will be just as clear (in fact, probably clearer) if we omit some of the details. The best illustration of this approach came in the Fundamental Theorem of Linear Algebra. The problem in Chapter 2 was to find b in the column space of A. After elim- ination and the four subspaces, this solvability question was answered in a completely different way by Problem 11 in Section 3.1: 8H Ax=b has a solution or there is a y such that yA=0 and yb(cid:54)=0. Thisisthetheoremofthealternative,becausetofindbothxandyisimpossible: IfAx= b then yAx = yb (cid:54)= 0, and this contradicts yAx = 0x = 0. In the language of subspaces, either b is in the column space, or it has a component sticking into the left nullspace. That component is the required y. For inequalities, we want to find a theorem of exactly the same kind. Start with the same system Ax=b, but add the constraint x≥0. When does there exist a nonnegative solution to Ax=b? In Chapter 2, b was anywhere in the column space. Now we allow only nonnegative combinations, and the b’s no longer fill out a subspace. Instead, they fill a cone-shaped"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. What is the dual of the following problem: Minimize x +x , subject to x ≥ 0, 1 2 1 x ≥ 0, 2x ≥ 4, x +3x ≥ 11? Find the solution to both this problem and its dual, 2 1 1 2 and verify that minimum equals maximum."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. What is the dual of the following problem: Maximize y subject to y ≥ 0, y ≥ 0, 2 1 2 y +y ≤3? Solve both this problem and its dual. 1 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. SupposeAistheidentitymatrix(sothatm=n),andthevectorsbandcarenonnega- tive. Explainwhyx∗=bisoptimalintheminimumproblem,findy∗ inthemaximum problem, and verify that the two values are the same. If the first component of b is negative, what are x∗ and y∗?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. Constructa1by1exampleinwhichAx≥b,x≥0isunfeasible,andthedualproblem is unbounded. (cid:163) (cid:164)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. Startingwiththe2by2matrixA= 1 0 ,choosebandcsothatbothofthefeasible 0 −1 sets Ax≥b, x≥0 and yA≤c, y≥0 are empty."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6. If all entries of A, b, and c are positive, show that both the primal and the dual are feasible."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. Show that x=(1,1,1,0) and y=(1,1,0,1) are feasible in the primal and dual, with       0 0 1 0 1 1       0 1 0 0 1 1 A= , b= , c= . 1 1 1 1 1 1 1 0 0 1 1 3 Then, after computing cx and yb, explain how you know they are optimal."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.3 TheDualProblem 443"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8. Verify that the vectors in the previous exercise satisfy the complementary slackness conditions in equation (2), and find the one slack inequality in both the primal and the dual. (cid:163) (cid:164) (cid:163) (cid:164) (cid:163) (cid:164)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. Suppose that A= 1 0 , b= 1 , and c= 1 . Find the optimal x and y, and verify 0 1 −1 1 the complementary slackness conditions (as well as yb=cx)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "10. If the primal problem is constrained by equations instead of inequalities—Minimize cx subject to Ax = b and x ≥ 0—then the requirement y ≥ 0 is left out of the dual: Maximize yb subject to yA ≤ c. Show that the one-sided inequality yb ≤ cx still holds. Whywasy≥0 needed in equation (1) butnot here? This weak duality can be completed to full duality."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11. (a) Without the simplex method, minimize the cost 5x +3x +4x , subject to x + 1 2 3 1 x +x ≥1, x ≥0, x ≥0, x ≥0. 2 3 1 2 3 (b) What is the shape of the feasible set? (c) What is the dual problem, and what is its solution y?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "12. If the primal has a unique optimal solution x∗, and then c is changed a little, explain why x∗ still remains the optimal solution."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13. Writethedualofthefollowingproblem: Maximizex +x +x subjectto2x +x ≤ 1 2 3 1 2 4, x ≤6. What are the optimal x∗ and y∗ (if they exist!)? 3 (cid:163) (cid:164)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "14. IfA= 1 1 , describetheconeofnonnegativecombinationsofthecolumns. Ifblies 0 1 inside that cone, say b = (3,2), what is the feasible vector x? If b lies outside, say b=(0,1), what vector y will satisfy the alternative?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15. In three dimensions, can you find a set of six vectors whose cone of nonnegative combinations fills the whole space? What about four vectors?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "16. Use 8H to show that the following equation has no solution, because the alternative holds: (cid:34) (cid:35) (cid:34) (cid:35) 2 2 1 x= . 4 4 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17. Use 8I to show that there is no solution x≥0 (the alternative holds): (cid:34) (cid:35) (cid:34) (cid:35) 1 3 −5 2 x= . 1 −4 −7 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "18. Show that the alternatives in 8J (Ax ≥ b, x ≥ 0, yA ≥ 0, yb < 0, y ≤ 0) cannot both hold. Hint: yAx."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.4 Network Models Some linear problems have a structure that makes their solution very quick. Band ma- triceshaveall nonzerosclose tothemain diagonal, and Ax=biseasyto solve. In linear programming, we are interested in the special class for which A is an incidence matrix. Its entries are −1 or +1 or (mostly) zero, and pivot steps involve only additions and subtractions. Much larger problems than usual can be solved. Networks enter all kinds of applications. Traffic through an intersection satisfies Kirchhoff’s current law: flow in equals flow out. For gas and oil, network programming has designed pipeline systems that are millions of dollars cheaper than the intuitive (not optimized) designs. Scheduling pilots and crews and airplanes has become a significant problem in applied mathematics! We even solve the marriage problem—to maximize thenumberofmarriageswhenbrideshaveaveto. Thatmaynotbetherealproblem, but it is the one that network programming solves. The problem in Figure 8.5 is to maximize the flow from the source to the sink. The flows cannot exceed the capacities marked on the edges, and the directions given by the arrows cannot be reversed. The flow on the two edges into the sink cannot exceed 6+1=7. Is this total of 7 achievable? What is the maximal flow from left to right? The unknowns are the flows x from node i to node j. The capacity constraints are ij x ≤c . The flows are nonnegative: x ≥0 going with the arrows. By maximizing the ij ij ij return flow x (dotted line), we maximize the total flow into the sink. 61 Figure8.5: A6-nodenetworkwithedgecapacities: themaximalflowproblem. Another constraint is still to be heard from. It is the “conservation law,” that the flow into each node equals the flow out. That is Kirchhoff’s current law: Current law ∑x −∑x =0 for j =1,2,...,6. (12) ij jk i k The flows x enter node j from earlier nodes i. The flows x leave node j to later ij jk nodes k. The balance in equation (1) can be written as Ax = 0, where A is a node-edge incidence matrix (the transpose of Section 2.5). A has a row for every node and a +1,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.4 NetworkModels 445 −1 column for every edge:   1 1 −1 node 1   −1 1 1  2   Incidence  −1 1 1  3   A=   Matrix  −1 −1 1  4    −1 −1 1  5 −1 −1 1 6 edge 12 13 24 25 34 35 46 56 61 Maximal Flow Maximize x subject to Ax=0 and 0≤x ≤c . 61 ij ij A flow of 2 can go on the path 1-2-4-6-1. A flow of 3 can go along 1-3-4-6-1. An additional flow of 1 can take the lowest path 1-3-5-6-1. The total is 6, and no more is possible. How do you prove that the maximal flow is 6 and not 7? Trialanderrorisconvincing,butmathematicsisconclusive: Thekeyistofindacutin thenetwork,acrosswhichallcapacitiesarefilled. Thatcutseparatesnodes5and6from the others. The edges that go forward across the cut have total capacity 2+3+1=6— and no more can get across! Weak duality says that every cut gives a bound to the total flow, and full duality says that the cut of smallest capacity (the minimal cut) is filled by the maximal flow. 8K Max flow-min cut theorem. The maximal flow in a network equals the total capacity across the minimal cut. A“cut”splitsthenodesintotwogroupsS andT (sourceinS andsinkinT). Itscapacity is the sum of the capacities of all edges crossing the cut (from S to T). Several cuts might havethe same capacity. Certainly the total flowcan never be greater than the total capacity across the minimal cut. The problem, here and in all of duality, is to show that equality is achieved by the right flow and the right cut. Proof that max flow = min cut. Suppose a flow is maximal. Some nodes might still be reached from the source by additional flow, without exceeding any capacities. Those nodes go with the source into the set S. The sink must lie in the remaining set T, or it could have received more flow! Every edge across the cut must he filled, or extra flow could have gone further forward to a node in T. Thus the maximal flow does fill this cut to capacity. and equality has been achieved. This suggests a way to construct the maximal flow: Check whether any path has unused capacity. If so, add flow along that “augmenting path.” Then compute the re- maining capacities and decide whether the sink is cut off from the source, or additional flow is possible. If you label each node in S by the previous node that flow could come from, you can backtrack to find the path for extra flow. The Marriage Problem Suppose we have four women and four men. Some of those sixteen couples are compat- ible, others regrettably are not. When is it possible to find a complete matching, with everyone married? If linear algebra can work in 20-dimensional space, it can certainly handle the trivial problem of marriage. There are two ways to present the problem—in a matrix or on a graph. The matrix contains a =0 if the ith woman and jth man are not compatible, and a =1 if they are ij ij willing to try. Thus row i gives the choices of the ith woman, and column j corresponds to the jth man:   1 0 0 0   Compatibility 1 1 1 0 A=  has 6 compatible pairs. matrix 0 0 0 1 0 0 0 1 The left graph in Figure 8.6 shows two possible marriages. Ignoring the source s and sink t, it has four women on the left and four men on the right. The edges correspond to the 1s in the matrix, and the capacities are 1 marriage. There is no edge between the first woman and fourth man, because the matrix has a =0. 14 Figure 8.6: Two marriages on the left, three (maximum) on the right. The third is created by adding two new marriagesandonedivorce(backwardflow). It might seem that node M can’t be reached by more flow—but that is not so! The 2 extra flow on the right goes backward to cancel an existing marriage. This extra flow makes 3 marriages, which is maximal. The minimal cut is crossed by 3 edges. A complete matching (if it is possible) is a set of four is in the matrix. They would come from four different rows and four different columns, since bigamy is not allowed. ItislikefindingapermutationmatrixwithinthenonzeroentriesofA. Onthegraph,this means four edges with no nodes in common. The maximal flow is less than 4 exactly when a complete matching is impossible. Inourexamplethemaximalflowis3,not4. Themarriages1–1,2–2,4–4areallowed"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.4 NetworkModels 447 (andseveralothersetsofthreemarriages),butthereisnowaytoreachfour. Theminimal cut on the right separates the two women at the bottom from the three men at the top. Thetwowomenhaveonlyonemanlefttochoose—notenough. Thecapacityacrossthe cut is only 3. Whenever there is a subset of k women who among them like fewer than k men, a complete matching is impossible. That test is decisive. The same impossibility can be expressed in different ways:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. (For Chess) It is impossible to put four rooks on squares with 1s in A, so that no rook can take any other rook."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. (For Marriage Matrices) The 1s in the matrix can be covered by three horizontal or vertical lines. That equals the maximum number of marriages."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. (For Linear Algebra) Every matrix with the same zeros as A is singular. Remember that the determinant is a sum of 4!=24 terms. Each term uses all four rows and columns. The zeros in A make all 24 terms zero. A block of zeros is preventing a complete matching! The 2 by 3 submatrix in rows 3, 4 and columns 1, 2, 3 of A is entirely zero. The general rule for an n by n matrix is that a p by q block of zeros prevents a matching if p+q>n. Here women 3, 4 could marry only the man 4. If p women can marry only n−q men and p>n−q (which is the same as a zero block with p+q>n), then a complete matching is impossible. The mathematical problem is to prove the following: If every set of p women does likeatleast pmen,acompletematchingispossible. ThatisHall’scondition. Noblock of zeros is too large. Each woman must like at least one man, each two women must between them like at least two men, and so on, to p=n. 8L A complete matching is possible if (and only if) Hall’s condition holds. The proof is simplest if the capacities are n, instead of 1, on all edges across the middle. Thecapacitiesoutofthesourceandintothesinkarestill1. Ifthemaximalflow is n, all those edges from the source and into the sink are filled—and the flow produces n marriages. When a complete matching is impossible, and the maximal flow is below n, some cut must be responsible. That cut will have capacity below n, so no middle edges cross it. Suppose p nodes on the left and r nodes on the right are in the set S with the source. The capacity across that cut is n−p from the source to the remaining women, and r from these men to the sink. Since the cut capacity is below n, the p women like only the r men and no others. But the capacity n−p+r is below n exactly when p>r, and Hall’s condition fails. Spanning Trees and the Greedy Algorithm A fundamental network model is the shortest path problem—in which the edges have lengthsinsteadofcapacities. Wewanttheshortestpathfromsourcetosink. Iftheedges are telephone lines and the lengths are delay times, we are finding the quickest route for a call, If the nodes are computers, we are looking for the perfect message-passing protocol. A closely related problem finds the shortest spanning tree—a set of n−1 edges connecting all the nodes of the network. Instead of getting quickly between a source and a sink, we are now minimizing the cost of connecting all the nodes. There are no loops, because the cost to close a loop is unnecessary. A spanning tree connects the nodes without loops, and we want the shortest one. Here is one possible algorithm:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. Start from any node s and repeat the following step: Add the shortest edge that connects the current tree to a new node. In Figure 8.7, the edge lengths would come in the order 1, 2, 7, 4, 3, 6. The last step skipstheedgeoflength5,whichclosesaloop. Thetotallengthis23—butisitminimal? We accepted the edge of length 7 very early, and the second algorithm holds out longer. Figure8.7: Anetworkandashortestspanningtreeoflength23."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. Accept edges in increasing order of length, rejecting edges that complete a loop. Now the edges come in the order 1, 2, 3, 4, 6 (again rejecting 5), and 7. They are the same edges—although that will not always happen. Their total length is the same—and that does always happen. The spanning tree problem is exceptional, because it can be solved in one pass. In the language of linear programming, we are finding the optimal corner first. The spanning tree problem is being solved like back-substitution, with no false steps. This general approach is called the greedy algorithm. Here is another greedy idea:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. Build trees from all n nodes, by repeating the following step: Select any tree and add the minimum-length edge going out from that tree. The steps depend on the selection order of the trees. To stay with the same tree is algorithm 1. To take the lengths in order is algorithm 2. To sweep through all the trees"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.4 NetworkModels 449 in turn is a new algorithm. It sounds so easy, but for a large problem the data structure becomes critical, With a thousand nodes, there might be nearly a million edges, and you don’t want to go through that list a thousand times. Further Network Models There are important problems related to matching that are almost as easy:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. The optimal assignment problem: a measures the value of applicant i in job j. ij Assign jobs to maximize the total value—the sum of the a on assigned jobs. (If ij all a are 0 or 1, this is the marriage problem.) ij"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. The transportation problem: Given supplies at n points and demands at n markets chooseshipmentsx fromsupplierstomarketsthatminimizethetotalcost∑C x . ij ij ij (Ifallsuppliesanddemandsare1,thisistheoptimalassignmentproblem—sending one person to each job.)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. In Figure 8.5, add 3 to every capacity. Find by inspection the maximal flow and minimal cut."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. Find a maximal flow and minimal cut for the following network:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. If you could increase the capacity of any one pipe in the network above, which change would produce the largest increase in the maximal flow?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. Draw a 5-node network with capacity |i− j| between node i and node j. Find the largest possible flow from node 1 to node 4."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. In a graph, the maximum number of paths from s tot with no common edges equals the minimum number of edges whose removal disconnects s from t. Relate this to the max flow-min cut theorem."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6. Find a maximal set of marriages (a complete matching, if possible) for     0 0 1 0 0 1 1 0 0 0     1 1 0 1 1 0 1 0 1 0     A=0 1 1 0 1 and B=0 0 1 0 1.     0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 Sketch the network for B, with heavier lines on the edges in your matching."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. For the matrix A in Problem 6, which rows violate Hall’s condition—by having all their 1s in too few columns? Which p by q submatrix of zeros has p+q>n?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8. How many lines (horizontal and vertical) are needed to cover all the 1s in A in Prob- lem6? Foranymatrix, explainwhyweak dualityis true: If k marriagesare possible, then it takes at least k lines to cover all the 1s."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. (a) Suppose every row and every column contains exactly two 1s. Prove that a com- plete matching is possible. (Show that the 1s cannot be covered by less than n lines) (b) Find an example with two or more is in each row and column, for which a com- plete matching is impossible."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "10. If a 7 by 7 matrix has 15 1s, prove that it allows at least 3 marriages."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11. For infinite sets, a complete matching may be impossible even if Hail’s condition is passed. If the first row is all 1s and then every a =1, show that any p rows have ii−1 1s in at least p columns—and yet there is no complete matching."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.5 GameTheory 451"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "12. If Figure 8.5 shows lengths instead of capacities, find the shortest path from s to t, and a minimal spanning tree."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13. Apply algorithms 1 and 2 to find a shortest spanning tree for the networkof Problem"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "14. (a) Why does the greedy algorithm work for the spanning tree problem? (b) Show by example that the greedy algorithm could fail to find the shortest path from s tot, by starting with the shortest edge."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15. If A is the 5 by 5 matrix with is just above and just below the main diagonal, find (a) a set of rows with 1s in too few columns. (b) a set of columns with is in too few rows. (c) a p by q submatrix of zeros with p+q>5. (d) four lines that cover all the 1s."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "16. The maximal flow problem has slack variables w = c −x for the difference be- ij ij ij tween capacities and flows. State the problem of Figure 8.5 as a linear program."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.5 Game Theory The best way to explain a two-person zero-sum game is to give an example. It has two players X andY, and the rules are the same for every turn: X holds up one hand or two, and so doesY. If they make the same decision,Y wins $10. If they make opposite decisions, X wins $10 for one hand and $20 for two: (cid:34) (cid:35) Payoff matrix −10 20 one hand byY A= (payments to X) 10 −10 two hands byY one hand two hands by X by X If X does the same thing every time,Y will copy him and win. SimilarlyY cannot stick to a single strategy, or X will do the opposite. Both players must use a mixed strategy, and the choice at every turn must be independent of the previous turns. If there is some historical pattern, the opponent can take advantage of it. Even the strategy “stay with the same choice until you lose” is obviously fatal. After enough plays, your opponent would know exactly what to expect. In a mixed strategy, X can put up one hand with frequency x and both hands with 1 frequency x = 1−x . At every turn this decision is random. Similarly Y can pick 2 1 probabilities y and y =1−y . None of these probabilities should be 0 or 1; otherwise 1 2 1 the opponent adjusts and wins. If they equal 1, Y would be losing $20 too often. (He 2 would lose $20 a quarter of the time, $10 another quarter of the time, and win $10 half thetime—anaveragelossof$2.50. Thisismorethannecessary.) ButthemoreY moves toward a pure two-hand strategy, the more X will move toward one hand. Thefundamentalproblemistofindthebestmixedstrategies. CanX chooseprobabil- ities x and x that presentY with no reason to move his own strategy (and vice versa)? 1 2 Then the average payoff will have reached a saddle point: It is a maximum as far as X is concerned, and a minimum as far asY is concerned. To find such a saddle point is to solve the game. X iscombiningthetwocolumnswithweightsx and1−x toproduceanew“mixed” 1 1 column. Weights 3 and 2 would produce this column: 5 5 (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 3 −10 2 20 2 Mixed column + = . 5 10 5 −10 2 Against this mixed strategy, Y will always lose $2. This does not mean that all strategies are optimal for Y! If Y is lazy and stays with one hand, X will change and start winning $20. ThenY will change, and then X again. Finally, since we assume they are both intelligent, they settle down to optimal mixtures. Y will combine the rows with weights y and 1−y , trying to produce a new row which is as small as possible: 1 1 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) Mixed row y −10 20 +(1−y ) 10 −10 = 10−20y −10+30y . 1 1 1 1 The right mixture makes the two components equal, at y = 2. Then both components 1 5 equal 2; the mixed row becomes [2 2]. With this strategyY cannot lose more than $2. Y has minimized the maximum loss, and that minimax agrees with the maximin found by X. The value of the game is minimax = maximin = $2. The optimal mixture of rows might not always have equal entries! Suppose X is allowed a third strategy of holding up three hands to win $60 whenY puts up one hand and $80 whenY puts up two. The payoff matrix becomes (cid:34) (cid:35) −10 20 60 A= . 10 −10 80 X will choose the three-hand strategy (column 3) every time, and win at least $60. At the same time, Y always chooses the first row; the maximum loss is $60. We still have maximin = minimax = $60, but the saddle point is over in the corner. In Y’s optimal mixture of rows, which was purely row 1, $60 appears only in the column actually used by X. In X’s optimal mixture of columns, which was column 3, $60 appears in the row that entersY’s best strategy. This rule corresponds exactly to the complementary slackness condition of linear programming."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.5 GameTheory 453 Matrix Games The most general “m by n matrix game” is exactly like our example. X has n possible moves (columns of A). Y chooses from the m rows. The entry a is the payment when ij X chooses column j andY chooses row i. A negative entry means a payment toY. This is a zero-sum game. Whatever one player loses, the other wins. X is free to choose any mixed strategy x=(x ,...,x ). These x give the frequencies 1 n 1 for the n columns and they add to 1. At every turn X uses a random device to produce strategy i with frequency x . Y chooses a vector y = (y ,...,y ), also with y ≥ 0 and i 1 m i ∑y =1, which gives the frequencies for selecting rows. i A single play of the game is random. On the average, the combination of column j forX androwiforY willturnupwithprobabilityx y . Whenitdoescomeup,thepayoff i i is a . The expected payoff to X from this combination is a x y , and the total expected ij ij j i payoff from each play of the same game is ∑∑a xjy =yAx: ij i     x 1 a a ··· a (cid:104) (cid:105) 11 12 1n   yAx= y 1 ··· y m   . . . . . . . . .    x . . .2   = =a av11 ex r1 ay g1 e+ p· a· y· o+ ffa .mnx ny m a a ··· a m1 m2 mn x n It is this payoff yAx that X wants to maximize andY wants to minimize. Example 1. Suppose A is the n by n identity matrix, A = I. The expected payoff be- comesyIx=x y +···+x y . X ishopingtohitonthesamechoiceasY,towina =$1. 1 1 n n ii Y istryingtoevadeX,topaya =$0. IfX choosesanycolumnmoreoftenthananother, ij Y can escape more often. The optimal mixture is x∗ = (1/n,1/n,...,1/n). Similarly Y cannot overemphasize any row—the optimal mixture is y∗ = (1/n,1/n,...,1/n). The probability that both will choose strategy i is (1/n)2, and the sum over i is the expected payoff to X. The total value of the game is n times (1/n)2, or 1/n:    1 1/n (cid:181) (cid:182) (cid:181) (cid:182) (cid:104) (cid:105) 2 2 y∗Ax∗ = 1/n ··· 1/n   ...    . . .  = 1 +···+ 1 = 1 . n n n 1 1/n As n increases,Y has a better chance to escape. The value 1/n goes down. The symmetric matrix A = I did not make the game fair. A skew-symmetric matrix, AT = −A, means a completely fair game. Then a choice of strategy j by X and i by Y wins a for X, and a choice of j byY and i by X wins the same amount forY (because ij a =−a ). The optimal strategies x∗ and y∗ must be the same, and the expected payoff ji ij must be y∗Ax∗ = 0. The value of the game, when AT = −A, is zero. But the strategy is still to be found. Example 2.   0 −1 −1   Fair game A=1 0 −1. 1 1 0 In words, X andY both choose a number between 1 and 3. The smaller choice wins $1. (IfX chooses2andY chooses3,thepayoffisa =$1; iftheychoosethesamenumber, 32 weareonthediagonalandnobodywins.) Neitherplayercanchooseastrategyinvolving 2 or 3. The pure strategies x∗ =y∗ =(1,0,0) are optimal—both players choose 1 every time. The value is y∗Ax∗ =a =0. 11 The matrix that leaves all decisions unchanged has mn equal entries, say α. This simply means that X wins an additional amount αat every turn. The value of the game is increased byα, but there is no reason to change x∗ and y∗. The Minimax Theorem Put yourself in the place of X, who chooses the mixed strategy x = (x ,...,x ). Y will 1 n eventually recognize that strategy and choose y to minimize the payment yAx. An intel- ligent player X will select x∗ to maximize this minimum: X wins at least minyAx∗ =maxminyAx. (1) y x y PlayerY does the opposite. For any chosen strategy y, X will maximize yAx. There- foreY will choose the mixture y∗ that minimizes this maximum: Y loses no more than maxy∗Ax=minmaxyAx. (2) x y x I hope you see what the key result will be, if it is true. We want the amount in equation (1) that X is guaranteed to win to equal the amount in equation (2) thatY must be satisfied to lose. Then the game will be solved: X can only lose by moving from x∗ and Y can only lose by moving from y∗, The existence of this saddle point was proved by von Neumann: 8M For any matrix A, the minimax over all strategies equals the maximin: Minimax theorem maxminyAx=minmaxyAx=value of the game. x y y x (3) If the maximum on the left is attained at x∗, and the minimum on the right is attained at y∗, this is a saddle point from which nobody wants to move: y∗Ax≤y∗Ax∗ ≤yAx∗ for all x and y. (4) At this saddle point, x∗ is at least as good as any other x (since y∗Ax ≤y∗Ax∗). And the second playerY could only pay more by leaving y∗."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8.5 GameTheory 455 As in duality theory, maximin≤minimax is easy. We combine the definition in equa- tion (1) of x∗ and the definition in equation (2) of y∗: maxminyAx=minyAx∗ ≤y∗Ax∗ ≤maxy∗Ax=minmaxyAx. (5) x y y x y x This only says that if X can guarantee to win at least α, andY can guarantee to lose no more than β, then α≤β. The achievement of von Neumann was to prove that α=β. The minimax theorem means that equality must hold throughout equation (5). For us, the striking thing about the proof is that it uses exactly the same mathematics as the theory of linear programming. X and Y are playing “dual” roles. They are both choosingstrategiesfromthe“feasibleset”ofprobabilityvectors: x ≥0,∑x =1,y ≥0, i i i ∑y =1. What is amazing is that even von Neumann did not immediately recognize the i twotheoriesasthesame. (Heprovedtheminimaxtheoremin1928,linearprogramming began before 1947, and Gale, Kuhn, and Tucker published the first proof of duality in 1951—based on von Neumann’s notes!) We are reversing history by deducing the minimax theorem from duality. Briefly, the minimax theorem can be proved as follows. Let b be the column vector of m 1s, and c be the row vector of n 1s. These linear programs are dual: (P) minimize cx (D) maximize yb subject to Ax≥b, x≥0 subject to yA≤c, y≥0. To make sure that both problems are feasible, add a large number α to all entries of A. This cannot affect the optimal strategies, since every payoff goes up by α. For the resulting matrix, which we still denote by A, y=0 is feasible in the dual and any large x is feasible in the primal. The duality theorem of linear programming guarantees optimal x∗ and y∗ with cx∗ = y∗b. Becauseofthe1sinbandc,thismeansthat∑x∗=∑y∗=S. DivisionbySchanges i i the sums to 1—and the resulting mixed strategies x∗/S and y∗/S are optimal. For any other strategies x and y, Ax∗ ≥b implies yAx∗ ≥yb=1 and y∗A≤c implies y∗Ax≤cx=1. The main point is that y∗Ax ≤ 1 ≤ yAx∗. Dividing by S, this says that player X cannot win more than 1/S against the strategy y∗/S, and player Y cannot lose less than 1/S against x∗/S. Those strategies give maximin = minimax =1/S. Real Games This completes the theory, but it leaves a natural question: Which ordinary games are actually equivalent to “matrix games”? Do chess and bridge and poker fit into von Neumann’s theory? Ithinkchessdoesnotfitverywell,fortworeasons. Astrategyforblackmustinclude a decision on how to respond to white’s first play, and second play, and so on to the end of the game. X and Y have billions of pure strategies. I do not see much of a role for chance. If white can find a winning strategy or if black can find a drawing strategy— neither has ever been found—that would effectively end the game of chess. You could play it like tic-tac-toe, but the excitement would go away. Bridge does contain some deception—as in a finesse. It counts as a matrix game, but m and n are again fantastically big. Perhaps separate parts of bridge could be analyzed for an optimal strategy. The same is true in baseball, where the pitcher and batter try to outguess each other on the choice of pitch. (Or the catcher tries to guess when the runner will steal. A pitchout every time will walk the batter, so there must be an optimal frequency—depending on the base runner and on the situation.) Again a small part of the game could be isolated and analyzed. On the other hand, blackjack is not a matrix game (in a casino) because the house follows fixed rules. My friend Ed Thorp found a winning strategy by counting high cards—forcing more shuffling and more decks at Las Vegas. There was no element of chance, and no mixed strategy x∗. The best-seller Bringing Down the House tells how MIT students made a lot of money (while not doing their homework). There is also the Prisoner’s Dilemma, in which two accomplices are separately of- fered the same deal: Confess and you are free, provided your accomplice does not con- fess (the accomplice then gets 10 years). If both confess, each gets 6 years. If neither confesses,onlyaminorcrime(2yearseach)canbeproved. Whattodo? Thetemptation to confess is very great, although if they could depend on each other they would hold out. This is not a zero-sum game; both can lose. Oneexampleofamatrixgameispoker. Bluffingisessential,andtobeeffectiveithas to be unpredictable. (If your opponent finds a pattern, you lose.) The probabilities for and against bluffing will depend on the cards that are seen, and on the bets. In fact, the number of alternatives again makes it impractical to find an absolutely optimal strategy x∗. A good poker player must come pretty close to x∗, and we can compute it exactly if we accept the following enormous simplification of the game: X is dealt a jack or a king, with equal probability, and Y always gets a queen. X can fold and lose the $1 ante, or bet an additional $2. If X bets, Y can fold and lose $1, or match the extra $2 and see if X is bluffing. Then the higher card wins the $3 from the opponent. SoY has two possibilities, reacting to X (who has four strategies): Strategies (Row 1) If X bets,Y folds. forY (Row 2) If X bets,Y matches the extra $2. (1) Bet the extra $2 on a king and fold on a jack. Strategies (2) Bet the extra $2 in either case (bluffing). for X (3) Fold in either case, and lose $1 (foolish). (4) Fold on a king and bet on a jack (foolish)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. How will the optimal strategies in the game that opens this section be affected if the $20isincreasedto$70? Whatisthevalue(theaveragewinforX)ofthisnewgame? (cid:163) (cid:164)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. With payoff matrix A= 1 2 , explain the calculation by X of the maximin and byY 3 4 of the minimax. What strategies x∗ and y∗ are optimal?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. If a is the largest entry in its row and the smallest in its column, why will X always ij choose column j and Y always choose row i (regardless of the rest of the matrix)? Show that the preceding problem had such an entry, and then construct an A without one. (cid:163) (cid:164)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. ComputeY’s best strategy by weighting the rows of A= 3 4 1 with y and 1−y. X 2 0 3 will concentrate on the largest of the components 3y+2(1−y), 4y, and y+3(1−y). Find the largest of those three (depending on y) and then find the y∗ between 0 and 1 that makes this largest component as small as possible."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. With the same A as in Problem 4, find the best strategy for X. Show that X uses only the two columns (the first and third) that meet at the minimax point in the graph."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6. Find both optimal strategies, and the value, if (cid:34) (cid:35) 1 0 −1 A= . −2 −1 2 (cid:163) (cid:164)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. Suppose A = a b . What weights x and 1−x will give a column of the form c d 1 1 [u u]T, and what weights y and 1−y on the two rows will give a new row [v v]? 1 1 Show that u=v."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8. Find x∗, y∗ and the value v for   1 0 0   A=0 2 0. 0 0 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. Compute min max (x y +x y ). 1 1 2 2 y≥0 x ≥0 i 1 y +y =1x +x =1 1 2 1 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "10. Explain each of the inequalities in equation (5). Then, once the minimax theorem has turned them into equalities, derive (again in words) the saddle point equations (4)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11. Show that x∗ = (1,1,0,0) and y∗ = (1,1) are optimal strategies in our simplified 2 2 2 2 version of poker, by computing yAx∗ and y∗Ax and verifying the conditions (4) for a saddle point."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "12. Has it been proved that no chess strategy always wins for black? This is certainly true when the players are given two moves at a time; if black had a winning strategy, white could move a knight out and back and then follow that strategy, leading to the impossible conclusion that both would win."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13. IfX choosesaprimenumberandsimultaneouslyY guesseswhetheritisoddoreven (with gain or loss of $1), who has the advantage?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "14. If X is a quarterback, with the choice of run or pass, andY can defend against a run or a pass, suppose the payoff (in yards) is (cid:34) (cid:35) 2 8 defense against run A= 6 −6 defense against pass. run pass What are the optimal strategies and the average gain on each play? A Appendix Intersection, Sum, and Product of Spaces A.1 The Intersection of Two Vector Spaces New questions arise from considering two subspaces V and W, not just one. We look firstatthevectorsthatbelongtobothsubspaces. This“intersection”V∩Wisasubspace of those subspaces: If V and W are subspaces of one vector space, so is their intersection V∩W. The vectors belonging to both V and W form a subspace. Suppose x and y are vectors in V and also in W. Because V and W are vector spaces in their own right, x+y and cx are in V and in W. The results of addition and scalar multiplication stay within the intersection. Two planes through the origin (or two “hyperplanes” in Rn) meet in a subspace. The intersection of several subspaces, or infinitely many, is again a subspace. Example 1. The intersection of two orthogonal subspaces V and W is the one-point subspace V∩W={0}. Only the zero vector is orthogonal to itself. Example 2. Suppose V and W are the spaces of n by n upper and lower triangular matrices. The intersection V∩W is the set of diagonal matrices—belonging to both triangular subspaces. Adding diagonal matrices, or multiplying by c, leaves a diagonal matrix. Example 3. SupposeVisthenullspaceofA,andWisthenullspaceofB. ThenV∩W is the smaller nullspace of the larger matrixC: (cid:34) (cid:35) A Intersection of nullspaces N(A)∩N(B) is the nullspace ofC = . B Cx=0 requires both Ax=0 and Bx=0. So x has to be in both nullspaces. 460 AppendixA Intersection,Sum,andProductofSpaces A.2 The Sum of Two Vector Spaces Usually, after discussing the intersection of two sets, it is natural to look at their Union. With vector spaces, this is not natural. The union V∪W of two subspaces will not in general be a subspace. If V and W are the x-axis and the y-axis in the plane, the two axes together are not a subspace. The sum of (1,0) and (0,1) is not on either axis. We do want to combine V and W. In place of their union we turn to their sum. Definition. If V and W are both subspaces of a given space, so is their sum. V+W contains all combinations v+w, where v is in V and w is in W. V+W is the smallest vector space that contains both V and W. The sum of the x-axis and the y-axis is the whole x-y plane. So is the sum of any two different lines, perpendicular or not. If V is the x-axis and W is the 45° line x=y, then any vector like (5,3) can be split into v+w=(2,0)+(3,3). Thus V+W is all of R2. Example 4. Suppose V and W are orthogonal complements in Rn. Then their sum is V+W=Rn. Every x is the sum of its projections in V and W. Example 5. If V is the space of upper triangular matrices, and W is the space of lower triangular matrices, then V+W is the space of all matrices. Every n by n matrix can be written as the sum of an upper and a lower triangular matrix—in many ways, because the diagonals are not uniquely determined. Thesetriangularsubspaceshavedimensionn(n+1)/2. ThespaceV+Wofallmatri- ces has dimension n2. The space V∩W of diagonal matrices has dimension n. Formula (3) below becomes n2+n=n(n+1)/2+n(n+1)/2. Example 6. IfVisthecolumnspaceofA,andWisthecolumnspaceofB,thenV+W is the column space of the larger matrix [A B]. The dimension of V+W may be less than the combined dimensions of V and W (because these two spaces might overlap): Sum of column spaces dim(V+W)=rank of [A B]. (1) The computation of V∩W is more subtle. For the intersection of column spaces, a good method is to put bases for V and W in the columns of A and B. The nullspace of [A B] leads to V∩W (see Problem 9). Those spaces have the same dimension (the nullity of [A B]). Combining with dim(V+W) gives dim(V+W)+dim(V∩W)=rank of [A B]+nullity of [A B]. (2) We know that the rank plus the nullity (counting pivot columns plus free columns) al- waysequalsthetotalnumberofcolumns. When[A B]hask+(cid:96)columns,withk=dimV and (cid:96)=dimW, we reach a neat conclusion: Dimension formula dim(V+W)+dim(V∩W)=dim(V)+dim(W). (3) Not a bad formula. The overlap of V and W is in V∩W. AppendixA Intersection,Sum,andProductofSpaces 461 A.3 The Cartesian Product of Two Vector Spaces If V has dimension n, and W has dimension q, their Cartesian product V×W has di- mension n+q. Definition. V×W contains all pairs of vectors x=(v,w). Adding (v,w) to (v∗,w∗) in this product space gives (v+v∗,w+w∗). Multiplying by c gives (cv,cw). All operations in V×W are a component at a time. Example 7. TheCartesianproductofR2 andR3 isverymuchlikeR5. Atypicalvector x in R2×R3 is ((1,2),(4,6,5)): one vector from R2 and one from R3. That looks like (1,2,4,6,5) in R5. Cartesianproductsgonaturallywithblockmatrices. FromR5toR5,wehaveordinary 5 by 5 matrices. On the product space R2×R3, the natural form of a matrix is a 5 by 5 block matrix M: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) R2 to R2 R3 to R2 2 by 2 2 by 3 A B M = = = . R2 to R3 R3 to R3 3 by 2 3 by 3 C D Matrix-vector multiplication produces (Av+Bw,Cv+Dw). Not too fascinating. A.4 The Tensor Product of Two Vector Spaces Somehow we want a product space that has dimension n times q. The vectors in this “tensor product” (denoted ⊗) will look like n by q matrices. For the tensor product R2⊗R3, the vectors will look like 2 by 3 matrices. The dimension of R2×R3 is 5, but the dimension of R2⊗R3 is going to be 6. Start with v = (1,2) and w = (4,6,5) in R2 and R3. The Cartesian product just puts them next to each other as (v,w). The tensor product combines v and w into the rank 1 matrix vwT: (cid:34) (cid:35) (cid:34) (cid:35) (cid:104) (cid:105) 1 4 6 5 Column times row v⊗w=vwT 4 6 5 = . 2 8 12 10 All the special matrices vwT belong to the tensor product R2⊗R3. The product space is spanned by those vectors v⊗w. Combinations of rank-1 matrices give all 2 by 3 matrices, so the dimension of R2⊗R3 is 6. Abstractly: The tensor product V⊗W is identified with the space of linear transformations from V to W. If V is only a line in R2, and W is only a line in R3, then V⊗W is only a “line in matrix space.” The dimensions are now 1×1 = 1. All the rank-1 matrices vwT will be multiples of one matrix. 462 AppendixA Intersection,Sum,andProductofSpaces Basis for the Tensor Product. When V is R2 and W is R3, we have a standard basis for all 2 by 3 matrices (a six-dimensional space): (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 Basis . 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 That basis for R2⊗R3 was constructed in a natural way. I started with the standard basis v = (1,0) and v = (0,1) for R2. Those were combined with the basis vectors 1 2 w = (1,0,0), w = (0,1,0), and w = (0,0,1) in R3. Each pair v ⊗w corresponds to 1 2 3 i j one of the six basis vectors (2 by 3 matrices above) in the tensor product V⊗W. This construction succeeds for subspaces too: Basis: Suppose V and W are subspaces of Rm and Rp with bases v ,...,v 1 n and w ,...,w . Then the nq rank-1 matrices v wT are a basis for V⊗W. 1 q i j V⊗W is an nq-dimensional subspace of m by p matrices, An algebraist would match this matrix construction to the abstract definition of V⊗W. Then tensor products can go beyond the specific case of column vectors. A.5 The Kronecker Product A⊗B of Two Matrices An m by n matrix A transforms any vector v in Rn to a vector Av in Rm, Similarly, a p by q matrix B transforms w to Bw. The two matrices together transform vwT to AvwTBT. This is a linear transformation (of tensor products) and it must come from a matrix. What is the size of that matrix A⊗B? It takes the nq-dimensional space Rn⊗Rq to the mp-dimensional space Rm⊗Rp. Therefore the matrix has shape mp by nq. We will write this Kronecker product (also called tensor product) as a block matrix:   a B a B ··· a B 11 12 1n   Kronecker product a B a B ··· a B 21 22 2n A⊗B= . (4) mp rows, nq columns  · · ··· ·  a B a B ··· a B m1 m2 mn Notice the special structure of this matrix! A lot of important block matrices have that Kronecker form. They often come from two-dimensional applications, where A is a “matrix in the x-direction” and B is acting in the y-direction (examples below). If A and B are square, so m=n and p=q, then the big matrix A⊗B is also square. Example 8. (Finite differences in the x and y directions) Laplace’s partial differen- tial equation −∂2u/∂x2−∂2u/∂y2 = 0 is replaced by finite differences, to find values for u on a two-dimensional grid. Differences in the x-direction add to differences in the y-direction, connecting five neighboring values of u: AppendixA Intersection,Sum,andProductofSpaces 463 b b b b −1 b b b −1 b b u i+1,j +2u i,j u i 1,j − bb1 2 b − b1 + bb 2 b b −→ − bb1 4 b − b1 − u i,j+1+2u i,j − u i− ,j 1 − − − = 0 b b b b −1 b b b −1 b b x-differences y-differences sum A5-pointequationiscenteredateachoftheninemeshpoints. The9by9matrix(call it A ) is constructed from the 3 by 3 “1D” matrix for differences along a line: 2D     2 −1 0 1 0 0 Difference matrix   Identity matrix   A=−1 2 −1 I=0 1 0. in one direction in other direction 0 −1 2 0 0 1 Kronecker products produce three 1D differences along three lines, up or across:   2I −I 0   One direction A⊗I =−I 2I −I. 0 −I 2I   A 0 0   Other direction I⊗A=0 A 0. 0 0 A   A+2I −I 0   Both directions A =(A⊗I)+(I⊗A)= −I A+2I −I . 2D 0 −I A+2I Thesum(A⊗I)+(I⊗A)isthe9by9matrixforLaplace’sfive-pointdifferenceequation (Section 1.7 was for 1D and Section 7.4 mentioned 2D). The middle row of this 9 by 9 matrix shows all five nonzeros from the five-point molecule: (cid:104) (cid:105) Away from boundary Row 5 of A = 0 −1 0 −1 4 −1 0 −1 0 . 2D Example 9. (The Fourier matrix in 2D) The one-dimensional Fourier matrix F is the most important complex matrix in the world. The Fast Fourier Transform in Section"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.5 is a quick way to multiply by that matrix F. So the FFT transforms “time domain to frequency domain” for a 1D audio signal. For images we need the 2D transform: Transform along each row, Fourier matrix in 2D F =F⊗F = 2D then down each column Theimageisatwo-dimensionalarrayofpixelvalues. ItistransformedbyF intoatwo- 2D dimensionalarrayofFouriercoefficients. Thatarraycanbecompressedandtransmitted 464 AppendixA Intersection,Sum,andProductofSpaces and stored. Then the inverse transform brings us back from Fourier coefficients to pixel values. We need to know the inverse rule for Kronecker products: The inverse of the matrix A⊗B is the matrix A−1⊗B−1. The FFT also speeds up the 2D inverse transform! We just invert in one direction fol- lowed by the other direction. We are adding ∑∑c eikxei(cid:96)y over k and then (cid:96). k(cid:96) TheLaplacedifferencematrixA =(A⊗I)+(I⊗A)hasnosimpleinverseformula. 2D That is why the equation A u = b has been studied so carefully. One of the fastest 2D methods is to diagonalize A by using its eigenvector matrix (which is the Fourier sine 2D matrix S⊗S, very similar to F ). The eigenvalues of A come immediately from the 2D 2D eigenvalues of A : 1D The n2 eigenvalues of (A⊗I)+(I⊗B) are all the sumsλ(A)+λ (B). i j The n2 eigenvalues of A⊗B are all the productsλ(A)λ (B). i j If A and B are n by n, the determinant of A⊗B (the product of its eigenvalues) is (detA)n(detB)n. The trace of A⊗B is (trace A)(trace B). This appendix illustrates both “pure linear algebra” and its crucial applications! Problem Set A"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. Suppose S and T are subspaces of R13, with dimS=7 and dimT=8. (a) What is the largest possible dimension of S∩T? (b) What is the smallest possible dimension of S∩T? (c) What is the smallest possible dimension of S+T? (d) What is the largest possible dimension of S+T?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. What are the intersections of the following pairs of subspaces? (a) The x-y plane and the y-z plane in R3, (b) The line through (1,1,1) and the plane through (1,0,0) and (0,1,1). (c) The zero vector and the whole space R3. (d) The plane S perpendicular to (1,1,0) and perpendicular to (0,1,1) in R3. What are the sums of those pairs of subspaces?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. Withinthe space of all 4 by 4 matrices, let V be the subspace of tridiagonal matrices and W the subspace of upper triangular matrices. Describe the subspace V+W, whose members are the upper Hessenberg matrices. What is V∩W? Verify formula (3). AppendixA Intersection,Sum,andProductofSpaces 465"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. If V∩W contains only the zero vector, then equation (3) becomes dim(V+W) = dimV+dimW. Check this when V is the row space of A, W is the nullspace of A, and the matrix A is m by n of rank r. What are the dimensions?"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. Give an example in R3 for which V∩W contains only the zero vector, but V is not orthogonal to W."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6. If V∩W = {0}, then V+W is called the direct sum of V and W, with the special notation V⊕W. If V is spanned by (1,1,1) and (1,0,1), choose a subspace W so that V⊕W=R3, Explain why any vector x in the direct sum V⊕W can be written in one and only one way as x=v+w (with v in V and w in W)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. Find a basis for the sum V+W of the space V spanned by v = (1,1,0,0), v = 1 2 (1,0,1,0) and the space W spanned by w = (0,1,0,1), w = (0,0,1,1). Find also 1 2 the dimension of V∩W and a basis for it."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "8. Prove from equation (3) that rank(A+B)=rank(A)+rank(B)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. TheintersectionC(A)∩C(B)matchesthenullspaceof[A B]. Eachy=Ax =Bx 1 2 inthecolumnspacesofbothAandBmatchesx=(x ,−x )inthenullspace,because 1 2 [A B]x=Ax −Bx =0. Checkthaty=(6,3,6)matchesx=(1,1,−2,−3),andfind 1 2 the intersection C(A)∩C(B), for     1 5 3 0     A=3 0 B=0 1. 2 4 0 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "10. Multiply A⊗B times A−1⊗B−1 to get AA−1⊗BB−1 =I⊗I =I . 2D (cid:163) (cid:164)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11. What is the 4 by 4 Fourier matrix F =F⊗F for F = 1 1 ? 2D 1 −1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "12. Suppose Ax=λ(A)x and By=λ(B)y. Form a long column vector z with n2 compo- nents, x y, then x y, and eventually x y. Show that z is an eigenvector for (A⊗I)z= 1 2 n λ(A)z and (A⊗B)z=λ(A)λ(B)z."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13. Whatwouldbetheseven-pointLaplacematrixfor−u −u −u =0? This“three- xx yy zz dimensional” matrix is built from Kronecker products using I and A . 1D B Appendix The Jordan Form Given a square matrix A, we want to choose M so that M−1AM is as nearly diagonal as possible. In the simplest case, A has a complete set of eigenvectors and they become the columns of M—otherwise known as S. The Jordan form is J = M−1AM = Λ; it is constructed entirely from 1 by 1 blocks J = λ, and the goal of a diagonal matrix is i i completelyachieved. Inthemoregeneralandmoredifficultcase, someeigenvectorsare missing and a diagonal form is impossible. That case is now our main concern. We repeat the theorem that is to be proved: If a matrix A has s linearly independent eigenvectors, then it is similar to a matrix J that is in Jordan form, with s square blocks on the diagonal:   J 1 J =M−1AM =  ...  . J s Eachblockhasoneeigenvector,oneeigenvalue,andisjustabovethediagonal:   λ 1 i    · ·  J = . i  · 1 λ i An example of such a Jordan matrix is (cid:34) (cid:35)      8 1 8 1 0 0 0 J   1    0 8    0 8 0 0 0  (cid:34) (cid:35)          J =0 0 0 1 0= 0 1 = J . 2       0 0 0 0 0  0 0     (cid:104) (cid:105) 0 0 0 0 0 0 J 3 The double eigenvalueλ=8 has only a single eigenvector, in the first coordinate direc- tion e = (1,0,0,0,0); as a result, λ = 8 appears only in a single block J . The triple 1 1 AppendixB TheJordanForm 467 eigenvalue λ = 0 has two eigenvectors, e and e , which correspond to the two Jordan 3 5 blocks J and J . If A had 5 eigenvectors, all blocks would be 1 by 1 and J would be 2 3 diagonal. The key question is this: If A is some other 5 by 5 matrix, under what conditions will its Jordan form be this same J? When will there exist an M such that M−1AM =J? As a firstrequirement, anysimilarmatrixAmustsharethesameeigenvalues8, 8, 0, 0, 0. But the diagonal matrix with these eigenvalues is not similar to J—and our question really concerns the eigenvectors. To answer it, we rewrite M−1AM =J in the simpler form AM =MJ:      8 1         0 8       Ax x x x x =x x x x x  0 1 . 1 2 3 4 5 1 2 3 4 5          0 0  0 Carrying out the multiplications a column at a time, Ax =8x and Ax =8x +x (1) 1 1 2 2 1 Ax =0x and Ax =0x +x and Ax =0x . (2) 3 3 4 4 3 5 5 Now we can recognize the conditions on A. It must have three genuine eigenvectors, just as J has. The one withλ=8 will go into the first column of M, exactly as it would have gone into the first column of S: Ax = 8x , The other two, which will be named 1 1 x and x , go into the third and fifth columns of M: Ax = Ax = 0. Finally there must 3 5 3 5 be two other special vectors, the generalized eigenvectors x and x . We think of x as 2 4 2 belonging to a string of vectors, headed by x and described by equation (1). In fact, 1 x is the only other vector in the string, and the corresponding block J is of order 2. 2 1 Equation (2) describes two different strings, one in which x follows x , and another in 4 3 which x is alone; the blocks J and J are 2 by 2 and 1 by 1. 5 2 3 The search for the Jordan form of A becomes a search for these strings of vectors, each one headed by an eigenvector: For every i, either Ax =λx or Ax =λx +x . (3) i i i i i i i−1 The vectors x go into the columns of M, and each string produces a single block in J. i Essentially, we have to show how these strings can be constructed for every matrix A. Then if the strings match the particular equations (1) and (2), our J will be the Jordan form of A. I think that Filippov’s idea makes the construction as clear and simple as possible1. It proceeds by mathematical induction, starting from the fact that every 1 by 1 matrix 1A.F,Filippov, AshortproofofthereductiontoJordanform, MoscowUniv. Math. Bull., volume26(1971) pp. 70–71. 468 AppendixB TheJordanForm is already in its Jordan form. We may assume that the construction is achieved for all matrices of order less than n—this is the “induction hypothesis”—and then explain the steps for a matrix of order n. There are three steps, and after a general description we apply them to a specific example. Step 1. If we assume A is singular, then its column space has dimension r<n. Looking onlywithinthissmallerspace,theinductionhypothesisguaranteesthataJordan form is possible—there must be r independent vectors w in the column space i such that either Aw =λw or Aw =λw +w . (4) i i i i i i i−1 Step 2. SupposethenullspaceandthecolumnspaceofAhaveanintersectionofdimen- sion p. Of course, every vector in the nullspace is an eigenvector corresponding to λ = 0. Therefore, there must have been p strings in step 1 that started from this eigenvalue, and we are interested in the vectors w that come at the end of i these strings. Each of these p vectors is in the column space, so each one is a combination of the columns of A: w =Ay for some y . i i i Step 3. The nullspace always has dimension n−r. Therefore, independent from its p-dimensional intersection with the column space, it must contain n−r− p additional basis vectors z lying outside that intersection. i Now we put these steps together to give Jordan’s theorem: The r vectors w , the p vectors y , and the n−r− p vectors z form Jordan i i i strings for the matrix A, and these vectors are linearly independent. They go into the columns of M, and J =M−1AM is in Jordan form. If we want to renumber these vectors as x ,...,x , and match them to equation (3), 1 n then each y should be inserted immediately after the w it came from; it completes a i i string in which λ = 0. The z’s come at the very end, each one alone in its own string; i again the eigenvalue is zero, since the z’s lie in the nullspace. The blocks with nonzero eigenvalues are already finished at step 1, the blocks with zero eigenvalues grow by one row and column at step 2, and step 3 contributes any 1 by 1 blocks J =[0]. i Now we try an example, and to stay close to the previous pages we take the eigenval- ues to be 8, 8, 0, 0, 0:   8 0 0 8 8   0 0 0 8 8   A=0 0 0 0 0.   0 0 0 0 0 0 0 0 0 8 Step 1. Thecolumnspacehasdimensionr=3,andisspannedbythecoordinatevectors e ,e ,e . To look within this space we ignore the third and fourth rows and 1 2 5 AppendixB TheJordanForm 469 columns of A; what is left has eigenvalues 8, 8, 0, and its Jordan form comes from the vectors       8 0 0       0 1 8       w =0, w =0, w =0. 1 2 3       0 0 0 0 1 0 The w are in the column space, they complete the string for λ = 8, and they i start the string forλ=0: Aw =8w , Aw =8w +w , Aw =0w . (5) 1 1 2 2 1 3 3 Step 2. The nullspace of A contains e and e , so its intersection with the column space 2 3 is spanned by e . Therefore p=1 and, as expected, there is one string in equa- 2 tion (3) corresponding toλ=0. The vector w comes at the end (as well as the 3 beginning) of that string, and w =A(e −e ). Therefore y=e −e . 3 4 1 4 1 Step 3. The example has n−r− p = 5−3−1 = 1, and z = e is in the nullspace but 3 outside the column space. It will be this z that produces a 1 by 1 block in J. If we assemble all five vectors, the full strings are Aw =8w , Aw =8w +w , Aw =0w , Ay=0y+w , Az=0z. 1 1 2 2 1 3 3 3 Comparingwithequations(1)and(2), wehaveaperfectmatch—theJordanformofour example will be exactly the J we wrote earlier. Putting the five vectors into the columns of M must give AM =MJ, or M−1AM =J:   8 0 0 −1 0   0 1 8 0 0   M =0 0 0 0 1.   0 0 0 1 0 0 1 0 0 0 We are sufficiently trustful of mathematics (or sufficiently lazy) not to multiply out M−1AM. In Filippov’s construction, the only technical point is to verify the independence of the whole collection w , y , and z . Therefore, we assume that some combination is zero: i i i ∑c w +∑d y +∑g z =0. (6) i i i i i i Multiplying by A, and using equations (4) for the w as well as Az =0, i i   λw i i   ∑c  or +∑d Ay =0. (7) i i i λw +w i i i−1 470 AppendixB TheJordanForm The Ay are the special w at the ends of strings corresponding to λ =0, so they cannot i i i appear in the first sum. (They are multiplied by zero in λw .) Since equation (7) is i i somecombinationofthew ,whichwereindependentbytheinductionhypothesis—they i supplied the Jordan form within the column space—we conclude that each d must be i zero. Returning to equation (6), this leaves ∑c w =−∑g z , and the left-hand side is in i i i i the column space. Since the z’s were independent of that space, each g must be zero. i Finally, ∑c w =0, and the independence of the w produces c =0. i i i i IftheoriginalAhadnotbeensingular,thethreestepswouldhavebeenappliedinstead toA(cid:48)=A−cI. (TheconstantcischosentomakeA(cid:48) singular,anditcanbeanyoneofthe eigenvaluesofA.) ThealgorithmputsA(cid:48) intoitsJordanformM−1A(cid:48)M=J(cid:48) byproducing the strings x from the w , y and z . Then the Jordan form for A uses the same strings i i i i and the same M: M−1AM =M−1A(cid:48)M+M−1cM =J(cid:48)+cI =J. This completes the proof that every A is similar to some Jordan matrix J. Except for a reordering of the blocks, it is similar to only one such J; there is a unique Jordan form for A. Thus, the set of all matrices is split into a number of families, with the following property: All the matrices in the same family have the same Jordan form, and they are all similar to each other (and to J), but no matrices in different families are similar. In every family, J is the most beautiful—if you like matrices to be nearly diagonal. With this classification into families, we stop. Example 1.   0 1 2   A=0 0 1 with λ=0,0,0. 0 0 0 This matrix has rank r =2 and only one eigenvector. Within the column space, there is a single string w , w , which happens to coincide with the last two columns: 1 2       1 2 1       A0=0 and A1=0, 0 0 0 or Aw =0 and Aw =0w +w . 1 2 2 1 The nullspace lies entirely within the column space, and it is spanned by w . Therefore 1 p=1 in step 2, and the vector y comes from the equation     2 0     Ay=w =1, where solution is y=0. 2 0 1 AppendixB TheJordanForm 471 Finally, the string w , w , y goes into the matrix M: 1 2     1 2 0 0 1 0     M =0 1 0, and M−1AM =0 0 1=J. 0 0 1 0 0 0 Application to du/dt =Au As always, we simplify the problem by uncoupling the unknowns. This uncoupling is complete only when there is a full set of eigenvectors, and u = Sv; the best change of variablesinthepresentcaseisu=Mv. ThisproducesthenewequationMdv/dt =AMv, or dv/dt = Jv, which is as simple as the circumstances allow. It is coupled only by the off-diagonal 1s within each Jordan block. In the preceding example, which has a single block, du/dt =Au becomes   0 1 0 da/dt =b a=a +b t+c t2/2 0 0 0 dv   =0 0 1v or db/dt =c or b= b +c t 0 0 dt 0 0 0 dc/dt =0 c= c . 0 The system is solved by working upward from the last equation, and a new power of t enters at every step. (An (cid:96) by (cid:96) block has powers as high as t(cid:96)−1.) The exponentials of J, in this case and in the earlier 5 by 5 example, are   e8t te8t 0 0 0     1 t t2/2  0 e8t 0 0 0     eJt =0 1 t  and  0 0 1 t 0.   0 0 1  0 0 0 1 0 0 0 0 0 1 Youcanseehowthecoefficientsofa, b,andcappearinthefirstexponential. Andinthe secondexample,youcanidentifyallfiveofthe“specialsolutions”todu/dt =Au. Three ofthem arethe pureexponentials u =e8tx , u =e0tx , and u =e0tx , formedas usual 1 1 3 3 5 5 from the three eigenvectors of A. The other two involve the generalized eigenvectors x 2 and x : 4 u =e8t(tx +x ) and u =e0t(tx +x ). (8) 2 1 2 4 3 4 The most general solution to du/dt = Au is a combination c u +···+c u , and the 1 1 5 5 combination that matches u at timet =0 is again 0 u =c x +···+c x , or u =Mc, or c=M−1u . 0 1 1 5 5 0 0 Thisonlymeansthatu=MeJtM−1u ,andthattheS andΛintheoldformulaSeΛtS−1u 0 0 have been replaced by M and J. 472 AppendixB TheJordanForm Problem Set B"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. Find the Jordan forms (in three steps!) of   (cid:34) (cid:35) 0 1 2 1 1   A= and B=0 0 0. 1 1 0 0 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. Show that the special solution u in equation (17) does satisfy du/dt = Au, exactly 2 because of the string Ax =8x , Ax =8x +x . 1 1 7 7 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. For the matrix B in Problem 1, use MeJtM−1 to compute the exponential eBt, and compare it with the power series I+Bt+(Bt)2/2!+···."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. Show that each Jordan block J is similar to its transpose, JT = P−1JP, using the i i i permutation matrix P with 1s along the cross-diagonal (lower left to upper right). Deduce that every matrix is similar to its transpose."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. Find “by inspection” the Jordan forms of   (cid:34) (cid:35) 1 2 3   1 1 A=0 4 5 and B= . −1 −1 0 0 6"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6. FindtheJordanformJ andthematrixM forAandB(Bhaseigenvalues1, 1, 1, −1). What is the solution to du/dt =Au, and what is eAt?   0 0 1 0 0   1 −1 0 −1   0 0 0 1 0      0 2 0 1  A=0 0 0 0 1 and B= .   −2 1 −1 1  0 0 0 0 0 2 −1 2 0 0 0 0 0 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. Suppose that A2 =A. Show that its Jordan form J =M−1AM satisfies J2 =J. Since the diagonal blocks stay separate, this means J2 =J for each block; show by direct i i computation that J can only be a 1 by 1 block, J =[0] or J =[1]. Thus, A is similar i i i to a diagonal matrix of 0s and 1s. Note. This is a typical case of our closing theorem: The matrix A can be diagonalized if andonlyiftheproduct(A−λ I)(A−λ I)···(A−λ I),withoutincludinganyrepetitions 1 2 p of the λ’s, is zero. One extreme case is a matrix with distinct eigenvalues; the Cayley- HamiltontheoremsaysthatwithnfactorsA−λI wealwaysgetzero. Theotherextreme is the identity matrix, also diagonalizable (p=1 and A−I =0). The nondiagonalizable (cid:163) (cid:164) matrix A = 1 1 satisfies not (A−I) = 0 but only (A−I)2 = 0—an equation with a 0 1 repeated root. SolutStoe iloencst ed [xercises ProblSeem1t . p2a,g 9e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.T hel inienst ersaet( cxty, ) = (31,) T.h en3( colum1n)+ 1( colum2n)= (4,4)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.Th ese \"planes\" iinan l tienriesnf e ocutr m-ednisionsapla ce. Thep lfaonunero trh­ w malliyn tersects itnah p aoti nlAtin.n i en consiesqtueantti loinku e + = 5 leaves nos olut(inooni ntersection)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.T he tpwooi notnst hep lanaer e( 10,,0 ,0 )a nd( 01,, 0,0 )."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.S olvabfloe(r 3 ,5,a8n)d ( 1,23,)n;o ts olavblfeo br = (3,57,)o rb = (12,,2). vw,)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.C olum3n = 2(colu2m)n- colum1n.I fb= (00,, 0 )t,h e(nu , = (c-,2 c, c)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.Bo tha = 2 anda = -2 givael inoefs olutiAolln ost.h ea rg ivxe = 0,y = 0."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13.Th er owp ictuhraest wol ines meaett( i42n,)g T. h e column phiacs4t (1u ,1r )e+ 2(-21,) 4(colu1m)n+ 2(colu2m)n right-hsaind(de0 6,) . = ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.Th er owp ictusrheo wfso ulri nTehsec. o lumpni ctuirsie n /our-dsiimoennsapla ce. No solutuinolne tshser ight-hsaindides a c ombinatoifto hnetw o columns."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.If x ,y ,z satitshfefiy r sttw oe quatitohnesay,l ssoa titshftheyi redq uatiTohnel. i ne v (!!,) , �v! w, L ofs oluticoonnst ain = s( 11,,0)W, = 1, andu = + anda ll cv w combinatio+n ds witch+ d 1. =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19.Co lum3n colum1n; s oluti(oxn,ys, z ) = (1,1,)0o r( 01,,1)a ndy ouc ana dd == anym ultiopfl e (-01,1, )b; (46,,c )n eeds 1c0 f osro lbviality. = ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21.Th es econd palnadrn oew 2 o ft hmea trainxd a ll coloufmth nesm atriaxr ceh anged. The solutiison no tc hanged. v w"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23.u = 0, = 0; = 1,b ecau1s( ec olum3n) = b. ProblSeem1t a p3a,g 1e5 �o"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.M ultipblyyf = = 5,a nds ubtrtaocfi tn d2 x+ 3y = 1 and- 6y = 6.P ivots 2,- 6. -! !"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.S ubtractt imes equ1a( toiarod nd timeesq uati1o)Tn.h en ews econedq uation is3 y = 3.T heny = 1 andx = 5.I ft her ight-hsadinecd h angseisg ns,od oethse soluti( ox ny, :)= (-5,- 1)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. 6 x+ 4y is2 time3sx + 2y.T heries n os olutiuonnl etshs er ight-hsainddie s"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. 1 ° 20.T hena lplo inotnst he l3ixn+ e 2 y 10a rseo lutiionncsl,u d(i0n5,g ) = = and( 4-, 1)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. I fa = 2,el iminamtuisoftna iTlh.ee quatihoanvsne o s olutiIofa n =. 0 ,e limination stopfso ra rowe xchangTeh.e n3 y = -3 givye =s -1a nd4 x + 6y = 6 gives x 3. ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.6 x - 4yi s2 time(s3 x 2y)T.h erefowree n,e ebd2 = 2b1T•h ent herwei lble - lnflnltAlrrHlu nu C'Al11t1AnTC'h i3 r-nl111'l'1nfC'� A) �nrfl_ 'J _Lt.) <:lri3 An thi3 C'f\".l1\"Y\\O 111'113 SotliuontsoS elteecEdx recises 429 = 11 2. x -3y = 3 2x -3y 3 x= 3 Sutbr2a x c rto 1wf ormr o2w y + 1zg ivye s + 1z n ady 1 Sutrbac1t r o1wf ormr o3w = = = x . = 2y- 32: 2 - 5z ° z ° Subt2r arco2twf ormr o3w = = x"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13.T hsee cpovinodpt o tsiiwoincl olni tn-a 2 h.I bf -2,w ee xchwainhrtg oew = - 3I.bf = -1( nsgiaurcl ae,s)t hsee ceoqnaudt iio-sny - z ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "0. sloutiiso n A (11,, -1). 15I.r fo 1w ro2wt, h reon2w i zse arfott hefierr ssttee xpc;h atnhzgeee rr ooww i th = ro3wa ntdh eirnseo t hpiirtvId.ocf o ulmn coulm2nt ,h eirnseos ecpoinvodt . 1= 17R. ow2b eco3my-e 4sz = 5th,e rno 3wb eco(mq+e 4 sz) = -5I.qf = - 4, t = thseye smit ssig nual-r not hdip rivTohetni.,f 5t,h teh ierqdu ait°si = o0.n t = Chosoizn g thee qua3tyi- o4nz 5 giyv e3sa nd aoetinqu gives = 1, = 1 x= -9. 19T. hsey esmit ssi ngiurflo 3awi r asc mobniatofir oownas n2 dF.r otmhe env di ew, 1 thterh epel afnnnoe sta ar nig.Tl hehipasp einrfso s w +2 ro3wo n ltethhf-ead n 1 = Z= side burti gnhotst-i hdftareohen :eaxd m ep x,l+ ,yz+ = 0, x 2y - 1, - 2x- y = 9.N ot wpol aanrepesra al,bl usetitl lnlos oultoin. �. (n.� l)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21.T hfietf hp iovits T hneth p iviost W = = u + v + 2 u 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23.T rainguslyeasmrt 2 +v 2 w -2 Solutio-n2. v = = = 2w 2 w = 1 w)="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.( ,uv , (/321/,2. -,3.C)hantgole+w oumladkt ehs eye smst niguleaqru al (2 colnus.m)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27.a °r equari orewex sac gnheb,ut ths eye smit nso innsgual 2am ra:ki estsi ngular = = (opnietv i,on fiyon ifst ouiltnosa)= ;- 2m aksie stni gu(loanre psilovuontt)i,.o no"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.T hes ecotnedrb mc+ a di s(a b()+c d )-\" ca- bd( onlayd ditional + 1 mulptltiiic.oa n)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31.E limifniaalftsoia ro n2 ( eqlcu loaunmsa) ,4 (erqwous,aa) l= °( zero = = colnu.)m Porblem pgae Se1t4 m, 26 5 7 [] 2 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. 4 ,- 2, .W iths idteo,s 1 (a)2n (d0,3 )th,ep alrlaelgoogtero(sa 2m4 ,.) 4 3 17 3 5 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.I nnperro duacnt0ds,c lo5u4mt ni mesgv iers-o w6 -10 -2. 21 35 7"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.A x= ( ,00 ,0) s,o x = (21, 1, ) i ass louito;on thseourlt iso nacrx = e( 2,cc c,.) ° ° 3 4 3 1 1 1 4 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.E xmapslD:ei agon2a l0 , symm3e 2t r0i ,tc r ian0g u2l 0a ,r 0 ° 4 0 ° 0 7 7 7 0 3 4 .>:��: sk-eswmymercti -30 0. -4 0 ° ontso S elteecEdxe rcises a l'l .( )aa ll (b i) li= aiall/l (c n) e waiija si -j- -alj 9 all a21 (ds)e cpoinvad 2o 2-t -- a1'2 all"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.T hceo efficoifre onswot fBs a r2e,1 ,4fr omA. T hfiert rs oowAf B i[s 63. ] _�}� �. � . [� [J [�J D =n 13A.= B= = = A,E = F= c U O. 15A.B 1= BlAg ivbe =cs = AB2= B Ag isva ed.S oA= al. = 2 17A.( A+ B)+ B(A B),( A+ B)(+B A),A 2+ AB BA B2a lawyesq ual + + + 2 (A+ B). + [p ] [!] :[� ] [: ] �q ] [] !s ] [ :;a �q b!S �"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. = + = 9 [r cq+ ds. [� � � -�t ] l [ 21A.n = A;B n= ( = = zemraot .r ix c 3 -5,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23. E3 2E2=1 b( 1, 35 )b u Et 2 132Eb (1-,5 ,.0T )herno wf eelnsoe ffect - = forirno 1w."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.C hangifnorgm7 t ao131 w 3li clh nagthee t hipridvfr ootm5 t o9. C hnagian3g3 form to2w iclhla tnhgpeei vfrootm5 t no povi ot. 7 1 0 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. O."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27.T or esveeEr 13,a dd7 t imreos1wt ro owT hmea tirsi3 1 =x 0 1 R 7 0 1 1 01 1 0 1 2 0 1 O."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.E 1=3 0 1 ; 00 1 0;E 13 E3 0 1 Tesotnt hied enmtait!tr yi x 1 = 0 0 1 1 0 1 1 0 1 -!, �, -%. 31 E. 2 h 1 a .es 2= 1 E3h 2 a.es3= 2 - E4h3a.e s4= 3 OtherthweEi 'mssae t Ic.h a+b+c= 4 a=2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33.a +2 b+ 4c= 8 give b=s 1 . a +3 b+ 9c= 14 c = 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "35.( )a Ecalocuhmi nEs t imaec sou lmno fB. � ][ (b E)B= ] := � ; ; U U :l Rowso EfB arceo mbinoarfto siwo oBfn, ss o atrhmeeu ylp tloief[s 1 2 ],4 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37.( ro 3)w · i I:s a3jaXnj(,Ad ) 11= (r1o)w·( cuoml1n) I: = a 1j.a j1 x 315 513 3, 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39.BA isb y5,A B = isb y ABD = 5Di sb y1,A BD :N o,A (B + C): = No. 0 0 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41. O. O ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41.( aB) = (bB) (cB) = 0 1 = 1 0 0 O. (dE)v erroyow fB i1s, 0, 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.( a) (eveenryt.y)r (bm)n .p (cn)( thni2ds o ptird suo c)t.s mn 3 3 3 3 1 0 0 0 00 0 [33O J [ J 6 6"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "45.2 + 4 1 2 =1 0 + 4 8 4 10 14 4 . 6 6 8 2 1 0 1 2 1 7 1 Sotloiuntso S elteecEdx ercises 431 B"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "47. timBei ss , A A [ l [[ ]]l [ ][ [ l"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "49.T h(e22 , b) lc okS D C B1i thse bloicnk s Schucro mplement: == - A - d (-cjb a)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "51. times wiblethl e i dteinmtayt Ir ix A == [XlX 2X 3] == [AXIA X2A X3]' X ] ] [ ["
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "53. a+b a+b agreI. the sa+ c W b+d when and + b+ b == c a= d . c d c+ d a+ c d"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "55.2 x 3 y+ z + 5 t 8 i s wtiht h1eb y,m4a trix 3 1 T5he + = Ax= b A == [2 ]. sloutxifio lanl3 s D \"pilfnauo nrde i\"nm seni.so x"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "57.T hdeo ptr odu4c 5t (1b y3 ()b3y 1) i zse frroop oi(xn,t sz o)na [1 ] == Y y, z + plan+4e 5z0 i tnh rdeienm senisTo.hc eo luomnfsa roen dei-mseinonal x = A Y vecrtso."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "59.A [34 and' 50; A givaenes r rmoers sage. * v == 5]' v * v = v * 8 3 4 5+u 5 -u +v 5-v"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "61.M 1 95 5- u - v5 5+ u + v ; = = 6 2 5+v 5+u- v 5 ·-u 7 M3(1,1, 1) (1,1551),5M; 4( ,11, 1, 1 ) ( 3,34,4343,4) btehcneau bumesres == == 1 t o1 a6d tdo1 6 3w,h iic4sh3( 4.) . l' PrboleSme t paeg 56 1.39"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.U i nso nsairwn'hgneun lo eonntht erm ya idni algi ozsnre ao. 1 00 1 00 1 00 1 00 1 o 0 2 ,1 -02 1 00 01 ;-2 1 02 1 0 Ia slo."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. == = - 1\\\\ 1- -11 1 0 0 -111 -11 1- 1 1 \\ (E-1p I G-1 )G( EP) I E-1p - P1 E = E-1 E = I;a l(sGoPE ) ( E-1p -1G1)- = I. 1 00 '2 3 2 3 3u 2 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.L U 1 0 5'0 7 ;a fetleiirnm oant0,i 5 v 2 . == 7 o 0 -0 1 0 -01 -1 1 w 1 0 0 0 1 0 0 0 2 01 0 2 1 0 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. ; FGH == o 2 1 HGF 0 == 4 2 1 O' 0 0 21 8 4 2 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.( aN)o nnsgiuwlhaern i= O.( bS)pu psoe i= 0S:lo ve b going d 1d2d3 d3 Lc == o 0 0 d1 -d1 U downwardbg :i evs 0 .T hen0 0 gives Lc= c = d2 -d2 V 1 o 0 1 d3 W Ijd3 X = Ijd•3 Ijd3 ontso S eelctdeE xreceiss 2 5"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.L c= bg oidnogw nwgairvd=e s- 2 U; x == cu pawrgdi vxe =s - 2 . c 0 o 0 0 2 1 u Permutation 0 0 v - 13 1 - 3 . 2a n3d , 4, • rows 0 1 0 w 0 0 1 1 u permutation 0 0 -1 1 v row1a sn 2d 0, 0 0 1 w O il 0 1 0 1 00 0 0 1 0 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.P A L=D Ui s1 0 0 1 0 1- 0 1 0 o 1 0 o 1 1 · , 0 01 2 31 0 0 01 2 3 4 -1 o 1 00 1 00 0 01 2 1 1 2 1 1 O . P A L=D Ui s0 10 2 42 1 10 o -1 0 o 1 0 1 0 1 1 1 2 01 0 00 0 0 0 1 00 OM.A TLAB"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.L becom1e 1s and cootdhueesPsre A LU. = 2 01 a laO"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19. = 4l eatdors o aew x cnhg;a3e b+ 4l0e atdoass n igurl amraitx;0 c= == leatdaors o ewx chea;=n g3l eatdoass i unlgmaartr .i x c"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21.£ 13 an£d23= 2( £=3 31 )r:ve erssteet pors e cox+ v 3eyr+ 6z= 11fo rm 1 == Ux = 1t im(ex+s y + z5 +) 2 t im(ey+s 2 z 2 +) 1 t im(e=zs 2) g ives c: == == x+3y+6=z11. 1 1 1 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23. 0 1 -2 1 A= 0 2 3- U - . o -2 1 0 0 1 .0 o -6 0 0 1 A= 2 1 0 U = E21 1E 321U = LU. 0 2 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.2 b y2 :d 0n oatl l;o wed = 1 1 0 1 d g d= 1,e = 1,t ehn=.e 1 e 1 1 2 - f h f 0i s anlolto wed - .e 1 == 1 2 1 m 1 . nop ivionrt o w2 . n 1 2 4 8 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27.A = 0 3 has= Ia ndD = 3 ; AL =Uh aUs A (piovno ts 9 L == 0 0 7 7 2 1 4 thdei agoA n=aL DlU)h ;a Us D-1A 0 1 3 witIhos nt hdei agonal. == = 0 01 a a a a 1 a a a a a�O . a b b b 1 1 b-a b-a b-a b�a"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29. .N eed a b c c 1 1 1 c-b c-b c b =1= a b c d 1 1 1 1 d-c d =Fc . 1 1 0 a a 0 a 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31. 1 1 11 = LIU; a a+b b = (smaeL ) b 0 1 1 1 0 b b c + c (smaeU ). 4 1 0 0 4 1 1 1 4 3 . ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33. 1 1 0 5 gIvc=e s 0 x = gIvxe s=0 c = 1 1 1 1 . 1 1 0 0 1 1 1 6 1 1 ,- A = LU."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "35.T h2eb y2u ppseurbt mraBi hxa tshfi er tswtpo i v2o,tR se 7a.sEo lni:mt iiononan A satrittnsh uep pleetcrf o rnweiret lhi mionnBa . t ion 1 11 1 1 4 1 2 3 5"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37. 1 3 6 101 5 4 102 03 5 1 1 5 153 5 70 ,;, 1 1 11 11 Pascatilra'gnsl ei nL andU. 4 1 1 1 23 MATALB' ls uc oed wwirlelc k 1 2 thpeta t.e rn doneosr ow 1 1 3\"6 chal exchafnrosg myemse tric \\ 1 31 3 1 4 4 6 4 1 1 matreiswcti ph otsiipviev .o ts 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39.E achr giht-hsainddce o sotnnsl2 ys tecposmr peatdno 3 /f3o re fluitlmilion na new A\\.b"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41.2 e xcnhga;e3 es xcnhga;e5 s0e xcnhgaaensd 5t1h.e n 0 1 0 1 00 0 01"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.P =0 0 Pi1 0 O landP 2 0= 1 0 = ; 1 00 0 1 0 01 0 ( Pgivceosl ueam xnc h.a nge) 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "45.T here paerremt uimtoaant ricens.E voefn tourapdloelwryoe frPt sm wu sobt e n!"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. thsea mIef:p r = pst hpern- =s Ceritnral y- < s n ! 1 o o C � 0 1, nadP6 o 1 0 o"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "47.T hseo tliuioxsn= (11,., ,. 1 ..T) h en= Pxx. PrboleSmte pgae"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "16., 52 ] l [ l l [ Sine"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.A i �! A2} = [ -�Ail 3= _���: = cos.e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. A-I = Be-I A;- I = U-IL1 p- . nst oS elteedcE xerecsi s 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.A A(B)( mopvareen htes=e (sA))B (=) I. [ = [ [ ] -J3 /2 12/]--J3 /2 12/ 0 1] 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. alhla vAe I. 12/ --J3/2' 12/ -J3/2'1 0 ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.I rfo 3 wo f- I wA er(e,ab c,d, )t ,h Ae n1 A- Iw ouglid2v ea 0 a, 3 =b 0 , + = = + 4a8 b 1T.hs hi ansos l oiuot.n == [�� ][-� _� ][�� [��] [� � ][�"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.(a ) = (b) + l + = �l (c[� ) �] +[ _ b�] = [ � _� l ( - B 1+ A -l )-1 = BA( B)I - + A. l� ��n [ ["
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13.A TB=8;BTA=8A;BT= BAT="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.( an)n( 12)e /niterosna nadob vdei ag.o (nba()l-n l )2ne n/traobivees + dioangla."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.( aT)h ien sveer alo ofw (eurp ptierarn)g mualtarrsii txll loi w(seu rp pternrig)au ­ lraM.u ltilpolwy(eiurnp gpt aernrig)ur ml aaitecrsg ivale osw (eurp ptaergnriu)rl a , 1 l mait.xr T(hbme)a idni agoofLn IaLlDsa dn UD1U ;ares mateh etao hss e 2 2 1 l 1 ofD anDd resipveecLltILyD. = DUU:;,s o hwaevD e D.B y mc­o 2 2 2 2 1 1 1 1== l 1 ,arpintgho eff- idagoonfLa ILlDs = DUU:;,b ohmt aitcrmeussb ted iaglo.n a 22 1 1 LI1 L2D2 == D2, 1UD 1U:;1 == D 1, 1iD isn viebrsltoLe I 1 L 2 == I,U 1 U 2-1 == I. TheLnl= L 2,U 1 U 2• == 1 00 1 00 1 35"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19. 3 1 0 0 13 ; 0 0 1 5 11 0 0 20 01 [ b��[ a] � ] [� b i a] = L DLT. �2 d /a) _"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21.F roBml( A-B=) (I BAB) we( I-g BetA)- l BI(- AB)l B- -1,a n - = expliincsvpieertr o vBia dnedd A Ba rien viebrSlteen.cad op ropaIcIfh B:A I - - inso itn vilebrett,h eBnAx =f rxos moen onrzoxe T.h ereAfBoxAr e=A x,o r ABy y=a, n d AIB c uolndob tei nvelre(t.Ni otbtheya t=A xi nso nz,fe orrmo - BAx=x). , � -:�.;- :71 -�7. - [][ J[][ ] I [ ]"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23. = A-= = SO 10"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.( aI)nA x (010,,,)e quiao1tn equa2t ieouqnai to3ni 0s 1.( b) The = + - = rights-ihdmaeunsssd tai ytsb f b 2 b•3 (cR)o 3wb eocmears o owzf e ros-no 1 + == thipridv ot."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27.I Bfe xcnhgareosw 1as n 2do fA ,th Ben-1 e xchacnogle1usa m n2n do s fA -I . I"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.I Af hacsou lmaon zf e srs,ood oBeA s.S oB A Ii ismp ossi.bT lheeirse -no A == 1 1 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31. 1 1 -1 1 - 1 1 -E', - -1 1 -1 1 1 -110 1 then1 1 iLs E- 1 ,a tferre vetrhsoeir ndogetf rh s e3ee elmenmtaaitrycr e == 1 1 1 + ancdh an-gi1tno g1 . Soutlinost oS elteedcE xeirsces 435"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33. A on4e,sg1(vi) et shz ee ro vAec cnatnoboreti, ne vrsbtoli e. * - 31 31 ] 0 ]3 [; �] [� � � 7 -"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "35. 0 1- 2 [ 1- 12 [I- A -I ]; 7 -+ -+ 31 0 ] � � 8 0 I - [ IA -]. [8 � -+ [1 3 -=i J 1 10 01 01 0 a b a -b 01 01 0 01 ' 001"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37. c 0 100 10� 0 0 1 0 -0c 1 10 10 -a ac-b 01 0 0 1 � 0 000 -1c . 1 20 1 ][0 2 -1 -11//2 ] ] [� � [� �"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39. 21 0< 2 11o 1 /2[ IA -I ]. -+ -+ = o 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41.N oitn vbelfreotc ri 7( euqlac olnu)smc, (eqruwoaslc) , (zecrooln u.)m =: == == 2 1 10 O � 11 0 )"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.A �l 0o 10 T h5eb y5 A-1a l�hs aIsso n t hdei alga onnsdau per- = l ' 0010 daigoln.a 0 ']0 [I ] [ A -I [ D I]"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "45. , ad � 1 · I' -D-C1A- D-1 O - c 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "47.F oArx b witAh on4e,4s )( signurlm aatarnibdx onse(14)A,,\\ b == (==1 ,0== ,0,0== ) wlipli cxk andp invb(w Aipl)il ct ekhs horstoelsuxtt ion (1,== 1,1,* 1)/4. == ] ] [� [_;�' l [�D ;; 1 1/(A-1)T"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "49.A T �Al (TA) AT Aan d - - = = = = = 0 [c] thAe-nI � (-A1 )T (T A)-l . == c2 c -1= ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "51.( ( BA)-) T ( B-1A -l )T (-A1 T () B -T 1;() U -T 1i )l s w oetrrg iuarln.a l == = [�.]"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "53.( ax)AT y an 5. (bx)T A [45 6.] (c)y A = = = ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "55.( xPT)p(y) xTTppy xyTbeucsaep Tp Iu;us lalPyx· y X·pTy=j:. == = = = X·P y: 0101 1 011 01 0021 1. 2 . 0 0 1 1 1003 2=1= 31002"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "57.P A pTr ecotvhseeyr mstrym .e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "59.( )aT htern aspoofRs TAe ,Ri Rs TA TR TT RTA R by = = n n. (b()T R R ) (cuomlno fR ). ( cuomlno fR ) lehns gqtueardo fco lumn j j j. == = jj nst oS elteedcE xrecsies 1 0 1 YBC YBe+ YBS"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "61.T otaclu rreanrA teT Y s = -1 1 0 Ycs YB+C Yes - - 1 - 1 o YBS -Ye-sY BS Eietwrha y( A X)T Y xT (AyT) XBY+BXCB YB-SX CY+Bx Cc Yc- sX sYc-sX SY.B S := :="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "63.A x. Y i s ctohosefit n pwuhtesr,xe. Aa YTsi tsh vea olfou uet .p uts"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "65.T heasrege r po:suL owtearrni guwliarthd igaoln sad,1i agli onnvaleerDt ,ai nbd permtuitoPan.Tsw o m or:Ee vepne rmiuot,naa slntlo nnsgiurml aaitcr.e s [��] [��] ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "67.R eordtehrreoi wansngd c/oolru omfn s wimlolv e aen,nogtr tiy v ing"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "69.R andmoamtc reaisra el smsotu revleyrtl ieib.n"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "71.T h-e1, 2 -, 1m artiixSn c eti17o.h na As LDTL w iftih- I i= 1 �. := - , l PorblSetem pgae 1w7, 63 2 - 1 - 1 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. -1 -1 2 -1 - 1 2 1 2 1 1 2 3 2 1 1 1 2 2 3 LDTL 2 4 3 = 1 1 3 -3 5 4 det5 . 3 = 1 1 4 -4 1 - 1 c 0 - 1 2 -1 c 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.A o - 1 2 -1 .E acrhoa wd tdos1 s ,oA o c 0 = -1 2 - 1 c 0 - 1 1 c 0 2 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.( UlU2,, 3U. = )( n /80,, n /8i)ne sdaot ft htreu vea l(u10e,s-,1 ). - 9 -36 30 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.H - = -36 129 - 180 . 30- 108 108"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T h1e0b y 1 H0i bletmr atxir vsie irlyl -icoon.ne ddit"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.A larpgiev omtu ilpitlsbi yle edts hsa1 in ne limienaacethni trnbyge lio.tw An 12/ 12/ 1 ��,, extrecmaesw eihm,t u lptliie1ra snp di vots4 i,As - 1/2 01 . = = := -1/2 -1 1 PrboleSmte pgae 2�1, 73"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.( )aT hsee otfa l(l,uv ,) w heura en v da rreai top/s q o fin tre.sg (ebT)h see otf al(l,uv ) wh, e ur e0 o rv = = o. , 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.C A()i tsh xea- xsiN;( Aits)h lei tnheur go(h1 1)C;(B )i Rs; N (Bi)ts h lei ne 2 3 thorug(h-, 21 0,;)C (C)i tsh peon it( 00,)i Rn ;t hneu lls(pCai)cRs e .'iv Solutnisto oS eelctedE xecrsies 437 �.Bo'rk erlnue :(s a7)8,( b1)( c1)2, 8,. ,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.( )b(,d, () ea)rs eu bces.spC aan'mtu ltbiy-p l1iyn( ) aa n(dc. C) an'atdi dn( . t) +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hseu omtf wo n onnsgiumlaartc reisy b mesa ing(uAl( -arA )T)h.e osfu mt wo singmualtrcariem sa bye n ongsuir.ln a"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.( )aO npeo bsisyliT:ih tmea trciecsAf o ram ssupcbaen octot nai Bn .i ng ()bY e:ts hseu bascmpeus cto anitAn I. B = - (c )T hsebu pscaeo fma rticwehssome ai nd iangaiolas l zlre. o"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13.I (f+ fg (x)i)ths e u usaflg( x(),th )e(n+ gf )ixgsl ((x))w,h hii cdsfi free.In nt ru2lb eoh st idaer1se( ( gh( ) x). R )ul4ie bs r obkeecna tuhsemierg ehb ten ov eirnse fucnito 1n - (1)xs utchhfa ( t1 (-x1)= ) x I.tf h ievn erfsucent ieoxnii swtti sbl,el thvee c-tfo.r +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.T he osfu 0m0 ,an) d (0 4,0, i) ns o otnt hpeal n;e ixt ha2sz 8 . (,4 = - y"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.- ( )aT hsebu scpeaosfR 2 ar eR 2 i tlsfle,i ntehsr o(u,00g ,)ha ntdh peo i(n00t.,) (bT)h seu sbpaocfRe4 sa rRe4i tstehelref-,d imle npslniao v nn ea0st, w o­ = . . diemnnsaislou bsp(anc esa nnd v 0,)o en-dismieloln niantehusrgo h 1 V = ° 2 . = (000,,, a n(d,00 ,0 a)0l ,o.n e 0), 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19.T hsem aetls�lsupbaccoen tainainndig es i thoerRr . L P P ...,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21.T hceo lnu mspoafAic tesh xeax- is=a lvle ct(,ox0 r0,s.)T hceo lumno fBs pace itsh xe paln=e alvle ct(,ox r0s.)T hceo lusmpnao cfCe tihsle i onvfee ctors - y y, ( (,x2 ,x0 .)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23. cobmitnoiano tfh ceo luomnfCs aislsoa c mobitoninao tfh ceo lsu mAon( fsm ae A coulmns pa;c heaasd iffecr0elnustmp ena.) c B"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.T heex atc rolubem nnal regsthe c oulmns paucnesl,bsei asrl aeyd thsaptca e: , in 1 1] (a lregrcl oumsnpc ae) ° [Ab ][ __ = 1 (nsoou ltitooAn x b). ° ° = (abl reiacnld ouym snp ace) (Axbh aass olu.t ion) ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27.C oulmns pacRe8 E. vebri ys c moab inaotfti hcoeon l nusm, Asxi nbci es = = soblelv.a 1 1 1 12 1 20 °"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.A 1 or1 1; A 2 (colounmn1 is1n .e ) = ° ° ° = 4 ° 0 1 0 0 11 3 60 2 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31.R contvaeicnwtstoi htwr soc moponetnhtdesoy-'n bte ltoRon .g PorlbmeS te pa8g5e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2.2, + + + + +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.x = 1x, y z = 0C.h an1 gti Oon(,g,x z =) c( -l,1 0,)d (I,-0 1,). y z y,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.E chefloorUnm = [�� � freve� a arlbl ieXls, X 3,X 4;s pelcs ioialounts (10,,,0)0(,,00 1,0, ,)a n(d, 0- 3,01,)C on.is stwehnebtn 2b•C opmlete 2= I slout(i,0o ,n0b 0,p) l usc oabmnityni aoofsn p elsc oiilanoust. I ontso S elteedcE xrecises u -3 -3 2 2 1 - v - V V V o nos loution!"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. + == ; W o 2 2 1,1 ,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. 7a olwlsu OT.h celo umsnp aicaspe l na.e c == = v== w == 10 2 2 1 - X2X o XX24,· 02 0-2 1 +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.( ax) 4 ,ofran y Row-reRd uced 2 == o == 0 0 . 0 0 -2 1 o a 3b 2 2 - - 1 0 o X4 XX24,· + (bC)om plselotuet xi on foarn y = b o - 2' 1 o o 11X 1 [1] 1 ][[] (11,) -"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11. 1X20hanslu lspalcitenh er oughb untos lout.Ai noyn == [�] b hamsan yp artsicoiulolutnartoAs x b. = p = 1 10 0 11 1 1 1- 11- 1 1. 01 0 (ar) 0 00 0 1 00 0 == 0 . ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13.R ,R -00 0R0 - . (b) 2 == 00 0 0 - ' - 0 0 r == 10 . . 0 00 0 r (c) 0 = [-�]"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.A n uslplamcaet rix insb yn r-. N ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.th inki tstur h.ei s I -3 1 0 -2 -4- 5 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19.T hsep escoilauslat rtiehoc enlo umson f 10 and . N = N = 2 0 1 - 0 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21.T hrep ivcooutlm sno fA f roma n byr s ubmaotfrr ainrxks, ot hmaatrt iAx * m harsi ndeppeinvdroeostnwg, ti vainrnb gyr i nvleserut bimbaotrfAi .(x hT ep ivot roswo fA* a nAda rthee s ame, esliinmciein sa tdioonnse a moiern d etrh-ew e jsudtno 'stef ero tAh \"*erf ecelo\"u mson fz ertohasap tp earA .f)o r 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23.( uT)VW(Z)T u(TV)WTZh arsa nukn slseT O. = v w ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.W ea rgei vAeBn I whihcahrs a nnkT. h ernak n(BA) rakn()Af ocres = < rakn()A n. ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27. R EAa ntdh sea mRe E*B ,th ne B( E*E)A-.(1 oTg eBt,r e dAu cRteo If == = == antdh iennv setrset p baBc).k iBts o iv anrent miabtlrei xA ,wth ietmnhe seshy ar e thes maeR ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.S nicRes tratwsti hri ndeprewonsdR,T es n ttaw rihtrt is n denpcteo nuldmsen( and [��] thzeensr .)So oits r edueccehdef rlomoi ns whreeIi rsb yr. SoluitontsoS eelctedE xrecises 439 12 2 1 1,00 0 0 X3, 31I.cf = R = haX s2, X f4 r ee. 00 0 0 10 2 2 1,0 1 00 X3X,4 Icf =1= R = has free. 00 0 0 --1-2 2 --22 10 0 00 1) 1). Spceilsa oilouintns = 0 1 (c= and 10 ( c=I N = N 0 00 1 0 1 -2 1,[� ] � 2,[� ] I Icf= R = haX sl f r ;ei cef = R = o haX s2 f ereR; == if 12,. . c =1= 2 []� 1)[ ] 2)2 0 � speaCl.'Soonilsnu ti (c or = 1 (c or bye mpty N = = N == N = X matr. l 12/ -3 0 -2 - 3 0 1 0 0 1 . 33X .o omple+tX2e ;Xc o,mp1le+ /tX2e 2 0+ X4 - 2 = = 1 0 0 1 0 b22b 31-,b 3b1b4 3 �]3 + X �5 1 [ 2� 35(.a S)lov balief = and OT.h en = = b22 b3-1b3 b1b4 3 (nfoer ev arisa)b(.lb Se)lo vaibfl e and + == O. == 2b-31 I b 5 1 b-- 32 b-11 X3 Thexn= . + o 1 13"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37. bys sytehmaa slt e tawsfoter ev arbielas. A 1."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39.( )aTh ep raticsuollarux tpii alsow nas my ulptliibeyd (bA)n ys oluctnai on [;;[]�] U] [l� [ n bex.p (c) = Theni sshtoe(rrlehn�g)thtan (d) The 0 Xn \"homogesnoelouiuttnsih \"noe un l lissp acweh eAni isn vleert.i b == 2,[ �] x Xn; [O' l"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.1M ultxipbpy l sya me is spieasclo luatslioio nncslth uecd ouelm sn p , -I [] I of x apnt dh sep elsc oilauatrineooc nths a nged. � 3 2."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.3F oAr, gviersa nke very goitvrheaesnr Fk o Br, 6 vgerisa n1k, == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. q 1, q q eveorhtye rg ivreasn k q"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.5( ar)<m ,alwsar<y no( br)= m,r<n.( cr)<m ,r==.n (dr)=m = n. 1 0 0 0 0 0 o . -1 00 1 1 1 01 Xn"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.7R == 0 and , o 2: nos olubteicoaonuf s e 0 0 0 == 0 0 0 o o 5"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. row ontso S eelctedE xrecises 1 1 49A. 0 Bc na'etx issitnt cweeoq tuiaoinntsh ruenek nsoc wnan'htvae o ne 2 ; = o 3 souilto.n"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "51.A h arsa 4 nk 1 thceo pmlees toultitooAn x0 ixs 10),. = 3; == == (23,, - 1 0 0 -2 R 0 1 0 with itnh feer ec omlnu. == -3 -2-,3 0 0 1 o 53(.a F)la se(.bT )ur .e (cT)ur e( onlcyou lmns.) T(urde)(n olyr wos). m n 1 1 1 11 1 0 1 100 1 1 o 0 0 1 11 1 0 0 1 0 1( Rd1 o e'scton me o o 55U. anRd = == 000 1 1 1 0 0 0 1 1 for1mt hsUi ). o o 0 0 0 0 0 0 0 0 00 0 0 o o"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.7I fc loum1n coulmn5 t,h eXsni saf erev abrli.Iea tssp ecsioallui tsi on == (- 10,,0,0,1.)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "59.C olum5in ss u trhoea nvoep ivsoitni ictase c mobinaotefia orcnllo iuenmrsa ,nX d 5 , ifser eW.it hfuo rp iviotnth oset hcelorun mst,hs ep escloiuatlii (s0o1,n, 0 ,1 ). 1, 5 The nucloltnasipanalmsclU ielp tloef(0s 1, ,1 0 ,,1 )( lai inRne ) . 10 0 -4"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6.1A 0 1 0 == -3 . 0 1 o -2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6.3T hsic ornuscttiiispmo sons i:bt lwepoi vcootln ustm,wf oer ev arisoa,bn ltlrheye e colnusm."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6.5A [��J ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6.7R miossl ti ktebole Iy ;R i mso slti ktebole Iyw ihft uorrtohow fz eor.s �"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "6.9A nyze ror ows actfoetmrehe s rewo s: R [ 1 R [� �l == -2 -3], = R I. == PrboelmS te pgae 2a3, 98 1 1 1CI"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.0 1 1C 2 == °g ivC3e == s C 2 == Cl = O. tV BI +u V2- 4V3 + V4 = ° 0 0 1 C3 (depe.n dent)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.I af 0 thcenou lmn1 0;i d f thbecn(o mlnu1 )- a( cuomln 0;i f == == == ° 2)== f thaelncl o luemnndsz ei(rnao al rplee rpeunldtaio(0rc, 01,)a ,li lnt h xey == ° plamnuesb,ted epetn.)d en SoluitontosS elcetdeE xecsries 441 1 2 3 1 2 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.( )a 3 1 2--* o -5- 7 2 31 o -5 -1 1 2 3 ivneret:=:}ii bnldepceonlduemnnts --* 0 -5 -7 (couludsd e hwras.ov) e 0 0 -81/5 1 2 -3 1 2 -3 1 o coluamdntdso0 (b)- 3 1 2 --* 0 7 -7' ,A 1 o , (couuslrewdo s .) 2 -3 1 0 0 0 1 o"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.T �syu'\" mI- VV 2 + V 3 = 0b eca(u 2W-s eW 3) (-W I- W3) + (W I- W 2) = o."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.( aT)h feuo rv ectaortrehs ce o luman3 sb y4o m fa txr wiAih t laetao snfteer e vabrli,sea oA x O.( bD)e penid[�eIV n V]th arsa 0nok r1 . 2 = (cO)V+ (c,O0 0,=) 0 h aasn onzseorlou( tktieaao nncy 0=f..) I 3 3 3 3 R . R. R. R . 11(.a L)i nien (bP)ln ae in( cP)l ainne (d) ofA ll 13A.l dli mseinsoar ne2 T.h reo swp caeosfA a nUda rtehs ema e. J �(v �(v�- �(v �(v 15V. w) w)a nwd w) -w).T htew poa isparstn h e = + + + == - v samsep aThceeya. r eb aswihse na nda rwien pednedent. a 17I.ef l imipnraotediosuon conemr o rzee rroosw t,hr eo wsa rolefi rnAled yae pe;n dent 1 o 0 1 1 0 0 r 0 1 10 0 -11 0 froe xamipPnlr eo b1l6em --* 0 01 1 0 0 1 1 0 10 1 0 0 1 1 1 1 0 0 0 -11 0 . 0 0 1 1 0 0 0 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.9T hnei ndepveencdtesonprtaass n p aocfde ie mnsniTo.hn e ayra eb sasif rot hat spcaeI.tf h aerye -c otlhueom fAnt sh mein ns t ol etshnsa(nm n). > 21C.( ):UA nbya sferoR s 2 ; N (U:() ro1aw n rdo 2wo) r( ro1aw n rdo 1w ro2w.) + \"-"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23.I nedpendent rcaonnlkC.u om lnussm pnR asnm ram.C olumsn abraes is =} =} nk foR rm :=:} rankm n=. ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.( aT)h oen sloyul tiiosn 0 b ecsatehu ceo lnasuer m piennedd.ne (tbA)x b x = 5 = isslo vabbelceat huceso e lsupamnRn. s"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2.7C oulmn1as n 2da rbea sferos t( hdeeir ffecnoutlm)sn p aocfea sAn U d;r o w1as n d 2a rbea sfreot sh( ee qruoaswlpc )ae (s,1-; 1,1 i)asb asfroith se( eqnuUallcl)es sp.a"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.r a(nAk)2 i f 0a ndd 2r;a nke2 Re )x cwehpetn do rc= - d. = c == = = c ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.1L eVt= (1,0 0,0, ,). ,. V. (,00 0,1, b) eth ec oordtievn eacr.tsI of itsh e I 4 W = litnheur go(h 213,,4,,)n onoeft hVe'a sri en W."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.3( aI)if wt e rneoa tb assw,ie c uoladd mdo rien depevneodcr,etws nhtiw cohu ld exctehegedvi edni mseinon( bI)if wt e rneoa tb a,sw iecs oudledl ete some k. vectorsl,est s hltaehnage vvi eidnni gms einon k. ns SeelcteEdxr ecsies to 5 35(.a F)la smei,gb hent os oilount(.b T)ur e7,v ectRorasrd eei pne ndent. 1 0 0 0 0 0 00 0 0 1 0 00 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37.( a)0 0 0, 0 1 0, 0 0 0. (bA)d d1 0 ,0 0 0 0, 0 0 0 0 00 0 0 1 0 0 10 0 0 0 0 0 0 1 0 00 1 0 0 0 0 0 1 (c)- 10 0, 0 0 00 10 araeb asfrio sa ll . , 0 1 0 0 0 0 1 0 0 -100 - A _AT. == 39 Y.( 0)0r equAi+ rB e+ sC OO.n bea sicisos xs c o2sxa n cdo xs c o3s.x == - - =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.1Y l()X, 2 (Y)XY, 3( )Xc naebx2,,x (3dxi1o)m xr 2 ,,x 2 ( xd2iom) r ,xx2 ,x3 ( d3i.m) .., 1 1 1 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.3 1 1 1 1 1 == + + 1 1 1 1 1 1 - 1 1 Chket ch(e1 e,n tt1rhy)(e3 ,,n 2 ,)te hn(, 33 ,)t h(en21 t),so h otwht aohtsfi ev Pes' arien depeFnoducenrond itio.ton ns theeni tnemrisan keer oaswlu lma snc dou lmn sumesql ur:ao swu 1m roswu 2m roswu 3m coulmn 1su cmlo umsnu m == == == == 2 colum3ni assu utmo mbcaeutasisecu omfa l rlos w suomfa lcllo unms.) (== - =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.5I tfh 5eb ym atr[ixAb i]is n vileb,erb it nso tcm oab inaottfih coeon l uomfAn .s 5 , [Ab i]ss i nagrau,nlA dh aisn depceonldue,bmnn i stas c mobinaotftio hosne If clounm.s PrbolmeS te pgae"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2.lt 110"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.F lasweeo, n klnyo dwi mseoinnasre eq u.La etlnf Ullhsapssam caeld liemr == m - r."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.C (:A )2(,,01 ,1(),01 0,;) (NA:n) 2,( 2-,11 0,,,) - 1(0,0,1,;) r == - r == C(T)A: 2(,,21 0,1,)(,0,1 1 ,0, ;) (NA)T: r== 1 (-,1 0,1,;) r == m - C():U( 010,,,)( ,01 0,;) N()U(:,2- 1 ,0) (,11-, ,0,00;), C(TU:)( ,210,1,)(,0,1 1 ,0, ;) (NA):(T ,001,.)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.A t imeevsec rouylm on fBi zsre ,os oC (iBcs)no taiintneh ndeU llNs(pAa.c) e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.F roAmx 0th,er oswp aacnthede n ullmsupsatco ebr etho g.So eenC ahlap3 t. e r == 1 24"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.[ 12 4;]2 4 8 hatshs ema en Uslpclae. 3 6 12"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.A x 0h aasn onzseloruot tihoern<n ,n anCd( TA)i ssm altlhReanrn.S o If == A YT inso stlo vafbrosl eoe m ExamepA:l [11 a ] n fd (,12 .) == f f. == == 13d. b/e;at hoen pliyv ioast . == n Witihn depcelonudmsern:na tn kn ull�parcoesw p aicse ; R ilnervftse e. 'i�5. == {OJ; n; 17A. [11 0;]B [00 1]. == == SotliuontsoS elcetdeE xrecises 443"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.9N o-fore xam,pa llielvn ertibbyl em atreishc avee s tamhef ours usbpcaes. n n 1 ° 1 ° 1 1 3=j:.. [11 ]. 21(.a ) . (Ibmp)o sbslied:i mseinnos (c) + ° 1 -]3 (d) (eI)m psosbileR:o ws pacec omlnu sapcre equires m [3 -l' 9 == == n. Then m r - == n - r . 23I.vn ertA ibr: lo ews pcaeb asisc olusmpnca eb asis( 100,,,)( ,01, )0(,,00 1,) ; == == B: (10,0, 1, 0, 0,,) nuslplcaea ndl enfutl lcsebp aasareese mpty.r ows apcbea sis (,0 00;1,, 0 ,) (,00 1,0, 0, 1, ) (,01 0,,)( ,01 0,,)( ,00, 1 ;) and ;c omlnus pcaeb assi \"'1, (-,010,,,01,)0(,,0- ,1,001,0,) , (,0,-0 ,1,0,01;) nullcsebp asais and left nUlplasces ibissae mpty. . . // � 25(.)a Samreo ws pcaea ndn UslplcaeT.h erefroarn(ekd imseinoonfr ows pcaei)s tehs ame(.b S)am ec olusmapnc aen dl enfutsl plca eS.a mrea n(kd imseinoonf colnus mpcea.) <"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27.( )aN os loutimeoann s thaAtl ways Canc'otpm arean d r < m. r m n. n. 0, A (bI)f thneu llsopfa cceo ntaani onnsz evreoc tor. m r > - 1.:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2.9R ows apcbea si( s1 23,,4,)(,,01 2,;( ,001,2,;) n ullcsebp asai( s,0 1 , 3), -2,1 ); (,100,,)(,.01, 0,)( 001,,) ; colusmapnc beas is letnf uslplcaeh as ebmapitssy. Av° A 0. ° 31I.f andi sar owo f then Only isin b ohts paces. == v v . v == v == 33R.o w3 2 (2 r) owr ow1 zerroo ,ws ot hvee cte or(,- s l 21, a) r ient hlefe t == - + nUllcse.Tp haes maev ecthoarpsp teobn e i nt hneu llspace."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.5( a) andw spnaC (A.) (b)v a nds pnaC (TA) (. cr) a nk2 i fu a nd ware u z < + v uTv T depnedeonrt anda rdee pen.d e(ndtT)h er anokf w is"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. z z"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37.( aT)ru e( samera kn).( bF)a l( seA [10) ]. (c F)al s( e A ca nb ei nviebrlte == andas lou nsymmertic)(.dT )r eu."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39.a 111a ,21 0a ,13 1a ,2 2 0 a ,23 1a ,13 0a ,23 1a ,3 3 0, a2 1 1 == == == == == == == == == (noutn iq.u e) 41R.a nk nm eannsu slaplce zervoe ctaonrXd n 0. r == == == paeg ProblSeem2t . 5n 122 1 -1 ° 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.A ° 1 -1 N(A)c ontamiunlst iopfl 1 es N (TA) c ontamiunlst iples == ; ; 1 ° -1 1 1 1 . of -1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.Th ee nrtieisne acrho wa ddt oz er.To heroerfea,n yc obmintiaowni lhla vthea t smaep ropty:e r1 12 3 0A;Ty YlY 3 Y Y2 12, 1 1 == == 1 == 11, 1+ == -2Y- Y3 3 + 1 + 1 2 3 0. =} + - == 1 1 + + 1 == Itm eantsh attht eo tcaulr reenntte rfoirmng =} outsiiszde er. o st oS elected Exercises"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. ]hap si vCl ot C3s, + C1C3C 1C2+ C 2C3 + C1+ C 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.C ondiotnb ia orhneIs b -4b 5 oh�3- b4 +b 6 0 b ,2 - bs + h6 o. + == == == 3 -1- 1 -1 C1 C2 C5 - C1 - 2C -C5 + + - - -1 3 - 1- 1 1C C1+ C 3 C4 -C3 C4 +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. , - - -1- 1 3 -1 C2 C3 C2 C3 + C 6 - C6 - - +- -1- 1- 1 3 Cs C4 C6 C+4C 5C 6 + Thsoce' s th acto nnteonc otd wei alplep airrn o w j j. 1 0 0- 1 1 0 )'1 0 0 1 0 2 0 0- 1 0 1 Y2 0 -4 73 1 5 0 0 2 0 0 1 0 Y3 0 3 4 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.0 00 1 0 0 - 1 Y4 0 . ,x - 14 , 10 -1- 1 0 0 00 0 Xl 1 - 3. - )'- 3 . 1 14 1 0 1 0 00 0X 2 2 0 1 3 0 1 0 - 1 0 0 0x 3 13 6! 13T.h earr2ee0 c hoiocf3ee dsg oeuostf6 b eca\"u6cs hoeos 3e\" 2.0F our = 3!= 3! choigcievasen getlslr,evi ain1g6s pantnrisen.eg"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.t hiinitaks l rebaudii.yln t I"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.79 n ode1se2 d g+e4 sl opos 17;n o d-e1se2 d g+e sl6 o op1s. == - ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.9X (=,11 1,1, g) i sv e= 0t;h eTnA x= A 0 r;a inaskg ian- 1. Ax n 0 1 1 1 1 0 1 1 M="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2.1 and 1 1 0 1 1 1 01 2 3 2 2 2 (M)i=j an a1j+ . ainanj + . · 2 2 3 2 2 anwde g ea ti ka1wk hj eth ne re M = . == 2 2 3 2 ias2 s-etpp tahi t ot o k j. 2 2 23 Not3ipc taehf sor ma noidtes etlof . PrboleStme pag13e3 2�6, [� [�l �"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.R otati-o� ln"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. I IA x == 1a lwspa ryodaunec leslsei .p 112"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.T heayrtr ea snofrmteo(d 31),(,2, )6, ( 1--,)3 T.h xea- xsti rnus;v ecrlatl iin shiufpt/ dbouwstnt v aeyr .t ical Sotliunost oS eelctedE xecrsies 445 o 20 0 Sceond 0 0 06"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. d eartieiv v Then ullsipssa pcaenb nye( d01 0,,0, )a nd o 0 O·0 matrix o 0 0 0 (010,,0, ,)w higcihvl eisrnP elSa.c eondde rievsoa flt iirnvf enuactiaorzene sr o. Thcelo umsnp aicasec cildlteyhns etma aea s� en ulclesb,peu acsasece nodd eriva­ tvieosfc ubairclesi r n.e a"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.et a dn tea r-aeb asfiotsrhs eo luotfui \"on s == U. jt [C -sO i]n[cS os - isn] [ 1 ]0 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.1 e, e e e _ soH -I . S. I n cos sIn, cos 0 1 _ h - tJ e ' e e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.3 ( aY)es .( bY)es W.e d o'nnte epda rheens(teBAsC) o rA B(Cf)o ArB C! 1 0 0 0 2 adn A te1;l doturnbaslpeoo fas em agtirviexs o 0 1 0== I; 15 A.== thmea tirtisxe lf. 0 1 0 0 A23 Noet 1b eucsatern aspoofmsa et 2r imixas t 3ri.x 0 0 01 == 0 0 0 0 000 o 1 00 1 o 0 1 00 o 01 O . 17 A. ;B== o 0 ;1AB 0 BA 0 1 O. == 0 1 0 == o 0 '1 == 0 o O� 0 o 1 ° I 0 01 o 0 0 1 1 1 T- T- 19(.a )i nivsle werit tihb l3/(ci)is n vbelwreit tih == Y- 1.1 (y)y ; (y) � w T(+v T(v) 21W.it h 0l,i rtiiegtaiysv e 0) + T ( .0T )huTsO) OW.it eh ( == == == == -1, -T(O).T ( T(O)T.( O) lirniegtaviyeT s-( O) CertaiOn)l y Thsu = O. - == == S(T(Sv(»v v)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23. == == T(2vT)( v T(vw )T (vT)( w). 25 (. b ) anldi an(r(ec,af )i)as l are 2 ),( df)ia ls+ = + 3 == 27T .( T»( (vV,3V IV2),T; ( v)V ;TI OOT(T(v9)(V9 » T(.v ) = == == == T(I,O) T("
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.( )a O.( b()00 1,,i)s inntoh ten g.re (ac)O ,1 )O . == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.1 A sosctieialv wag ivAeM(sI AMI+ AM D.i isbtrultwai voeev 'egsri ves + M2) 2 == Ae(M)e A(M). == [�� ][� �l. 33 N. o m atrAi xgv ieAs = Top reosfrssTo:h mea trixh assp ace diemns4i.Lo ninae rta rsnofrmoantosint hsapta mcuesc to mfeor m4 b y4 m a­ tric(e61sp r aamres.tT) ehsoem ulptltiiicoabnysA inP robl3ea1mn s3d 2we re speltrc aisnafoiromswna itth4 p oranalmeyer. ts [�� ] [�� ]"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.5 T (=I)Obtu M = = T(M);tshfieeltlhreaenM.g= itnhkeem.e l [ 37 (. )a M = ; : (lbN) = [� � r(lc=.)h acd. 39R.e orbdsaeibrsyp e rtmauntm iator;ci hxalnegthnesbg ypo siivdteig naolam atr.i x a a2A 4 I 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41.1 b B b 5 ;Van derdmedoenter=m (i-bnaa()ent-a)e(-b;) the == l e 2 e C 6 poianb,t,sm u sbted ifefreannthtde, n det=f.e0 r( miinntatenirpitoops nlo absls.ei ) e 43 I.T f i s innovtle rett,hi T e( bn )V , .I ., (v nT ) wnioblteal b asTihsew.nec uoldn't . cohose (i )V a so uptubta ss.i == Wi T 45S . (T( (v -1) 2,) )b uS t(v ) ( -21,a) n T d(S v()) ( 1-,2)S .oT SS T. -=I == == == 47T.h Hea dmaarmda txri hHa osrh togloc noaluomfln esn 2gS.t oth h ien sveeorf H iHsT / 4 H/4. =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.9F las:the en n onzveercowt ooruhsla dvt eob ei ndependent. PrbloneS lte3 � page 1, 148"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. I xII I -/21 ;I yII ==I 3 -j2 ;x T y 0. == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.( 2/XX)IY 2(/ IY) == - 1m eatnhsxa Yt+ 2XY 20 s,ox yT 0. II == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. anda roerh togloa,nsl ao and VI V3 V2 V3."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. x ==-(21,0 ,;)y==(-1,- 11;,)t rhoew==z(1,,21 i)os rh tonglao tthnoeu lcles.p a"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. Thoerh togocnmoaplle mietsnh tle i tnhoeru g(h-1 -,1, 1 a)n d0 0,()0., 11 I. Af T == 0t,h yeTbn yTA x (TyA )x0 w,h iccnohtd riacytbT s0 . == == == =j:. y 13T.h fieg eus rplainytyi s n iRnInct loou msnpc aep ar+ tl fent ullpsrapta.c e 15 N. o s umcahti rxb,eu csa(e12 ,,lT) ( 1 -2, ,1 i=)0 . 17T.h meat riwxti ht hbesa ifsoV ra s rwiostTs.h etnhn eu llisVsp..L a== c We. 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.9 ( )aI V f a nWd a rlei nieR ns, V ..La nWd..L a rienr tseecptlniae.ns (gb V). ;] � 21 (. ,1 ,21-)ipserpendtiocAPu=.l [ ar� h aNs(A=)P ;=B[l 2- 1] has srpoawcP e. == ; ; 23 A. [] ha ssu bcsepsaf uorl in(e11s,);o r tgholon a(t-1o1 ,) (1, 2, ) = = orhtoglo n(ta-o,21 .)A lwaryosswp acneU lcles.p a .1 1 2 -3 2 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2.5 ( a)2 -3 1 .(b )- 3i nso otrh toglo nta1o. (c )1 CiA(n) -3 5 -2 5 1 1 1 [ ] 1 -1 and inN ( AT)i si mposbsil:ne optre penadri(.c duA)l has 0 1 -1 o 2 A O.( (e1) ,1w )i bleil n ntUhlel asnprdao cswep ancose u;mc ahit .xr == 1, 27 (. )a I Afx bh aass loutaidno TnyA 0t,h beTyn (A)TXy 0. == == == == (bb )i s intnoh ctelo umsnp ascone,o;p t e rpendtiaocl yuil ltn arh l eetn f Ulplscae. 29 x.== X r + xn,w heX rir eis tn h reo swp aacnX edn i istn h neU lclesT.ph aeAnXn == ° X+r anAdx A AXn Ax.Ar lvle ctAoxar rsce o bmitnoianosft hceo lumns == == X ofA .Ix f (== 10,,)t hern (/121,/ 2.) == 31 (.)a � oar syrmcimm aettxrth,ie c loumsnp aacdnre o swp aarcete h sea me. (bx)i sin t hneu slpclae ainsid nt hcelo umsnp acreo wa csesp:ot hese == z \"eeingvoerchsta\"vx e T O . == Z 33 x. s pliinX ttr s+ o n ==X (,1- )1+ ( 11, ==) ( 20,.) [_ �] 35 A. x Bxm eathnas[t AB ] OT.h rheoem ogeenqeuoauitsnfi uo orn s = = unokwnnasl waysn ohnazvseeor loau. H t eiroen (x,31 a)dn x( ,10,) and == == 3 Ax== Bx (,565,)ii snb otclhou msnpc ae.Ts wop lnaeisnR (thrzoeurgoh) == musitn teecirtnasl ianltee ast! 37 A.Ty == 0g iv(exAs) T == xyTA Ty== OT.h eyn Axa dnN (ATC)A( ). 1- 1- [ : ;"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39. ; ;J subspace S-Lst hneullosfp aAc e TheorreS-Lefi s a eveiSnf niost . = 4 4 41I.Vf I aslo'lf R ,t hVe..L nc ontoanitlnhyz se e vreoc . Tt ho e(nr..L V) ..LR V. == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.3( 11,1,1 , i) as b sasif ro ..L.p A[ 11== 11h] a tsh e palsia tnnsUe l lspace. P 1 -"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.5C olum1on fA iosrh toglot ntoa hsepc aes panbnyte h2den .d,.,n h.t row.s of A 2 2 -1 47A. -1 2 2 , == 2 - 1 2 ATA 91id s iag (o ATAn )ja i l (c: uo mlino fA ). ( cmonl u == == j)."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "49. ()a( 1-,1,0 )i s biohntp lnae.Ns orlmv aectaorprees rp enrd,pi lcausnlteaisl l trhee inteecrt(s!)b N eed orhtoglo'nyaetcost pnoa rtswh heoo lrheto gloc noam­ plemienn t( cL)i ncenams e ewti thboeuiotnr gtg hoalon. R5."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "51.W heAnB 0t,h ceo lumno fBsi �csao cnet iatnih nneeu dl losfAp .Tah ceer efore == thdei mseinooCfn (B )< dimseinooNfn (A ).T hsmi enarsa kn(B< )4 r-a(nAk.) PorlbneS letp aeg"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.2, 151 ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. () a( x+ y )2 >/ FY (raithmmeenat > igce omemteraoinfxc a dny ). 2 2 2 2 (bIl)x+ yl<1 (1xlI+llyIl)1menastha(t+xy)(xT+)y< IxII+121xlIlylll+llyIl.1 T T T T Thleet h-fansdii dseX + x 2 xY + yy A.f tcenarcl elgit hnsii xs Y < IlxI yII .II I"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.p (1/03,/ ,311 /003;() /5,91 /09,1 /09.) == , 1 . . 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. ce=o1s/,J\"nso e==acrcso(/-fo1);P == [/ln li]n alle nets-r. I == n 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. C hoob s== e ( 1.,,.1 .)e ;ql uiaita f1y . .a (n t ha ie pnsra altlb o) e . l == == . aTaaaTa (Taa)Ta aTa"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. p 2 == == P. aTaa(TaaT a)== aT(a==a Ta) 1 [] [ ] 10 to io-t o [ 1 0]"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.1( )a P P 1 P-I== (bP)+ P ; == 2== 1 2== l 1 02. 10 ; _2. 10. l 10' 0 1 PIP = [�]� Th. seu om ft hperj coetioonnttswo po e rpendliicngueilsvta ehrse 2 vecitto.srT e hlpefr jeoctoinotno oannetd h aelp nirpe neen dirlc iugnlieav tehse zevreocr t. o a 1a 1 ana n aT a . ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.3T arce + 1. == aT+ a . aT==a a Ta== ns Steol teecdEx reicses"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.5 I IlAl2 x== (AX)T(Axx)A TAxIA,IT lx2 l (ATX)TX()==A xTAATXI.Af T A== == == AAT ,t hIeAInx I ==I 1A\\TX I I(.T hmeastcere aisrc ela lneodr mal.) e 17(.)a a Tba/Ta == 5/ p3 ==; ( /535,/533/,;) == (-21//331,,3/)ha eTsa 0. == ()ba T ba/Ta - 1;p (13,1,)b nade == (00,0,.) == == == 1 1 1 5 1 3 1 1 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19. IP 1 1== P12 a nP d1b 5 . P 3 9 an3d == - I == 2== - 3 3 1 1 1 1 1 5 1 3 1 1 Pb 3 2 == 1 1 -2 -2 4 4 -2 1 1 21P. I == - -2 4 ,4 P2 == 4 4 -2. 9 9 -2 4 4 -2 -2 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23. P3 P I P 2+ + 1 -2 -2 4 4- 2 4 -2 4 1 1 1 - -2 4 4 4 4- 2 -2 1 -2 I. - - 9 + -9 + -9 == -2 4 4 -2 -2 1 4 -2 4 25 S. ni cAe i isn vlerePt, ==i Ab(ATA )-AlT== AA-1(AATT) -I1 p:r jeocotn talol == 2 oRf. ProbSle3et.m3p , ga e 107"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.x 2E;2 (103 x2 )+ (54 x2 )i msi niizm;e( ,d4- 3T)3(4,) 0. == == - - == 2 1 3 - 1. 3 • x [ l 'p �3 lb ' - p - '- � i pse rpentdboit ochcu ollnausrm. 3 - -- 2 2 3 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. == b4 5,9,a tt - 10,1,th;e b elsitn e (i/5)s2 t p 6 ==; ( /726,1,7/)2. == + ° 1 12/"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. I P==A(ATA)-AT== 12/ 12/ - 1/2 . ° - 1/2 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.() a Tp == (ppT) T== P.T hePn pTP == p2 . (b)p rjePoc tst hosenp taoc e == {OJ. Z =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.1P + Q IP, Q== 0t,rn pasostoeQ P 0, (P s- oQ) (P-Q ) P °-- ° + == == == Q I. = 13B.es lti 6n/1e53 (-36)/t3;5( 3135/9,355/6,313/,5- 1/315f)r oCm Dt. p == +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15. H(2 I-2 P)2I 4-P 4p2I 4P 4P IT.w or eflecgtiiIvo.en s == == == + == - + 2l 17P.rj eotcioonnxt o 0 Prjeoctoinot(-no1 1,) [� :i� �� + y = = = 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19.P rojemcarttioixno rtnoo sw p caew oubleAdT ( AAT)A- ilf rtohwwese ridene pen­ dnet. Soltuoinst oS elteedcE xrecises 449 m 1 2 C o o . x D;b== . 1' == -3 E 2 4 -5"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27. ( aa)Ta m,a Tb bi+ . . bmT. herex fi otsrh eme e aonft h be' s(.bT )h e == == . + 2 2 vanrciiea ls Ie l l== �;t( bi_ X). (c p) (,333,)e, ( -2-,1 3,,)p Te O. I == == == 1 11 1 P 1 11 . == - 3 1 11 29(.x x)(-xX )T (AT A )-I A T[(-bA x)(-bA x)T]AA()A-I .TF oirn depen­ - == denetro rr,s ssitutbu(tbi- nAxg() b . -A'x T ) == (J2 givtehcseo vareim aan�c I tri(xAA T) -AlT (J2 A (AAT) -I .T hissip mlfiietso(J 2 (AA)T- nI:et fa romuflotarh e corviaamnacrtei. x 9 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31. __ +. 1b 0l O + 10x9 == 1( 0b l . . + blo), 1 0 0 1 8 ;"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33. � [ C' hafInghgt eI e S°t do p x [! slo] vA e x s p. ; � ]8. ' 0 == 1 ; = = = 1 4 20· 17 1 0 0 0 C 1 1 1 8"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "35. Clsoepsatbr oala: D . 1 3 8 9 E 1 4 16 20 -- 4 8 26 C 36 -- ATA x 8 26 92 D 121 == . -- 269 23 38 400 E -- --"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37. ( )aT hbee lsitn e 1i 4s,t wghiotcehhsr otuhcgeeh n ptoei(r,tnb t) (29,.) x == == + (bF)r otmhfi er esqttui ao: en m D � ti == � bi D• i vbiy md tego e Ct D-- t == b-- . + + b+ . . wi w�bm 39 x-- w_ i . + - • 2 . W w� I+ . . +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41.X w (12/,1/4)7;A xw (1/121,3 2/12,/521,) == == Axw (Axw)WT-WA(xbw ) b (-12/,1/82,-1 4/12), O. - == == ProblSeem3t . p4a,g 1e58"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.( a-)4 == C - 2D-,3 == C - D,- 1 == C + D,0 == C + 2D . (bB)es lti ne +t -2 gotehsr ou4gp ho i;aEn 2lt lsO .( cb)i st hicenou lmn csep.a == iontosS elcetedE xreicses b"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.P rjeoctoinao3: (n 2-/,13 3/-,2 /;t3 h)seu ims i tsneolttfih;caa1 a et T ' 2aa I, 3aa J arperj coeotniso ntthor eheo golodr nitarieo.cnT tshesiurim ps r jeocotnoi ntthoe whoslapeca en sdh obuethl edi dnetity. 1 1 1 1 2 2 2 2 I I 1 1 2 2 2 2 1 1 1 '1 2 2 2 2 1 1 1 \\\\ 21 2 22 \\ \\ 7\\\\. X (ql+ . +.x .qnn) T( lqXl+ .. .x qnn)== x f+ . . x; Ib lI2 l bTb I . == == + + =} xr. .x;,, . + +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. T hceo bmitnoianco lstet soq 3 i O sl q O 2q. + 11Q.i usp pternrig aucloalru:1hm anq sl 1 == ±1b;yo rhtogaloinctoyml nu2 m usbte . (,0± 10,.,. )b;yo rhtogloyinc taolu3im (sn0 0,±,1 ..,) .a;n sdoo n. 0 1 0 0 1 1 1 1 ,0\""
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.3A == 0 1 10 1 0 ==0 QR 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. 1 1 1 1 00 0 01 13/ 2/3 -2/3 15q.l== 2/,3q 2 == 13/, q 3== 23/ iistn h leetn f lulcs;ep a -2/3 2/3 13/ ...-[q-.b..!]..[ 1] . x q==ib = 2 9 17R. = xQTb g iv[ es�� ] = [[5 ax nx d6 = ] [ 3] 5]-6 19C.* - (iqcq* 2i) c s- (qq!-lc( )qc qi) 2b ceasuqe iq l== O. 8- x OxO . Byo rhtoglointtayhc,eol essftnu cotniasr0 es i2n = 0a n0d == + 2j3 a. ==o 12/a,l == 0b ,i == 2/.n 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25. (,xx ) Thcel eoslsti ne== 1i3/s( horiszionnctea== l 0 .) y"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27. ( /1-J2,- 1-J2/,0 0,,) ( /1-0),1-0)/,/2-0) ,)0, (-12/v'3,- 1/2v'3,1 2/v'3,- 1/v'3). 29 A. == a== (1-,10 ,, ; B== 0 b )- p== (,��, - 10) , ;C == - - == (��,,, - 1�) c A B P P /.4 Nottihcpeeta teirntn h soeor htoglov neactAo,Br ,s N ex(t11,,1, 1 , ) C. 2 31(.)a T reu.( bT)re uQ . x== Xlql+ x 2q 2.IQIxI =Ix +rx ibceuasq e Tq 2== 0. ProebmSl te pgae 305, 196 4 0 0 0 16 0 0 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.F2 == 0 0 0 4 , F4 == 0 16 0 ==0 42[. 0 04 0 0 0 16 0 0 4 0 0 0 0 0 16"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.T hseu btmraiiF sx3•"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.ex ==i - 1f ox ==r (2kl ),en i==iO f ro e == 2k+ nn / 2k, i inset re.g + Sotloiuntso S elceteEdx reicses 45"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.C (10,1,0 ,.) =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. ( a) F t im(e10s0,, ,)0 cloumn ozfF e r(o,11 1,1, .) == y == == (bC) (11,1,1 ,) /4 . == 1 1 2 2 0 Ceven1 y ' 0 0 11 c. == == == == 1 -+ C od==d 0 -+ YII 0== -+ Y 2 . 'r 0 0 0 0 ..'.."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.3C o == (fa+ f l+ f 2 f3)/4, C l( fa- == if f2+ if3)/4,C2 == (fa- fl+ f 2 - 1)/4 + 1- 3 , C3 == (fa+ if f2- if3)/ 4;f o dmde,afna ==s 0 f,2 == 0f,3 == -fl,T heCno == 0, 1- I C2 == 0C,3 == -C,Is oCi assl oo d.!d 1 1 1 1 1 '2 1 1 1 1 1 1 1 1 15F. - l - F. == - - == H 1 2 1 1 2 1 - 1 4 -2 1 1 . . l -l l 1 1 1 1 e2TCi/6 e2/TC3i e4TC/3i 17 D. == anFd3== 1 . e4TC /3i e 2i/TC3 1 0 1 0 2 3 3 19 A. == daig(i,li ,,i );p, == 0 O l adnp T l aedt A o -1 == O. 1 00 21 E. i egnalvuee so 2 -1 - 10 , 2 --ii 3 2,e 2 2 -(-1)( -)1 4, == == == == == == el - e 3 2 i-3- i9 Chetcrka0 +c 2e+ 4+ 2 8. == == 2. == 23T.h feo cuorm poanre( enc + t o s2)C (+C +I 3)Ct;h ( enc - o C2) i(IC -3)Ct;h en + (oc C)2 (-C+I 3 )Ct;h ( eo nc C)2 i-C(I- C3)T'h essteea prtsehF eF T! - + PorlbeSme tp aeg 4�2u 206 4 1 2 �"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.d e(tA2 ) 8a dnedt- A)( (1)deAt andde(tA) and(dA-e1 t) 2 . == == == == == - 3'.T hreo owp eorsnal teviaed eAt u nchanRgueld5e .T bh yem nu ltiapr loywi ng by 1( Ru3l)gie v etshr eo ewx cghera un:ld eeBt == -deAt. -"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. Fotrhfi er msatt ,tr owir xoewx cgheawsni plrlo dtuhiceede n tmiartti.yTx hseeo cnd matnreiextd hsrr eoeewx cnhgaterose aI.c h"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. d eAt o( isgnul;ad reU)t 1;6d eUtT 1;6d eUt-11 /1;6d eMt 16 == == == == == (e2 xcnhgsae.)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hnewe d eterim(si n1ma fn()tda bc.) - - n n n"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.1 QI Q Q) Q IIfd eti s tnhodeten t 1 == (detw oubllwdou p aoporpca rhz e.rB out remaainon rsot oghnmaalit .xrS od eQ tm usbte1o r- 1 . 3 13(.a R)u l3e( ftaocr-in1fgr oema crho wg)i vdee(sKtT ) (- 1)deK t.T h en == -det deKtT deKtg ivdeeKst O. == == - K iontsoS elteecEdx recises o 0 0 1 o 0 1 0 (b) hadse == t1 . - 10 0 o - 1 0 0 0 15A.d dienvgec royln u motfto h Afie r cslotu mmna ksie at zceorlous mdone A,t O. == Iefv erroyow fA a dtdos1th ,e env erroyow fA Ia dtd0osa ndde (tA I) o. - - == [!!l - Butd eAtn eendob te1 A: hadse( tA 0b,ud teA t 0 1 . = J) 1= == == 17 d. e (tA )1 ,0d e(t-A1 ) / 0' d e(tA- A A2 - A7 100 f roA 5a dn J) == == == + == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. A == n"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.9 Tkaindge termgiinv(aednset (tsd et (-I)(dDe)(t d etF ore vetnh e C) D) C). n == n reasfoianl(isbn egsc ( ea I- u) 1a)n tdh ceo ncliuwssr niogo.n == + d -b ad-b c 1 ad-b ca d-b c 21 d. e (t-A)1 det == -c a (da- cb2) ad- cb' ad-b c -bacd 23D.e etrmiannt3 6a ndde tenrmti 5na. == == -,� 25 d. e (Lt) 1d,e( tU) -,6d e(tA)- 6d,e (tU-1 L-1) and == == == == de(tU 1 L-- 1 A ) 1. =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27. R ow3- ro2w ro2w ro1ws oAi ssni gular. == - 29A. i rse ncgtuarsl oda e(tA T=j:. (A d)Ae Tt() d Ae):tt hseear neod te fined. 31 T. h Hei lbdeerttne arnmatirs1e8 , 1-02 ,.461 -04 ,1 6. 1-0,7.371 -012 , x X X X"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "54. 1-01 84,.81- 025 ,2 .71- 03 39,. 71- 04 ,3 .221 -053 .P ivotrsao tsai re X X X X X ofd etiennrtam,ss oth tee nptihvi ost r 1n -e053 /a01-4 13-01 ° :v esrmyl a.l == 33 T. h eg eldsaetrt ermoif0n -am1natt crseif sron 12,., ,. a r. 1e1 , 2, 3, 5, 9 ,3, 2 , == 5,61 443,20o,nt hwee ba tw w.wmhawrto.lwdoralfm.IcaHdomamardsiMmauxm DteemrintrPaonbelmhtm.la nalds oin t h\"eO ni-nLEen cyclopIendtieag ero f Sqeueen\"swc:w .rwseechaa.trtc.oWmi.t- hIsa nId,st hlega re4sb ty4 d eterminant (seHe admaaridin n die1sx. 6) 35 d. e t M) 1 a b c dS.bu trac4tf o rmrr ooww1 s2, a,n 3d. Then (J + + == + + + surbatac( tr o1w)b r(o2w) c r(o3wf) or mr o4wT. h sli evsae an gturrml iaaratix + + wit1h1,1 , a,n 1d a b c do n diitasg. o nal + + + + PorblSetem pgae21 5"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.3,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. ( aa) 12a 21aa3 434 1e;v seodn eA,t 1. == == (bb) 13b 2b 3211 4b 18od;sd od, eB t -1.8 == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. (aT)ur e( prordueu.l)c t F(absl)e( aIl)sl. (c F)as le( e d[ t11 0 0; 111; 10] 2 .) =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. T h1e, c1foca tioFsrn .T h1e2 ,c oaftcohraa s1 i cno mlnu1 w,ih ct focat�o-'lr2' -1 Multbiy(p_ l11 y+ )2 a nadl -so1t o F finn dF n F n Sot hdee tienramnts 1 2 == - + -• F arFei bonnaubcmrecsei,x cenip tsth ues luF na- 1• Sotloiuntso S elteecEdx recises 453"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. Coafcteoxnrpi sao:dn et4 3()-4 (+14 )-( 4) 4-(1)1 2. == == - ( 1 ' 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. ( a()n- ln)(!ae cthe rn -m 1 ).() b 1 + - ·+ ··+ ) n L 2! (n - I!) 1 3 (c )( n+ n2- 3.) 3 A ] J � 11 [_ � ] [� �] [ : � d�e [t � det [ �_ ] 1 . 1 = = � = .]� ![ � ;}[� _� 5 de de(tAB)T.es At [12 ,]B [det ] = = = = = [n [�� ] de(tAB);A B [12 ,]d e_t 0 de(tBA).S nigular: = = = = rakn(BA)< rakn(A< )n < m. 13 d. e At 1+ 1 +81 2-9, 4- 6 12s,ro o wasri edn peenddeeBnt t 0;s, ro o sw == - == == + ardee pen(dreonwrt o 12w ro3w;) d eCt -1C, h aisn deenpdrewonst. == == 15E.a cohft hseit xe rimdnse A ti zsre ;ot hrea inaksmt o s2tc; lo um2hn a nsop ivot."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.a la2a33424ah a-s,a 41a2a33421ah a+ss, od e At 0; 1 == deBt"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. 4 . 4 . 2 1 4"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. 1 4.8 == - . . == 19 (. a I)a fl l a22a 330 t hfeuonrt eramrsseu re. zeros == == == ()bF itfeteenr �mrzsee .r o 21 S. mo et eralm2a (3a.. .a nwi n btgifh roem luians o zte !rM oovreo sw1 2,., ,.ni .n to as' rosw f3,, ())t.eh snTeoh nezne wriobl eol n t hmea i dina glo.n a a, . . . 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "234.! 2 1e2v peenr miuot;nda se(tIt + 1o64r o r0( 1c6oe mfsor m + ).I j Peev)n == == 3 2 1 4 0 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25. 2 4 adn2 A 0 4 0 C C == T == , 1 23 0 04"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27.I Bn lIA n-lIAnII (n+ 1)n 1 . == - == - == 29 W. e m uscths oeIo fsor mc olu2ma nn1sdc, o lu4ma nn3sda, n sdoo nT.h eorrenef � n/2 musbtee vtehona vdeeA tn=J:. O .T hneu mboefer x cnhgaiessn s oen == (-1) . 31S.l 3S,2 8S,3, 21T.hr eu lloeo lkkiese v esreyc nounmdb ienFri o bna'csc i == == == sqeuen.c3 e5 ,, ,8 1,, 32 13,45 ,5 . ,s oth geu sei sSs4 5.5T hfiev neo nrzoe . . . . == terimtnsh ef oburilmgfar oS 4a r(eiw t3hws h ePrreoe bm3l 9h a2ss8 )1 1 - 9 - + 9 - 9� 5. =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33. Chan3gt i2oni gtne h c omreerd utchdeees t erFm2ni+nb2ay 1nt ti mtehcseo factor Sn-l oft hcaotm eenrt Trhyic.fso ca tiotsrh dee teromfi na(notns eis zmea rl,)l e whicFh2n T.i hse recfhoarne3g itncoghn ag2et shde e mtieanrnttoF 2n 2+ F-2n, whiicFsh2n +. 1 35 (. a ) dEevLte r1yd; eU tk deAtk 26,-, 6 f or 1k2, 3, . == == == == ��,. (bP)i v5o,ts"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37. T hsei txe rmasrc eo rrReocw1t 2.r o2w row 03, t hsmeoar tiixss i nrg.u la - == +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39.T hfiev neo nzteerriomnd se A t 5a re == (2()2()2()2)(- 1()-1()-1()-1)- (-1()- 1)2(()2-)( 2()2()-1()-1) + - ((2-1)( )-1 ()2.) ontso S eletcedE xrecises 41 W. i ta 1h 11 t,h -e1 2,-, 1m atrhiaxds e· t==1a nidvn ers1e n 1 - (A-)ji == == + i ma(, x j). 43 S. bu ttria1ncf gor mt hn en, e ntsruyb tirtcasoca fttcosC rn fn or mt hdee tenrtm.i na Cnn Thactoa fctiosr 1( msalPlaesrmc aatl)rS .ibu xtrnag1cf torim1 l eaOv.e s == 5 ProbleSmet4 .vp4 age 20- 10 - 12 20- 10- 12 . 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.d eA t 2;0C T 0 5 0 ACT 20'A I - 0 5 0 == == ' == ' == - 20 0 0 4 0 0 4"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.( ,xy ) (d /a (d b / d - be);) ( ,xy z,==) ( ,3- 1,- 2.) == - e) , - e (a [7]. ; -"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. ABC (aT)h aer eath atpo rafa lglreailmso det sot htear gnile haarse a � AB'C' ' 4 2. T(htbear) gni le hatshs ea maeera i;itj s su mto vtetodeh o irgin. =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.T hpei voofA ta sr2 e, 63f o,rmd etiennratm2s, 3 66t;,hp ei voofB tar se2 3,O,. 2 p"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.( a)t ask( e,123,4,5,t) o (, 325,,14),. 1 p- ()b task e,(,2314,5,)to (, 345,2,,1. ) 11T.h peo weorfP s a raelp lem rutoanmt aiitcrsesoe,v e unatlolnoyeft o hesm atcreis musbter eeptaedI.p f r its h sema ea sPs , te hnp rs- == I. 13 (. a )d eA t 3,d eB t1 -6,d eB t2 3 s,oX l -6/3- 2a nX d2 33/ 1. == == == == == == == ()bI AII 11B 3,I 2 1B -2,IB 31 1S.X 1 � a nxd2 - a� d 3x � . == 4, == == == O == == n = 15 (. )a X l� a nX d2 02 : n os oultionX l g a nX d2 � u: nd etde.r mine == == (b) == == 17I.thf e fi rcsotl uimnA ni assl oteh r ighntds- ihb dta eh deenA t deB t1. B othB 2 == B Xl I1B/I A I adn 3ar es inrgs,ui lnaacc oel uimrsne ptee.Tad heroerfe 1 1a nd == == X X 2 3 O. == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19. A Iafl cloa fcto0r( se vienns ian rgolowerc olnu,)thm edne t 0( noe ris.ne v) == == U� ] A hsan oz ecrooaft cobrusit t iisne vrnlt.oei tb = 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21. A CT A - A - Idfe t 1a nwde k notwehc oaftco,rt shen anadsl od et 1. == == == SnicA ei tsh ievn erosfA e 1 A m usbtet hceoa fctmoartf roiC rx. -, 23K.n owC iPn, rg o b2l2ge imvd eeA st (dC e) t1 w in t h4 So cwaecn o rnusctt == 1-11 == . I C/T A A. A- detu sitnhkgen owna cct.Ioo nfrvsteofir ntd == 25 (. )a C ofatcoC r21s C 31C 32 o. == == == l C C ,C C ,C C S- (b)12 21 31 1332 23m ake symmteir.c == == == 27 (. Aar)e 3 1a 2 1.0 T(irbna)g alree 5a. ( Tcir a)n agrale e5. == == == 4 2 1 1 1 1 � � 2 29 (. a )A rea o3 54 1 1 == 5 � (b) + nw5e tnrgiarlaeea - 0 1 51 == 5 + 7 == 1.2 1 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.1 T heed goefts hh ey perhcvauelb egehnt .J 1+ 1 1 1 2T.h veo ludmeHet == + + is 1.6( H h/a2os rh toonrmcaoll nusTm.h edne( tH/ 2)1 l eaadgsat ion 24== == deHt 1.6) == SotliuontosS elteecEdx ercises 455 Y 2 -r - - - - - X1Y 2-X 2Y1== recntgalAe+sB +(Dn C ot) . I I A I I B Arefaorsm rencgtAla e2 t(rn igaal)e =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33. Y1 smaeb saesr encgtBla e2 t(rnigabl)e == -*- - - I I C d smaeh eigrhetcgstl Dae n 2t(irnagdl)e. D == -- - ---1 -- - - - i - X 2 Xl Sot rnigalae s++ d b 21( 1XY 2- X 2Y1). == ,, - Cheacnek x apmlwei t,hb )( (a,32,)e ,d ) (14,,)a narde a 10T.hl ei ne == ( == == form( ,0e i)sn t 3eh pas sl oe /pa ae n edq tuiaYo ne+ ex/ a S.et p3w okrsb ecause == . (,bd )io sn t hlanit!ed e e+/b a i tsre us,i nadc- eb ea eraa ea stt 2e p. == == n n 35 T. h ned- imseinlo nacubec ohrannse2 -r1 2es gd,e asn,2d nf aecso fd imseinon n n 1T.h ceu bwehs oeed gaertseh r eo wosf2 1h avso lu2m.e - �--"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31.J rT.h ceou lmsna roer thoagnotdnh aellie rn gart1eha snr d . == I. B/rBx Br/cBoyes sien 39 • Bex/ BB/eBy (s-ien)r/ (oces)r/ r 41S. (21,-, 1) g ivperasa loalg erlwahmso,ea r etah leie shn gotfs psar ocdruoct: == IP IQ PS I II (-I2 -,2 -,1) I I3 T.h sic omaeslsof or ma d teerm!iT nhaen t x == == othfueorrc omecrosub le(d 00 ,0, ,) ( ,002,,)( 12,2,,)( 110,,.)T hve<\\llumteheo f tilbtoiexIsd e dlt 1. == X Y z 43 d. e t3 2 1 ° 7x y- 5placnnoet sa itntwheo v.e ctors == == + z; 1 23 45 V. I ShAa tsh fiev ree rvsleVasI V,S, V A, SlAAA,.n AdV ISh atsh e rterwvsloeas VIa nVdS. S nic5e -io2sd d, aVnAIdVSI ASh ave iotppeap riotsy. PrloebSme t 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.p1a,g e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.A 2 aAn d3 t,r ac5ed, e tnearnm6it. == == == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.A -5a nAd -4';b oAt'Sha rree dubcy7,e w dih ut nchaenieggvneeodcr .ts == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.A 3A, 1A, = 0w,i het igenv(e10c0,,t,) (o 2-r,s1,0 ,)( ,3- 21,)t ;ra c4e, == == == det0 .A 2A, 2A, -2w,i tehie gvnect(o11r,1,s)( ,,01 0,,)( ,10-, 1); == == == == trac2ed, e ==t - 8. == 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.A x AXg iv(eA-s 7 1x) (-A7 );Ax x AXg ivXe sA Ax-s,oA x == == == == - == (AI)/.x n1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hceo fiecfioef(n _t )- Ain( 1 A- A.). ( A. n- Ai)As + l. . +.A .nI nde (tA- A I), at ermi ntchlaautnod ff-edsi alag ioejnx aclbuohdat eus A a nadj - jAS.cu ha - n1 1 terdmo e'sitnn vo_lAv-)e.S ot hceoffi ecieonf(_t A n)-ind e(tA- A)Im ust ( comfoerm t hper odduocwttnh e dmiaaniga.nTol hcatof ecfiieia nslt +l . . . + nan == A 1+ . +A n. . . T T"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.T rsapnoAs eA - Ide: (t A- A)I de(tA- A)I de(tA -A)I. == == 13T.h eeie gvnaelsou f aAr1e 2, 3, 7, , 9 .8 , 15r. a(n)Ak 1, A0., .. , ,0 (nt rna)cr;ea( nC )k 2,A 0.,. ,n. 2/ -,n /2 == == == == (tra0c.)e ontso S elteecEdx recises 5,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4. 17T.h teh irrocdwo nt6a,i ns"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.9A a nAd2 a nAd00 a lhla vtehs ema ee igvceetn.oT rhseeie gnvuaealsr1 ea n0d. f5o r A,a n0 d.5 2 f oAr2 , 1 a n° df oAr00 . T hereA2 f i ohsra wela fbye twAea enAnd 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "00. 2 1 . } q== 4 a n IAd 2 == -1 ( chtearcckae n dde termwiinXtahl == n ( t1, )2 a) n Xd 2 == A-hatsh sema ee iegncvteoarsAs w, ih te iegnlvuaIeAs/l 14/ a nd (,2- 1). == -.1 IA/ 2 == (aM)u ltAixpt losy e e AX vewahAli.(sc bh S) lo rv(eeA- A )x tofi nxd. 2 3. / == 0 (aP)u (uuTu) uu(T)u soA 1 == . ( bP)v (uuT )vu u(Tv)0 ,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25. == == == U, == == == soA"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "0. ( cX) (1-,1,0,0,) X (-3,0,1,0,) X 3 (-50,,0,1 a) r e l 2 == == == == orhtoglot noua s,ot haeryee i genvoefPcw tihot rA0 s . == 3 27A.- 1 °g ivAe s1 a nAd � ==(- 1 ±i ,J3);th e tehiergnelveua aers1,e 1, - .1 == == 1 1 1)- 29 (. a r)a nk 2 .( bd)e( tT BB )0 .N ot( c.) (d()B + ha (s A+ 1)- == == == 1,�, � . 1 , 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.a 0,b 9==C, ° m ultiApA,l2 i y dn e (t A- AI)9 A-A :A comipoann 3 == == == == matr.i x 2 AlwaAy=s zermoa triifA=x O O, (Cyalye- �l [�[= �"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33. [� �l n Hamlit.o n)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.A x == CAIIXI+ ...+ nAcnXne quBalxs C IAIXI+ .. +. n AcnXnf roa lxlS. o 3 == A ==.B !U !: � [:] ] = ] =(a b)m A; =d - tbporodturca==ecae+d."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37. [ + 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39.W en ee3 d == 1 bA u ntoA t == 1 ( taovd oI i.)W ithAl == e2/1T a3i n Ad 2= e2 -i/ Jr,t3 he -deterwmiiblneAlalA n 2 t 1 a ntdht aer cieAs +l A 2= co2 ;s+ i s i2 ;n+ c o2 ;s- == 2 -1. -1 [� J�. is i;n Onmea twriihttx hs ti racaen dde tnear1nmi tis A :: ::= == ProebmSl te pgae"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.2, 250 ] ] ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. [-�[� ��[ -�r� \\ U n 1 ] ] [�[ _[;�� _r;, [ �]� � _ 1 A 0,0, 3; t hteh icrlodu monfS i s a mu1l taintpdhl oeet hcoeofmrl n uasr e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. == 1 ont hpel aonrheto glot nioa t. 1 A anAd3c anbnedo ita go.nT alhiehzyae voden olnyle i onfee ie gnvesc.t or"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. 3 5 1 30 -1 . 31 50 010 3 00 1 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.A == [ 1 -1] [ ° 1] [ - 11] g IvAe s == [1 - ] 1[ ° 1] [ -11] - 1= 5100 0501 [3. 1 3· 3] � 510+0 5 10-0 4 - + . 1 3 Sotloiuntso S elteecEdx ercises 457"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. t r(aAcBe) tr(acBeA==)a +qb +src d+.tT hetnr (aAcBe B-A)== 0 ( lawyas.) == SoA B- BA== I i sp oissmibfrolm ea itcsrse,i nIdc oene osht a vter azce.er o 1 1 1 1 0 0 11 1 ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.1 ( )aT ured;eA t == 2 O#-.( bF)la se0; . (cF)as le; 0 IS 0 0 2 0 0 2 0 diag!o nal l 1 13 A. [ � n [ � � [ �]-l r 1 '.[2 ] 1;£b u sru q arroe.o ts = - 2 11 1 1 111 [-�� l []�[ [�0][- I} [ � [ 0 ]0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.5 . � 1] ] - 1] = 0 3 l0' 2 2= 2 03"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. 1. ' 3 3 - 11 11 [ [2 ]0[ [ 2 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.7 A -_ ] 1] ] 0 1 05 0 = 0 5' 19 (. )a F laesd:o 'nktn oA' wS . (b )T ur.e (cT)ur e.( dF)la esn:e eedi gveenctors ofS ! 1) 1) 21 T. h ceo luomfSna srm eu llteoisf(p 2,a n(d,0 ieni tohred.Sre ma ref roA -I . 1 1. 1, 23 A. a nBd hAav==le a nAd==2 A+ B h aAs==l A==3 E.i egnlvuaoefAs+ B 2 aern oetq tueoa ilngv leauoefsp Ale uisegn lvuaoefBs ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25. ()aT ure.( bF)la es. (cF)la s(eAm ighhatv 2eo r3 i ndepeenidgeennt- vec)t.o rs 27A. = [�_� ] (oo rth re,)A = [ _1! ]. A == [ � �� lo n leyie gvnectaorre s (c-,c ) . 1; k 29 S. ASkI -a pproazcehreaosn odin fil efyv eIArI<y B 0f ormA .==a9 n d --+ A .==3 . . . [� . [i O[i [iBI[_O i[il"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31. �. ] -i}I ] 10 J ] 10- A S B ().9 (.3) = = = = B1[0� ] �umof tshetow .o = � �r k [ [ [ 3 2k 33B.k = � -n � � -n = [ � k). 2 35t.r aAcB==e ( qa b+s+ )( c drt ==)+( qar) c+ +( b s td+==) t raBcAPe.r oforof diagloibnzlcaaesa et:ht er aocfSe AS I -i s ttrehao ecf( SAI -)== SA w,h icthhe i s suofmt hASe'."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37.Th eA 'fsro ma s ubacs,esp inccAae n Adl+ A 2h avtehse ma eS W.h e Sn== I t, he As'g itvhese u bsopfda icaegl mo antrcaieDsi.m sein4o.n"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39.T wo plrmeos:Tb hneu slplaacncedo lusmpcnaec n ao veprs,lox ac oubled b iont h. Themraeny o bte i ndepeeinegdnevneitctn th oer sc soplaucmen. r � [� 41 A. [ � ha] As2 n an Ad2 - A- I== z emraort icxo nfiCraymelsy ­ = = Hamil.t on ontso S elteedcE xercises"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.B yS FB,h a tshs ema ee iegnve(c1t0o,)ardn (s ,0 1 a)As ,so Bi assl odi aglo.n aThe : [;C ]-[ [�] � ;]:= eqtuiaoAnBs- BA= ; aer-b == aOndc ==:O d ra2nk. A2 Xl X2 Al 45A.h a}sq 1a nd . w4 i th (12,)a nd (1-,1) A.OO h as 1a nd A2 == == == AlA == 2 == 100 100 0( asmeei egnvres.cA) t ohas 1a nd= (). 4,w hich zies.r onear == = 100 00 SoA ivse nreyra A • PorblSetem pa2g6e2 5�u3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.Th eF ibcocnniua mbearrestv eosndt,od d,Td he.no ddo dd Thnee txwto + == even. aroed (drf oomd + de ven a+ no dd .dThe )evner nee ptao d+do dd == even. 2 i ; ; F20 [ 3 [ 4 ["
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.A = a A = n A = ;} = 6 67.5 2 �] [��I]I -��] 5 .A = SAS1 -= [i = A�Al 2 [ �[J_ (no St -i) I.c e � � 2 SAk S-1 = A�Al [ �I 2 �] [ � 1 ��][ _� -��][� = ][ ( � AA )/At� I=(A 2)] L, o"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.D iercatd diL tiL o+ o1ng i ve. s.. L DI, a 2s1, 3 ,4 , 7, ,1 11,,82 ,94 776,, 1 23. k Ak i + 0 Myc alatcougrli sv e (.611. 8) . 1. 122.991. ,.w .h ircohu nfd st oo f == LID"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "132. == 1 0 7 6 112 ? 2' 4 ,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hMea rktornvast iimoant irsi x �O F.r aocntsi 1d o'nmto v.e 6 1 1 1 3 A A4"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.1 (a0),( , 11 -,2) .( b) 1a n-d0 .2(.cl )i m( i34 t,4, ) == eigenvector A== == fro 1. == O<a< 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13.( a) 0< b< 1. [/ -a) 1 lk o b(/-I a) b (1 ] (bU)- [ k _ 1 -1] 0 (a-b)[ k 1 l - ab- 2b --- - (a- b)k b -a +l b - al+ 2(1a ) 1 a b- --- - - - (a- b)k b -a + lb -a +l 2b a 1/3 == b -a + l ilfa- b l< 1 ; b == -/13 a) 2(1 - noMtar kov. b -a + l XXlX2 3 15Th.e c omnpeonotfAs xa dd +t o+ (ecahc omlnua ddtos1a dnn oboidsy AX XX2)3A ',i-X X lIX23 lo.sT thc)emo ponoenft asdt dA o X (1+ + If + musbte + zre.o : [:] 12/,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17. iusn stfabrol e andb elsf rto a 12/N.e utfroarl ± 1/2. lla> lla = < a Sotloiuntso S elteecEdx recises 459 0 0 2 1 12 2 3 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.9 A == 0 0 0a nAd= 0.oSI(-)A-1=I +A + A== 0 11. 0 0 0 0 01 21 I. Af i isn cretahsmeeondr ge,o oadrcseo nmseuidpn o rducatnitdoh enex npsoain mubsets lwoerM.at hetmilac,lAa yx> t rxe imnatsri uAfei isn cer;detm axa gs ouep.s"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. [;3 �] =� u n[ � -� ] [-� �] anAdk =�U - n[ � �][ -� n"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25. = S./A S-I= [i has 2� = A ].,JB w ouhladvA =e .j§ anAd= J=I so R R , ittsre ai ncso rte NaoLt tehl a� t _- � ]cnah avJ=Ie = ia n-di a,nr de saqlue ar . roo[- �t � J"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27. A SAS-la nBd SA2S-l.D igaolnm aatcreiaswl aygsi Ave A2 A2A. == l == I == l TheAnB BA,f orm S AS-lSAS2-l=- SA1A2S-l SAA2 S-l== == l = l SAS2-lSAS-l BA. l == 29 B. h aAs= ia n-di, s oB4 h aA4 s == 1 1Ca;h n adAs (==±1 ,J3\" i/)2 == e x(p±in/ 3,) 3 3 024 so ==A - 1a dn- 1T.h eCn -Ja n Cdl == -C. == PrboleStme page 5a,4 215 '*P8 +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. A l -2a nAd 2== 0 X;= (1-,1)a dn X 2= (,11 ); == l 1 -2t - 2t + 1e - + t ] A 1 e . [2t 2t e == 2 _ e- + 1 -e+ 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. [:! _;� }: 2t u(t=) ast 00,e +00. -+ -+"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. ( )ae �(+ tT) == SeA(t+T)8 -1= SeAtT S e -lA== SeA St -l S eA ST- l et Ae T A. == - [- (b)A= I+ A= Un eB=I + B= [�� lA + B= �� ] gieS � V + .[ co1s - isn1 . . · · eA B . ]fr o mx mape 3 Inth tee xatt=, t1. T hmIaSt InSx = SI1n cos1 E I dieffrefonrmte A.e B t [4t+ 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.e At = I A+t= [� ,. A et U( 0 )_ ] ] 4 1 - ·"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. ( aA) l 7+f!? ,A 2 7f!? - ,R eAl> 0 u, n bsltae(.bA )l ,J7,A 2 -,J7, == = = = 0, ReAl > 0 u,n tbsale( cA)l -l i,JTI ,A 2 -lvTI-;,R eA l> unbsltea = == (dA) 0A, = -2,n uetrsatblalley. l 2 == 11A. i1 us n tbsalfeot r 1n,e rualtslytb alfeot r 1A.i usn asbtflroe t4, n eutrally 2 < > < 4, 4 < satlbaett ==s tbalwei trheA alf ro t 5a,dn s tbalwei tcho mpAlf erox < t 5A.3 i usn bsltfeaoa rl l0 b,te ucsateht er aic2s.et > > 13 (. )a u �== C U 2- b 3U,u ;== -CU I+ a 3U,u �== bUl aU2 givueU�s+l U ; 2+U U U�3== - O.( bB)ec auesAiet as n o rhtoglom natairxlu,It( I)2 I== leIAut 0(1)2 I Iu(IO1)i1 2 s == ontso S elteedcE xrecises 0 + b cotnas.n (tc A) == an±d(J a 2 2+ c )2iS.ek wy-msmecmt artirhiacpveuesr e imgairnya AS'. ! [n_! J6t"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.u ( )t = co2st + cos G l A x AFx+ A 2 Xo,r( A A F- A2 I)xO ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17. == == - > 0a- b+c2 >2 )0 2 2 Eigelnuvaerasre ae lw he(nt ra-c4de e)t 4("
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19.a +22bc2>. =} - - =} [�J [�]+"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21.U l = e4t U 2= et[ -� IufJ O() = (, -5 2 )t, huet(n) = 3e4t 2et[ - n [�]:[� ,� ] !(5 [J�"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23. = The An = ±J4T ). 0 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.A l == adn 2==A 2.N owvt e) == 2 + I Oet2 -+ 00 ast -+ 00. \" �] � A [_"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27. = hatsr a6dc,ee 9t A, = 3a n3dw, ith oinnldye poeenineegdn evnetct or (1, 3 cte,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.)T hat yg == iveys ==' 3 e3.At lsot3e ts olyv\" ==e 6s'y 9-y. (01) ( 0)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.y ( t ==) c otss at ratyts == and ' == O.Th ev ecetqoturia hoanus == (,yy ' == ) y t). (cto-,ss in c b, (A"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31.S but csitu b ut ==i necgvt g ivceetscv == c Aec vt e-tc,bo r cI)v = orv == - (A I -)1 pratiacsruo tlliuIofni .s e ingvleau,te hA en inso itn vbelret:i == - an - I thvif sias l. 33 d. e A/dtt == A + A2 t+ � A3 t2 + � A4t3+ . == . A (+ IA +t � A2 t2+ �A �3 +t . ). == . . AeA.t"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "35.T hselo utaittoi nt m e +i assTl o eA( t+T) uO()T.h uesAtt i meeATse queaA(tl+ Ts) . +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.I Af2 == A t heeAn ==t I A++t � A t2 �tA+3. . .== + ( e-tI )A 7 10 1 1 1 I []+[ e t et ][ e te t ] · _- 10 0- 0- == 0 - 1 1 11 31 [00 l [] ][ ] � [ et A 0 [00 1_1 !'t heeA nt 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39. == == = 3 2 O. == t"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41.( )aT hien voefreA sitees - A. t(bI)Af x == AXt heeAnxt == eAtaxn edA =j:.tO . m · 1 [=�;l m A SAS-"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.A = 2a n5dw ietiheg nevctorasn d Then = = PrloebSme tp aeg"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.,5 288 + 1+ -"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.( bs)u m == 4+ 3 ;ip rod ==u 7c+ ti . (c3) 4i == 3 - 4i;1 i = i ; 13+ 4 iI == 5;1 1- iI == J2.B othn umbleioreusst i dtehu en ciictrl e. -1 +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.x == 2- ix,x == 5, yx == 7i,Ix / == 12/5- (I5/i)x,/ y== 1/2- (I2/i) ; chetchkIxa ty l== J50 == Ilxylla nId/Ix l == /0 == I/Ilx. 1:"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.( ax)2 == r e2i2B,X -I == (Ir /)e-i,xB == re-i;XB -I == x givIe1x2 s == ont hunei t cir.c le Sotliunost oS elteedcE xreicses 461"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.( )ad eAtT== deAtbrtdteAtH== det.A (bA)H == AgivdeeAst=dteA==re.a l _ � � 11P. :A l 0A, 2 = 1X, l= [��,X 2= [��;Q A l= 1A, 2 -1X, l== = : = [���,X 2= [���_;R:A l = 5A, 2 = - 5X, l= [Js]�X, 2= � [- ��.Js] 13'.a ()u , w aorrheto gaotlno ec aho the(rbT.)h neu slplaicssep n anebdyu ;t he v, lfent uslplaicse s tmahee asn ulplthscaee t;hr eo swp acseapn niebsdy adnw ; v � thcelo umsnp aictseh s ema ea st hreo'\\swap ce.(c X) V w;no utn iqwuee , == + cnaa dadn myu ltoifupt loxe . ( dN)e ebdT u O.( eS)-l == ST;S -I AS == == di(aOg1,2,. ) 15T. hdei mseinooSfns i n n1 /)+2n ,on .tE vesryymm etmraitcAr i iasxc ombtiinoan ( ofnp rjcoetsib,oun tth perj eocticohnnagsae sA c hnaegsT.h ereb saiiossf n n o fixepdrje octmiaoitnecr si,tn h sep acoefs ymmemtatriric.ce s S"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.( UVH)(VU) VHUHUV VHI V IS.o U Vi usn itary. == == == iB 19T. hteih rcdlou mnof U c nab e( 1,i /)-0) , m ulltiibepyad n nyu bmree • -2,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "21.A h a+s1 o r- 1i ena cdhi agleo nnt;ear iygphostsii blsi.t ie"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23.C oluomFfno su rmiatierxUr a reei genvoefPcb teocrasu se 2 3 PU== d i(al,gw ,w , )Uw( nadw i==. )"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.n2 s tsef podri retcitmx eo;sn nlly on gs tsef rpoF a nFd-I b yF F(Tna dnf ro A). C 0 l+ i 2 3 1 . . o 2 +1 ia dnA AH == [] ar nere mltmlan• atce ns . 1 3 U l-'i t -i 2 (HAAH)== HAAHH== HAAa agi.n"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "29.c A sitHsie lrltm niif aror ea(lAi )H iHA -==Ai i ssk eHwe-rm.it ian - c; == 0 01 2 3 l"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "31.p == 10 0 ,p I,p OO p99 P P;A cubreo ootfs 11 , == == == == 0 1 0 2 54 2 + 54+ 2 2n3i /4irr3/"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33.C 4 2 5 2+ 5 P+ 4 phaAsC( )== 2 5 +e +4 e == == 4/3 8in3/ 5 4 2 2+ 5 e ni +4 e - r 1 1 - 1+ i [ 2 0 1 1 1 i- ] ] ]"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "35.A = [ t -J3 1+ i 1 0 - 1-J3 - 1- i l ' r 1 1 - 1- i [2i 0 1 1 1+ i . T ] = (Az )= [ ] ]. K -J3 1 i- 1 0 -i-J3 r1-+ i 1 nst oS eelctdeE xrecises � [ 1+ -J3 -1+i ][1 0][1 -J3 1 i]. V_ � + - Ith 2 6+ 2-J3 ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37. L 1 + i 1 -J3 0 -1 - 1- i 1+ -J3 W L == L + - V VHgi s-v--aerAle,u ntiagriyv IeIA s1 t,h ternea z ceogr isvA e 1-, 1. == == == 39D. on'mtu lteiix-pt lisyme x e;i c ojngutateh fier tshtJ e,o2 1i n e2 ix d x [ e2ix/ 2]�1i i O. == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41.R + (R+i S )HR T- iS ;T isRsy mmrcieb tuS ti ssek wsy-mmei.tc r is== =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.[ 1an][d 1- ;][ a b +i c] wI. t a 2 h + b2 + c 2 =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. b ' - IC -a 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "45.( I -2uuHH==)I- u2u(H-I;2 uu=H1 )-4uu+H4(uuuH)Hu=I t;hm eatxr i uHup rjocetso t ohlneit ne uth.r ough Wea rgei vAe+ ni B( A+i )BHA T-i TB.T heAn ATan Bd _BT."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "47. == == == == 1_"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "49.A [/1 i][� �] �[2: i� -] ; S AS-I .R eeagile arilvsu1 ae n4d. = 2 1 = PrboelmS te pgae SaG, 302 1 I"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. N-BN N-IM-AMN (MN-)IAM(N;)o nMly-II M Ii ssi milar C == == == == toI."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.I Af I, . ., .aA rne ei gleunoevAfas t, h Ael+n 1 .,., .+ A 1n eairgele unoevAfsa+ I . SoA a nAd+ I nheavvtheeers amee iegnvusaeal,n cdna 'btes iamri.l"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.I Bfi isn viebrthletenB, A BA(B)BI i ssi mri AltaBo . == - The( ,31e)n torfMy - AIM i gsc oes+ h s ienw,h iczheo ir itfsae n -gh/."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hceeo fifciearn CIet == s 1 C, 2 == 2 d, l == 1d,2 == 1c;h eMcck == d. [�l 11T. hree flieomcnat triwxi bthsa iV Isa nVd 2i As � Th besa iV Isa nd 2( smVae = [�� l - rfleeico!tng)i vBe s IMf U- nth Aen M BM-I . = = = 0 1 0 0 0 0 3 (aD) 0 02 .( bD) 0 0 t0h idredr ivmaaitt.xriT vhteeh ird"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13. == == == 0 0 0 0 0 0 2 3 dertiivvoaef1s x, a, n xd arzee rsooD, O.( cA) 0( tprl;ieo )nloyn e == = idnepenediegnetn( v,10e 0,c.)t or [� [��J n ��l[ J � � 15T. heei gleunaevr1sae, 1 1,,- 1E.i geatinrcmes .[ - 1 I (ar)rH U AUHUHA(U )H I. ( bI)Tf sitrainguralnaudntiathreyin,ts"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17. == - - == diagle onneta(srth ieei gvleaunemuss)ht a avsbelo uvtael1 uT.eh eanlo lff- diagonal enetsarr izee breouc satehc eou lmnasrt ebo e u nvietc .t ors 2 2 2 2 19T. h1e1, e niteorsfr Hr rrHg iIvt 11e 1Iult +I 121t +I 131tsot 12 t 13O. == == == == Copmranigt h2e2, e niteorsfr Hr TTHg ivt 23e sO .S or m usbted igaoln.a == == I 1 I 21I. Nf UA U-,t hNenNH UA U(-U)-HUAHiHe sq lu aUtA oAH UH. == == 1 I Thiitssh sema ea sUA HUAH (AUU -)(HAUU -) NHN .S o insro m.a l N == == Sotloiuntso S elteedcE xercises 463 23 T. h eeie gnveasolf u I()-A are0 0,0, . A(A 21) - ! � !"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.A lwsa[ y : ���; ! ��! ]�( a d )[ �] + ( da b-)C,[] = [�� ] � - + 27M.- 13 MJ0 , MJ JM stoh leatsw qti nelqiiuetaasre ea Tsyryi.fn ogr 1 2 froctehse == == M M J M-1J M. fircslotu monf tboe z ersoo, c anbneio ntv leerCt.ani onbhta v1e 2 == 9 29A 10 210[ 61 - 4 595] . e A '2[ e 1 3 ] == · -80 == -61 - 1'1 :F 3 .[1 � l�[� n u �l [ n� ar s ei mi [� la� r b;y] i etls afn [ d� � ] by itself. 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.3 ( )a( M1A- M()Mx-)M -(1xA)M -10O . ( bT)h neu llsopfA aa cneds == == == ofM -1A M h vaet hsema ed iemnnsD.iffi oerevn]etc todriesffr eabnnsatde. s 35 · J 2 == [c 02 2 2C ] 'cJ ==3 [ c 3 3 cc2 ] 3'J ==k [ ckk 0 kc- 1 k . J 'O -_ c I, J- -_ 1 [ c - 01- Cc-2 ] 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. - 2 e wte) ((wO+t) xO ()� ty(0+�) 3Z t()0S)•t"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37. == + 1 39 (. )a C hoosM ei rveerdsiean gamola trgiexM t i- tJi oi M M( i ne acbhl ock == == Mo Mi Mo1J M o J.T (b)h atsh obsleo ckosni tdsi angatolgo e t == (cA )T == (M- )T1™ JT i s( M-l )TMol JMoTM (MoMMT-)I(AMoMMT,) == AT and issi mrti ol a A. I A 0, 41(.) aT ur:eS>.n� h as the dooteh'set (bnr. ) F lasDeig.ao lniaazn eos nym­ == - _� � [� ][ � ] metmraitcar niAdi ssmy mect.r( icF)la se: and� arsei mri.l a (dT)ur :eA lelie gnveasol fu arien cerdbe ya1 st,h udisff eenfrtor mt eh A + 1 eiegvnaloufe s A. 43D.i gaon6ab ly6s' a4nb dy haaslt lhs ema ee iegnlvuaaess pl6us 4�A B B A 4 - zesr.o ProebmSl te6 1.up a3g16e a-c 2b 2 -2 0;x2 4xy2 y2( x2 y2-) 22 y"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. (ideffreen cof == - 4 == < + + == + sqrueas.)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. d e( tA A I-)A2 - ( +a c ).).a -cb 2 0 g ivA el s( a( + c) == == == + + J(a- C2 ) + b2 )2j a nA d2 (a( + c-)J (-aC 2) 4bj22;A)) l0 i ass um of A 0 == c2) (ac+ 2 )- 42 b > acb 2 . poistniuvbmee ;r2 s beca(uas e recdeutso + > > > 2+ A12A a-c b. Better:p rwoadyuct == b"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.( )aP ositdievfienw ihte-en3 < < 3. b (b) �] [ ! [ 9�� �2 ] ] [ ]�·( c )TO h e. . mII. n SI2 m9u1 ( m [! b 1 - - 2 b) = � [ ]] [ l [�] � [.- � whe !n � [�which i9s 2 ](dN)o m inimluemt, = b = y x x- y -3yth,e n apoparches -+ 00 , == - 00 . ontso S elteecEdx recises - 11 J3 +1 1+ 1+iJ3 i1 - 6 V=[ J[ O 1[ ] =+v32o l+i1 J3+ 0- 1 -1- i1+ J3 wl. thL 2 r:;"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. L J 7 L VV H= ==1 , ==1 , gis-vr--aeeAl,u ntiagriyv IAeIs thternea z cegriov Ae s - 1 . ] �1T = /i527r"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39.D ont'm ulteiix-pt liyme ix e;cs o jngutateh fier stJ, e2 tixdh exn[ e2 ix O. Ri ==(s++ iR ) ==s RH -iT TRS ; S =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41. issmy mectb ruiti s ws-mksmeyect.r i +i 1]; [ bb +b2 c2 1 +.=="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.[ 1 a]n [- d a . CJwih t 2 a - - lC ;-a (2IUU H2)uuHH2 UU2HI 4) u+u-4H Hu )Hu(u u"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.5 - It;hm ea trix (/ uHu == I - u ==. == prjocetosn tthole i tnheur goh AB (+ iA B)=i AH -T BTi .A A T==B _ TB."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.7W ea rgei ven + Then and 1- i == 1 1 1 2+ -2 -i2i - . == ' A[= J O [[ ]== AS1S 14. . 6 - 12 0 l+4 i2 49 .R eeaIleg vnaluaensd . J PrboelmS te5 .,p6 age 302 NBN-N 1M==--A 11NM( ==M-AM Nl()N);MI -==M1 I"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.C only issi mri la == toI . , A,+ 1 ,+ 1 A +"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.I AfI . .,A .na reeie gnlvuaoefs t hAeln .,. n . Aa reeie gnlvuaoefs I. A A + I So and nevhearv es atmeheiee gn vuaealsdn,c na'btes iamri.l B BAB (B)=BA- 1 ABo"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.I fi isn vieb,rtl theen issi mrti ol a (,3 MA-M1 + =/ The 1e)n try ofig sc oes hs ienw, h iczhe iritfosa n -gh . . e 7 1=,2 ==,1 ,1 ;M =c"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hceo fecfiieanrtes d1 d 2 check d. C == C2 == 1 A[ �J=� 11Th. er eflemcattxiwr otiinbh asViia snV d2i s Thbesa iVisa nVd( smae 2 B=[ �� MJ [ -�n A BM=,- iM - refle!cg)ti ivoens If then = 0 1 0 0 0 0 == 20. ==0 0 0 0 3 13(.a D) (bD) thidredr ivmaattxriT.ivh teeh ird 0 0 0 0 0 0 = 1, == == 0 2 3 deriivvoeafst xa,n xd arzee ,rs ooD O. ( cA) (tperl;)io nloyn e (001.,), indepeeinegvdneecntto r 1,- 11., [ �[1 ��,J � J [� �J - 15T. heeie gnlvuaarees Eiegnmtcareis �J [� THTU 1-UAH(UAU==IH- .1 )H"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.( a) (bI)Tf sitarnigrualnuadnriytt,ahietns"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. = diagl oeinnea(tsthre eie gnlvuaemsu)sh ta vasbelo uvtael uTeh eanlo lff- idagonal enitearsrz ee breoc aucsoel utarmhetne bos eu nvietoc r.ts 11, TT==HT HT == === 2 2 2 2 19T. he eniteorsf giIv1e1t1 I111t I121t+I 131tsot12 t13 O. 22, TTHT HT == + Companrgti he enretiso f givt 2e3sO . S oT m ustd ibaeg onal. N==U A - ,UI NHN A== -=(U1 )UHUH-U AIHUA H AHU. 21I. f then iesq utaol AUH=H(AA U1UU A)-- UH=IN( H)NN . Thitsh seima es Ua s So inso r.m al Sotloiunst oS elteedcE xercises 463 23T.h eei genovfA a A l( u I e-( s}A- 2 1 a) r0 e , O.0 ,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.A lwyas[ !:�� �; !���-( ]+a d )[� ] +( da!-bc[� )� ] = [ � �] ! 1 27M. -J 3M == 0 s, to h leat swtio n eqieuasar eleai sTtryiy.nf goM rJ 1 == J 2M f roctehse 1 M M J M-JM. fircslotu monf tboe z er�oo, cn anboeit n vle.erC tainbhnaovt1e == 2 61 13 9 29 A10 == 210[ 45] .e A== 2[ e ] · -08 -59 ' - 1'1 - 16 � 31[ .� �J. [� � [ �l�J [ � ] ars e� i am;ri [ l �]� byi tsaenl[ d� f � by] it.s elf 1 1 1 33 ..( a( )M 1A- M)(M-x ==) M -(xA ==) M -0 == 0. (. bT) h neu slplaocfA e a sdn 1 ofM -AM h avteh e dsia]mmee. Dn iseffrievone]ntc taondrdis ff ebraes.ne ts 35J.2 == [c2 2C 2] J3 _ [c3 3 32 c Jk _ [ckk c-1 k. JO _ I, J 1 - _ [c - 1 - c2 1- ] 0 c' 0- 'c - 0 ck' - - 0 c-' 2 3 37w.t e) == ((wO+t) xO (+)� yt0(+)� Zt(0e5t) . ) 1 39 (. a )o oCsM hei == rveeerd sialgm oantatrogi exM t i- JiMi == M! i n ebalkco hc l Mo Mi MOJM o J.T ()b hatsh obsleo ckosni tdsi alg ogtneoat == l (cA )T == (M-1 )TTJMT i s( M)T-MoIJMoMT == (MoMMT)1 A-M(MoMT), AT '.A andi ssi amtrio l A 0,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "41.( aT)ur eO:n 'eah s == thoet hdeore' .st (n)bF lasDeai.g onaan loisnzyem - - _ [�� ][ �] metmraitxcar niAdi s syi.mc m(ecFtr)la s:e and �arsemi ialr. (dT)ur eA:l elie gnlvuaoefsA I+ a rien ecardsb ey1 t,hs ud ieffrefonrmtt he A. eingvleauoefs 4 4A;B BA -4"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.D iago6b nya alnsd b y haaslt lhs ema ee igveanlause sp l6u s 6 zeor.s"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "61., 316 ProbleSmet page"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.a -c 2 b == 2- 4 == -2 < 0;x2 + 4 yx 2+2 y == (+x2 y)2- 22 y ( deierffncoef squeas.r) 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.d e( tA- A I)== -(aC )++Aa -cb == 0 g ivAl e== s ( a( + c) + 2 A J (-ac 2 )+ b 2 )j2 A (a( c+-) J (-ac 2)+ 4 2 b)2j;)A 0 and2== 1 iass uomf > 2 2 2 2 A 0 (+ac )(-ac )+4 b acb . poistniuvmeb ;e 2r sbeucsae recdeutso > > > -2. A1Aacb Bettwear:yp rodu2c== t"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. ( aP)o isvidete finwihte-en3 < b < 3. (b[ )! � ][ !� ] [ � 9� b2] [ n� ( cT)h mei nimius-m 2 9 (� b2 ) = � n � [! [][ [] � � -.� when ] = whiicsh = [](dN)o m inimluemt, 9 b2 y � x == -y3t,h x e-n y a pparcohes 00, -00. ontso S elteedcE xrecises 1 - 1- 1 1 - 1- 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.( aA )l - 1 1 1 anAd 2= - 1 2 -2. = - 1 1 1 1 -2-1 1 2 ()b1 (X-lX 2- 3X) == 0w heXln- X 2- X 3 O. 1= = o 0 1 2 2 (c1 2)= X(I X- 2- 3X)+X( 2- x3 3)+ x�L;='- 1 1 O. - 1- 31 [� [�;[�] �"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.A ] = �[]n� t hc eo ecffiieonftt hsse uq araerse the = 1 pivionDt ,ws h ertehcaeeos fif icenitnsst ihsdeuqe a raercseo luomfLn .s 2 2 2 11(. aP)vi oatrsae a ncd I-b/alan dde At = ac -I b,l (bM)u ltIi 21pXblyy 2 (-cl bf)al.(c N)o wx AHx ia ss uomf rseqsu(.)ad d et- 1(i nndietfie) and == det+ (p1oi stdievfieen ,)i t = 2 13a . 1a n(d-a 1 ()c 1 )-b ,T hsi meanA -s I it psho iasttdi evfien ite. > >"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.I( ,xy )x 2 + 4 yx 9+2y= ( +x2 y2 +) 5 2 y;I (xy,)x 2 + 6 yx 9+2 y == == == 2 (x +3y). 17x .T A TA x == ( A)T(x xA ==) l negstuhqa r ==e 0do nilAfy x == OS.ni cAeh aisn depen­ decnotl nustm,ho inshl payp ewnhsex n O=. 4 -4 8"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19.A == -4 4 -8h aosn olnype i v == 4o,rt a ==n 1ke ,ie gnvuae2ls,40 0,, 8 -8 16 deAt O. = 2 2 21a . x+2 by+x c yhaass adpdoliaent( t, 0 i0af) c < b 2.T hmea tiriisxn fidinete (A< 0a nAd 0.) > PorbmlS ete page 6a2, 326 ;"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.A i psso itdienvfiieft roea 2B.i nse vpeoirst dievfie:n n iotte[i! cl e >"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.d eAt -=b2 -33 2 b+ 1 i nse agtiavt(e a ndr b)n e�. a =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.I xfTA x 0a nxdTE x 0f ro axn-# y0 , xtT( h eA+nB )x0 c;no dit()iI.o n > > > - [i [n- in"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.P oistAi'vbsee caRui sssey mmeatn,.,fArdi cO R. = R = >"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.I TAx yl2 IxTRT R yl2 IR(x)TR yl2 < (btyh oer driySn cawharizn equality) == == IRIxl2 1lR1yll2 (TxRT RX ()TyRT R y)( Txx (A )yT A y.) = == -];: []� 11A . [= _� ha As= a1n4d a, x 1es an�d[� aJ loe nieggn ve.c tors 13N . geatdifievneim tater: i (cIexT )As x < \"f0ro a lln onrzove ectxo.(r IsAI l)l theeie gnavluoefA s s tasiyfA i O.( IIdIeA)tl< 0, Ade0t, Ad3 <e 0t. < 2> Sotloiuntso S elteedcE xecrsies 465 (IVA)lt lhp ei v(owtistr hooewux tcn hegass)tai yds if< O .( VT)h eirasme a trix R wiintdhe pceonlduemnntts h Aas tu_= c RThR ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15. Flasem usctno taeiing envoefAc )tT;or ru(essm aee igeuneavssaA l);T reu (Q I (QTA Q Q-AQi ssi mrti oAl )aT;ur e( eivagleuonef-sA a r-e).. > 0 .) = e e"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.7S arttf orm = (roowfR T )c(oluomfRn ) = lensgqtruheao dfc omlnu of j j j ajj RT.h edneA t= (dRe)t2 ( volouftmeh e p Ra rlalelpdei2)<p eprodutchte of == . . . ,l engstqrhuesaod fa ltlhc eo mlnuosfR .Th pirso diuaslc a2t 2 an.n l ; 2 - 1 0 2 - 1- 1 ..,/ - �j,; 19 A. ==- 1 2 - 1 h apsi v2o,t sA = - 1 2 - 1i ssi ngular; 0 - 1 2 -1- 1 2 1 0 A 1 0 1 0 21x.T A xi sn optoi stiwvhee( nlX X,,X 3 ) (,010,)be caoufts heze e roont he 2 == daigo.n al,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23.( aP)osi tdievfien rietqeup ioirstedisev tee rm(ilansaaonl:tl> 0A) (.bA )l l pro­ jcetimoarnti ceexpscte a rsei unalgr.( cT)h dei ageonnetasrl Di ao rfie te si gen­ 1 val.u (edsT)h nee gadteifitvnmeeia txr ihadse t 1w henni e sv en. -1 = + 2 2 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.A l= IajanAd= l/2bs,ao 1/� anbd IFz/. Theell9ixp+s16ey 2 = = = � �. 1 haaxswe ishht alf-al e=na gntbd=h s 8"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "27.A =[�� ][i=� []n�=c �[�] haCsCT= [: 2 5] - ' - 29 a. x2 + 2bxy+ C2 = a -'!(yx2 ) ac y:2 ;2b x22 + 8 xy IyO2 2x(+2y2 )+ 22 y. � + + + = 2 31 x. AT x 2X(I- � 2X- �3X) �X( 2- X3)2 ;x TB x= ( lX+ 2X+ X )32 .B h aosn e =' + pivot."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "33.A anCdTA Ch avA 1e> 0A, 2 = .OC t()= t Q (-lt Q)R,Q = [�� l + - R =[ �l� Ch as pooinsveiae tno dn nee tgiaeviegv leaunbeu,Ith atsw poo isviet eigveaulne. s"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "35.T hpei voofAt s�- ar2e. 55.9,-, 0 .8s1o, e oingveea lnoufAe �- inse agvt.ei 1 1 �. TheAnh aasne ingveaslmuaelt lhaenr I"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "37.r a(nCTkA C<)r aAnk,b u atl rsao(n CkATC >)r akn((C-TC)TA C-CI ) r=a Ank."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "39. No.I Cfi nso stqa urCeTA, C i nso tths ema es imzaetx ar sAi . 6 4-A/81 3 A/18 . 54 41d. e [t - - ]= 0g IvAels5 4A, s· - 3 _ A/1 68 4A/18 = 2= _ - ni. Eigveenctors [ []"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "43.G orpu:so rhtongaomla itcr;et As froa ltal;l m la tcrewisi dteh=t 1 I.Afi pso istive e k deifit,nte hger ooufap lp lo weAr csot naionnspl oyis videte fitnmeia tcre.is ontso S elteedcE xreicses V2 ] [!��l"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.A TA [�2 h� a� so nl a yr 85wi htV I so �-�.� �J = = = = ] 2 2 3 a23 1 1 ["
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.A TA 1 haes. I ngveaels0' u1 + -J5 and2 - -J5 . == == == 2 2 A2 A AT A 2-A TA A2.A � Snice ,t ehe ingveectoofr sa rteh sema ea sf or Snice (1-J5 ) == 0 = - negvaet,i l a is but Theu nietie gnvcetoarrste h sem aea si n 0'1 == = a Seu c2 t)2 i' o:6 nf o.A r 2 e, x cefportt h eeff ecotft hsim inussi g(nb aeucsew en eeA dV 2 == [lAl/A+Ji] [2/Al+JA�] 2 1J1/ + A J1+ A� U== V == andu 1 ==1 2 == - v 0 r 1/ ar 1 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.A AT [;i] UIG �� a U �_��.� has with andt with 2 = = = = = 1 1 1vre;/ 1v'2/ 1 01 1 3 ATA 21 haa s 1l w ihtV I== 210) 0)/ / ,a i witV 2h -/0 v'21 , = = == = o 1v'3/ -10/ . anndu llveV 3ct or1 0/ == [ � �] [ �� � ] Then � [U I U 2] [V I V 2 V3T ,] = u a"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.A T l 12 haso nse ignulvaalru e 1.2 = v = VT VT.)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.M ultiUp ly uisncgo mlnus( oUf) t imes (roofw s l: l: A a2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "11.T om ake singrut,lh saem allcehsantsg e+eit 1tsss m la.lsets i ngrvu allauet oz er.o A / aj 13T.h es ingullaureo sfv a+ arneo t Thecyo mfeor me iegnvuaelsof (A/ T)(A + I.) + 1 !4 , [� 15A.+ B = = 1 4 A A; +i st h1rei gh]vter-s1i oenf1 B+ 1 i s ltethif-en 1vsee1or fB . ] [06 [ ] 10[-]l41 ' AT A - [ takseq urae roofo4 at nsd 1 6 == 6 10 2 06 17 1 1 1 11] · == 1 1 3 1[ 11 ][ 0[-] 11 ] [ 1 too btaSi == n - 2 0 4 3 an dQ = A -S 1 == 2 3 1 == [1 ] 1 - 3 ' v10 (TAA )+Ab A Tb"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19.() aW itihn depecnolduenmns,t trhoews paicasel olfR n;c heck 1 = . AT( AT) - bA AT (b) isi nt hreo ws pcaeb ecuase times ainnyt hvasetpcc ae;t or is l (A)TAA+bA TA(AAATT)b- A==bT . ATA +x AbT'. now Bothc saegsi ve == == Sotloiuntso S elcetdeE xrecises 467 21T. ak e A[ � =� an] Bd= [�n Th eAnB= [�l� Fr oCm+i Pnr ob1l5em l [Jt [t wehavAe+= � B+= � =(AB +),nad(AB)�+B+A+."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23.A QI :EQI A+ = 2 :E+Q12f AA+= QI :E+:EQI.S uqargiinvge s == =} => 2 2 (AA +) == Q I:E:E+:E:E+ Qf == QI :E:E+ QI.S ow eh avperjc oeitosn(:A +A)= AA+ (A+AT ) a nsdi mrilflyoAa r+ A .A A+ a nAd+ A p rjeocotn tthoce lo umn == spaacnredo swp aocfAe . PorblSete6m 8, 4p age '- 344 -- -)i,x{\\.rt -;5-tN)-� 1 Px()A) - 4Xl 4- 3Xh aBsP/ BIX= 2XI- X 2- 4, • . BP/ BX 2X-x/3nadBP f<j1 3 - X+2 X-4. 2� -- Xi + 2 == 2 3"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.B PI/Xa x+ y= 0a nadP /IyB =+ 2xy- 3 0g ixv e- 3a nyd 3=P. 2 == == == [�]�. hansom inim(ulme t yI itas ss otceiwdai thtehs emidmeafitnriitxe (0). -+"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.P uxt (1...,1 i),Rn ay lh e'iqsgu ot(itedhneetn otmobiren caonm)Seni.sc R ex( ) == iasl wsba eytwAeIa e nnAd ,n w eg entIA < XT A x sumof a la ijl< n A.n =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.S nicxeB� x 0f roa lnloz nevreoc txox,r T(sA B)wxi blell a rthgaexnTrA x. + > Sot hRea ylqeuiogthiil sea nrftgro e rA +f caBat ln e(l ii ngv eaenlasur ien acsr.ee d)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.S nicxeT B x 0t,hR ea ylqeuiogthfi oAer+ n Bti lsa rtghetarhnq e u otfioAer.n t ;. , 11T. hsem laleeiesgvnta liunAex s= A xan Adx = 'AMx a� ar ne(d 3 v'3 )-/4. 13(. aA)j mijn S[mSaRjxx (x)i]n0 m eatnhsea vte S rjcy o nitnaasv ecxt or = > yTCTACy TAxx wihtR x() O.( by) C=- 1 x . vgeIqsu . ot_ R iy(e)n t > = YT Y Rx()O . > 15T. heex trseumbes S piasscp ea nbnyte hedei e gnevctXolarn sXd 2 2 ' I I 17I. fC x CA(-b)e uqaldst heCnA- b di sze riont hcerro ecttieormn in == - equa5t)io.n ( PorlbDeS lte6 .,5p a3g50e 2 - 1 0 31/6 12/"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.A y bi 4s - 1 2 -1 4/61 -b-- 12/ .T hlei rnfi enaietlee ment == - o 1 2 31/6 12/ - 3 �1,�, . = 16VI + 1�V 2+ tV63e uqlatsh eex a uc= t t ' 6� , 136a t ntohdxee = s u 2 - 1 0 2 1 �."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.A 33= 3,b� TheAn 3 - 1 2 - 1, b 2, yA = b gives == 3 o -1 1 1 5 1 8 y == - . 9 9 ontso S elteedcE xrecises J�V V/,j J��V' ; ['�jV;J J��� V' ;"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.I ntebgyrp aa:tr et s dx dx dx = = = - - Au. smae � . A 4,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. M Their1 2(rR ayatlieqoiu goht itehnsetu p baoscnem uiolpftlo efs = = 2 V ( )xi)ls g aertrh tahnte re eu ingveaA l u]'( e. == h6/ 1,41,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hmea smsa txr iM itsi mthees tridimaagtxor.ni al PorbelmS te7 .,p2 gae35 7"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.I fQ iosrh togointnasol r,im I sI Q I I m aI xQ I xI/ IlI x I I1 b ecaQup sree serves = l = II Q- lengQtxlhl:I lxlif oerv exr.Ayl so iosrh togoannahdla n so rmo nseo, == c() 1. Q =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.I AIBxI I < IAIII/B Ix1/, b yt hdee finoitfth ineoo nro mfA, a ntdh I eB nIxI < I 1B /I Ix IIl· I IAIBIII AIIIIBIII . DivibdyiIlx nlagln mda xiinzm,gi < Thsea mietsu r ef otrh e inrvseI eB ,I1 -A - II <I I B-Il ll A llI-I ;cI( AB) < (Ac)(B) by m ulltyiitpnhsgee c ineqiue.as lit IAII I 1/A xI/I I II, I"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.I nt hdee finitimoanx x choxot soe t bhpeea i rctrue liagenvector == iqnu eosnI tA ;iI xI IIAI lI I,x sI o rtaohti eiI sA I a nmdxa imruamtai lot e tiIa I sAs . == AT AAT AATx AX AA(TAx)"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7. and havtehs ema ee igveaunle,ss icne gives A == = A TA( )Ax A( xA.) /AI\" I AIIT. I Equaltihltergya eteos iefgn lvuameesa ns == == [��l [ �l�"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.A B Aam(x+A B )A amCxA +) Aa mCxB (i) sn 1 ce 0 + 0,) = = > > Aa(AxB)A m(a)AxmAa(xB.) ma(xA ) and > So is nnoortm .a m A 11(. aY)es c ,A ()I AIIIIAI-111cA (l -),s in( cAI e)- I- i A sa ga.i (nbA )I - b x = == == 181 lb l 11 l8 xI I 181IxI 181 lb l leatdos IAIIIII1 lA . T hiiss > � . Ibll-< l - Ilxli Ilxllc I l b ll -"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13.IA III ancd 1I;AII I .J2 ancdi isn fi(nsigiturnel aIAII I .J2 and 1. = 2 = = f); = c = I 15I. A fa mxA inm 1, t haelnA l i 1 a nA d SSI- IT.h oen mlaytc reish wit = == = == = IAII I IA -III I1 ar eo trhgoonmaalt rbieccaAeuAsT sh , eat sbo eI . = = b- A (-1,70)0 b A-z (.0031,.0 01).6"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "17.T hree sli dua imsu cshm altanlh er y = == Buti msu cchls oetrto h seo lutthiaonn z y. . 19x. t +x ;i sn ot llstemhraam na( xtx) ( lIIx)I2 a nndo lta rtghearn + .. . 2 == 00 (iixi + In 1X )2 ,w hiics( h1x 11 1 )1. C ertxali+n ...l+ y x ;< nm axt)(sx,o + . . IlxI <I -fo lx1 l 0001 C hoose (siX l, gs n i xg 2,n , s ixgn)nt goe X t· I1I 1. B1x y y == o • • Y == I·I (11.,,,. 1) . Shcwrzat,hiiassmtosllxtllyilll-fo ilx Chooxs e fomra ximum == == raito-fose 9 -36 30 I 3 3 A- -36 129 -810 . 21T. heex aicntsv eeorf t heb yH ilmbaetrxrt i is == 30 -108 108 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "23.T hleag reIsxlltl II A-bl ilIs/ Ain tm; hl era geesrtri o1 sr- 01 /6Ainm. = [10 [0 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "25.E xchang] et o[2 ] [ 22 ]U 2 I.wtPh ]an d 1 0 -+ 0 -1 == = 1 0 22 Sotloiuns tSoe lteedcE xrecises 469 2 2 2 2 2 2 0 0 0 2 L = A 1 0 1 0 - 11 0 0 � � � U �l 2 2 -+ 0 0 0 0 0 -11 2 2 0 0 1 0 1 0 0 2 0 0 U.Th ePnA= LUwiPt h=0 0 1 anLd= 0 1 0 = . 0 0 1 1 0 0 .5 -.51 '\" ' Po�lbeSmte7 m3p,a 3g6e5 norlmiazteod unvietc .t or k k"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.U k/1A= C IX I+ C 2X 2( 2A/ lA)+_.' C+nXn(nA/lA) C IX Ii fr aailtoIAls/i IAI 1. � < �] Thleag rersattc inooto rl,ws he kni lsa .r g=eA [ � haIAs12 lA=d a nndo covnergee.n c 2x(-y)x T Hx x - (x x - (x HH(x) Hy"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. -y) = -y) yT.h en = is = (-xy )(-Txy ) == Hy. X = -' 1 0 0 1 -5 0 �] 3 4 1 12"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.U = [�0 -,- 5 5 U-antdh Ue-nAl U -5 29 5 25 = == . 4 3 12 16 0 0 5 5 25 25 e e e e ee [?CS si]n [C?S- isn] [1 coss i]n .. e e e. R= Q 9 2 SIne O sIn cos 0 -sin . C(l+ t) _3.�] TheRnQ =[ -s C -s k 11A . ssthuam(teoQ '. Q. k I )R(k1• R•)o• th ieQs Rf catoroinoz fAa ti - - (cretnaltiyre iu kf= 1.)B yc ornusictotAnk+,= RkQk, sk=o A k+RQ1I = l (QI· ·Q JA·Q o'\"Q k)QI·P omsutlltnyiigbpy( kR ··R·o, t) haesu smptgiiovne s -l Rk··R·o= IQ· ·QJ·A 1 •kA +ftmeorv itnhQge' tstho e l etf-hsaintddhes ii ,ts h e k+ I requriersfeuodAlr t ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "13.A haesie gnveas4la un2 dP. u otn e euiengnivte icnrt oo1wro fP: ieti tihse r -]: -]i �[ -�]a�n PdA P-l= [� or� U anPdA P-l= [ �- l�"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.Pi Aju s4esnm u lltaiiotcpin( sf2 ro eeanrctyih nr owasn jdi ). B yf atconrgoi ut e, cost heeni ter1sa n±dt aenn eeodn l2ynm ultaiotpinlwsih,ci cht ol�n e3 ads foPRr . PorbmlS ete7 a,4p a3g7e2 o I o 2 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.D -(-L- U)= � 0 � e,iegnlvuae=s 0 ±, 1,J'2/;( D+ L)-(l-U)= JL I o I o 1 o 2 2 o I i,e igveaulne0s0, 1, 2 /; =4 -,J'22,r eduAcamitxn3 o g 2- ,J'2 0.2. 4 Wopt � o I I 4 8 ontso S elteedcE xersceis"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.A Xk( -2 2 co kshrc ) kXJ;Xk ! (i s2 n khrcs, i3 nkh +rc s iknhrc . ,). . = = = 1 rc rc (ckohrcs) kXF.oh r -, haAes i gveaunle2 s- 2 ocs 2- vi2,2 - cos 2, = 4 -4= -2= 3rc . 2- co- s 2+ M2 . 4 = v 1 1 ° 3 3 2 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.J D-(L+ U ) -° ° ;t hteh rcreicelh easvr ea dsri lu ,r 2 �, = = = = 4 3 4 2 2 0 5 5 4 . r 3 .T hIrec netersz earrseooa ,l aIA lt 1i 4 15 1. = 5 < < 1/2 ° -bal (b)C 1 1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.- D-(+L U ) [ ]haJL s ;- D( L+)- U = -/ed 0 = ± ad == [�b� e�l �A = 0b,e d/A; ma adxo eeqslu JL �a . .."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.I Af x AX, th-eAn) x( -1A ).Rx elae iegnalvuoefsB I- Ah ave (1 == = = 1-1A I1 p,r ovtihdAaei tbds te we° ea nn2 d. < 11A. lwa/AylsBI iIA IIIBIII.CI hoosAe Bt ofi nIdB2 I1!IB II2 I.T hecnoh ose < = < 3 3 A B2 t ofi nIdB II II B2III BI\"I IB1I • 1C onti(nouuresi en diuoc.nS tn)ice = < < IBII I m aIxA( BIi),it ns o s uirsptreh IaBtII I 1g ivcenosvg enerce. > < [1�] 13J. achosabS i-l T � � �] wi tIlhmA ax � G. auss -eSidheaSls-l T [ = = = � witIAlhm ax (AlI m froJ caboi.) 2 = = ax"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "15.S cucevses rioravexelat(OiSRoi)nnM ATLAB. 17T. hmea ximruosmwu mgsi avlellI A .a9dn IA I 4T.h e cairuroncddli easgl ona < < enrtiest iggibhvoteue. nFr di tsrA :sT hcei rIAc- l.1e2 cotnasit nhoet hceirr cles .7 < IA- .13 .a5dn I - A. 11 .a6dn a ll etihgrleeune.evS scaeonA:dT hceri cle < < IA- 21 < 2 c notsat ihcnei rIAc - l 2e 1< 1a nadllt heree ingveal2+u ,Jie ,s2 a,dn 2 - ,Ji."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "19.r b l-Aab b (-bbTlTAb b)iAsob rot ghnolat or o b:t hrseedi ausl l = = = r b A-xae ro trohgoanetaa slcet .hpT os hotwhP aiIts th oogroltn Aoa po Ab, = = simypP lIti ocf IP:P I IAIbl2 bl- (bATbA )badn bTb(lbATb2 . C) ertainly c = = (APb I) 0Tb, ea ucsAeT A(.T ths maipilfiatcipounatl i nPtIo b- a lAb+ = = = (T bb- 2 IabTA b+a IrAlbI2 )llbTbbFo.ar g odod sicuosnss eeNi umeircLanile ar Agleabb yrT reetfhaenBnd ua .) PorbmlS ete page"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "81., 381"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.T hcemo erasra et( , 0 6) ( , 22,),( 60;),s eeF ig8ur.e3 ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.T hceno srtaignit3v(se 2 +x5 y + )2(- x3+ 8y)9- 1,0o r3 y1 - 1C.na' t < < havye 0 . >"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.x 0y, 0w,i th canodsrdtaientdaht xt + y ° a dmointtlshy pe o in0 t.) > > < (0,"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.x ( %5b nods)(9 %b on)d 2s,000a 0dny ( 6b%no ds6) 000,.0 = z = = Soutlointso S elteedcE xrecises 471"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.Th ec otsobt e m inimiis1z0 0ex0d+ 2 00+0 3y00 0+z1 050+u3 00v0+ 37w0.0 Thaem ouxn,y t s, t oC hicaangdov w, t oN ewE ngnldsa taiys fxu + u, Z 10,00;, +0v y0 010,0000;,z0 + w 10,0000;, x0 + y z 80000;,0 + == == == u+ v w 22,000,0.0 + == PorblSetem page 82e6 391 Atp resX4e n4 ta nXds 2 a rient hbeai ssa,nt dh ceo iszstre .oT heen tering"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. == == vaarlbiseh obueXl3 ,dt or edtuhcceeo sThtle.ev aivnragib alseh obuexl ,sds ince 21/ 2, 2, ilsse tsh 4a1/nW .it Xh3 a dnX 4i tnh besas it,h ceo nasitrgntiXsv3 e X4 == = antdh ceo isnstwo X l+ X2- X3 -2. == Th\"er edcusocteasdr\" e [11 ] sco, h nagies gnoooatdndt hcero nieosrp ti.m al"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. r == [� J"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.A tP, [-53; ] t ehna tQ, � iosp tibmeacla u>0 s e0 r = r == - ; R r"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.F or a mparxoibtmlhuseemtm o ptpeibsnetgc omeOs. t ihfsal isa,nt dh ieht If r < cmoponietsnh ltera getshtte,hnc a otl nuo mfN e ntteehbr sasi; th ser ulef otrh e 8C vecltevoairtn hgbe a siitshs se ma e."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9. . ..0 .] [. . ... . ] s , i nv c eu S.o is ctrroheecm tat xr.i BE B [· B E == v = u = T T l 11I.A fx 0 t, h Penx x- A(A)A- Ax x. � = == PorblSetem page 8�ju3 99"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.M axim' iY4 z+l e l 1 2Y,w ihY t >l0 Y, 2> 0 2,Y +l Y 2 < 13,Y ::S2 1 t;hp er ihmaasl � , � xi 2, xi �t,hd eu haaly s i yi ,ocst5 . = == == == ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.T hdeu maalx iizmyebsw, ih y t > cT.h erex fobar ney d car ef aesbil,ae nd = == gitvhesem a ev alcuf eob r c sotithtn eh p er imadlu abalyn ;td h meuysb teo pt.i mal SF \" fb Ot,htehnoeptixm*iacslhantg(oe,Obd 2 ,. . b)nady* (,0 C2 , '\"c n). l l < 0, n =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.b [0l T] a ncd [1- 0.] == =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.S niccex 3 ybx,a nda roep timal by SF. = = y T"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.x * [1O ], * y[10 ,] w ihy t *b 1 c*xT.h sece onidn equiabnlto iht ies == :=' := == A x*> b anyd*A ca rseit crstot,eh s eccoonmdp oonfy e* na tnxsd* a rzee .r o < 11(.) a i x 0 x,i 1�,: 0c,T X 3. (bI)it ts eh fi rqsautd rwaintth the = = == == tetrahientd hrceomo ne cru otff .( cM)a xiizmes ujebcttoY l> 0 Y, l5 , < ' Yl, Y l 3,Y l4 Y; l* 3 . < < = � � 13H. erce [1 1 w itA1 h ][ �] .N o c ornasixt>n 0 ts ot hdeu waill l = = T haveeq uyaly iA t (oA ry c)TT.h agti v2Y esl 1 a nY dl 1 a nY d2 2 == c == = == == anndfo e assioboll.nuSe ott hiper immuashlta vea sm axi:mX ulm -Na nd 00 = X2 2Na nXd3 0 g iCvoes Xtl + X 2+ X 3 N b(irartarliarlg.ye ) = = . = == 1 0 0- 1 0 0 1 0 -10 15T. hceo luomfn0 s 1 0 0- 1 0 or0 1 -10. o 0 1 0 0- 1 0 0 - 11 17T. kaey [1- ]1 t; h y enA> 0 y, bO . == < nst oS elteecde Erxcises PorblmeS te page 8�4, 406"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. T hmea xiflmoaiwl s1 3w,i tthhm ei nicmuastle r panatgni od6fe or mt hoet her nod.e s"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.I ncsrietnahgce pa cayio tfp ipfeormsn od4te on od6oe r n od4te o n od5e will prodtuhlcergae e isntc rietnah mseae x iflmoawTl.h m ea xiflmoaiwln cerfseor ams 8t o9 ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.A ssicganp aci1tt oi eeagsdle .Tls h mea ximnuumm boefdr ij isonptta hfsor m s = ttot hen tehmqeau xailmsflu wom. The nmuimnebori fmeu dmgw ehsso ree moval disccotnfsonr met i tsh mei nimcuuTmth .em na flxo wm in cut. s ="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.R osw ,,14 a5vn idto Hela a'lcslo nido;itn theb y3s3 u btmxrac iomifrnogm r1o,w s 4,5 a,n d co1l2,,u5 h m an3ss 3 + 5. >"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.( )aT hmea thrai2sxn I wsh iccnahon tb ec ovebryles edst h anln i nbeeuscs ae ealcihcn oev eexrastc wtIolsIy t.t a sk leni ;nt ehsemruesb teac ompmlaehttice.n g 1 1 1 11 1 00 01 (b) 0 0 1 0 . cTahbneec o1rv eewsdi ftohu r; lfimivanerea rsgiaerse 1 1 00 01 1 11 1 1 noptos sible. 11I.e fa ch+ m1a rersti he aocnpclteyam balne t ehriens o foon#re1t om arry m m, (veetnoh ugahla elra cpcteatbol e #1). 13A. logri1tg himv 1e3-s3, - ,22 -,52 -,44 -,6a nd al2gg oirv2ie-t,s45h -m26 -4, , 3-123,-T. he saere eq ualh-s lheonrsgtpnteansittner ge. s"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.5 ( )aR osw1 3,5, o nlyI hiscan vo uelm n2sa n4d. ( bC)ol um1n3,s5, (i2n ,r ows"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "4.) (cZ)e rsou btmrafiorxmr ow13s,5 , a ncdo lsu1 m3,n5, . R(odw2),s a 4n d colu2m4,nc sor va elIl.s PorlbmeS eBt.,p5 age 431"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.- 10XI+ 70(-IXI) 1X0I- I0I(-xI)o,xrl � , X � -10Y+I 10I(-I)Y 2 = = = = ; 7Y0I- 101-(Y)Io,rY I � Y � avegrepayaoyffAx 6. 2 = , = = ;"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.I fc ohossc eolumnw illo sicetsh smo alelnetasjrit( y i n ir)ow.wi nlolt j, X Y X mov,se intciheists h lera egsetn tirnty h raotwI .nP romb 2l,ae l 2w aasn 2 = equiloiftb hrkiiisun mId fw.e e xacnhgteh2 ea n4db leoiwtn ,oe ntrhyath si s preorpatny,d msitxrgeaidate resre e qu.i red"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.T hbee ssrtta tferog cyo mbtihnteew lsoi ntpoer so dahu ocreni tz1aoilgnuear,a n­ X � � teetihnhiges i og7fh/ t3T .hc eo mbinias( t3+iy2 o (n-1 y)+) (y+ 3 (-1y)) = �, � 7/ 3s,o cohostehcseou lmnwsi fterhq nuceie0s, X ."
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.F ocro lnuswm,e wXaain+ t ( -1 xI)bX el (-1 xI)d so u, = + = X(la- b e+- d) d- bF.o rrwo sYi, +a ( -1Y )Iey bl+ ( -1Y )Id = = = v excnhgabeasn d.C omeparwei th u v: (a- b)d(- b) ad -be u_ - · xI(a-b) +b-- +b -- smaea ftbe re � = v. a -b- e+ d --a -b - e+ d --- Sotloiuntso S elteedcE xercsi se 473"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hien nmearx imiutsmh l ea rogfYel a rn Yd 2;x c oennctraotnet sho antSe u.jeb ct toY I Y 2 == 1t ,h mei ntiimm ofth �a egr eYir �s N .o tAi ==c Je . + [��T J"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.1A x* anyd Ax*�Y I �Y 2 �f ro satrlalt oefg iy*es A == == == == + Y; -Jl �; [�� - 1 anydA* x �Xl� X 2- X 3- X,4w hich canniont exceed == + betwieyseA *nx* �. == (,� 13 V. al ue( frga maie.) choos2e so yrc hsoeoosd odre v:ex n*y * �.) 0 X 3, = == PorblSetem p age A, 420"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1.Ya e)L aersgdti ( m S n T ) == 7 wh Se en T. (bS)ma ellsdti ( m S n T ) == 2 . Smaelsdlti m T)8 w hen (e) (S+ == S e T. 3 ()dL aersgdti m T) (aolfRl 1) . (S - 13 + %t� au a 12a 13 a 14 all a12 0 0 a 21 a 22a 2 3 a 24 0 a 22a 2 3 0"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3.V W anV d n W cnotain and + 0 a 32 a 33a 3 4 0 0 a 23 a 34 .. 0 0 a 34 a 44 0 0 0 a 44 di( mV W) and d( iVnm W ) 7a;d dg etto diV m diWm. + == 13 == 20== + Thlei ntrehosu (g,11h1 ,a)n (d,1 1 2,)hv aeV nW {O.}"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5. =="
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "7.O nbesa ifsoV r + W iVs, IV 2 , di( mVn W ) == 1w ihbt as (i ,01s -, 1 0,). WI;"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "9.T hietn rescetiocno lousfmp nai cse sth teh rloiunge(,6h 3 ,6): == y 1 1 1 5 3 0 1 53 0 [1] 1] 1 2 y 1 [ macthe[sA B x] O. == 3 0 == 0 == 3 0 0 == 3 -2 2 4 0 2 2 4 0 2 -3 Thcelo umsnp ahcaedvsie m seinoTnh esiuram n idn teecrtgsiiovne1"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "2. 3 == 2+2 . + 1 1 1 1 - 1 - 1 1- -11 . - 1- 11 13A. 3D ( IAD A ID A ID)' == 0 J® J)+ 1( 0 ® 1)+ (1® 1® PorblSetem p age 8, 421 0 1 0 [��]"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "1. (Aids i agloiblnze;aa) o (egievncetors and = = 0 0 (,10 ,0) J J 0 0 0 (,2 1,-0)). 1 t 2t 2"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "3. I B+ sinBceO A.sl o I +t .J eBt == 0 1 0 == t == elt == o 0 0 1 00 [�� ] ��"
    },
    {
        "chapter": "LinearProgrammingandGameTheory",
        "question": "5.J � (disetieignnlvcusat;e)J (hBa As but = = = 0,0 1.) rank"
    }
]